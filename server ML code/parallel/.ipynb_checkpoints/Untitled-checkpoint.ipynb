{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=pd.read_csv('temporal-data.csv')\n",
    "fix=pd.read_csv('fixed-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3142"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fix['county_fips'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips=fix['county_fips'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips=fix['county_fips'].unique().tolist()\n",
    "tempfips=fips[:157]\n",
    "fips=fips[157:]\n",
    "tempfix=fix[fix['county_fips'].isin(tempfips)]\n",
    "temptemp=temp[temp['county_fips'].isin(tempfips)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempfix.to_csv('./01/fixed-data.csv')\n",
    "temptemp.to_csv('./01/temporal-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    j=i+1\n",
    "    tempfips=fips[:157]\n",
    "    fips=fips[157:]\n",
    "    tempfix=fix[fix['county_fips'].isin(tempfips)]\n",
    "    temptemp=temp[temp['county_fips'].isin(tempfips)]\n",
    "    if (j<10):\n",
    "        tempfix.to_csv('./0'+str(j)+'/fixed-data.csv',index=False)\n",
    "        temptemp.to_csv('./0'+str(j)+'/temporal-data.csv',index=False)\n",
    "    else:\n",
    "        tempfix.to_csv('./'+str(j)+'/fixed-data.csv',index=False)\n",
    "        temptemp.to_csv('./'+str(j)+'/temporal-data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix=pd.read_csv('./01/fixed-data.csv')\n",
    "temp=pd.read_csv('./01/temporal-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_fips</th>\n",
       "      <th>state_fips</th>\n",
       "      <th>state_name</th>\n",
       "      <th>county_name</th>\n",
       "      <th>total_population</th>\n",
       "      <th>female-percent</th>\n",
       "      <th>area</th>\n",
       "      <th>population_density</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>...</th>\n",
       "      <th>smokers</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>Religious</th>\n",
       "      <th>party</th>\n",
       "      <th>airport_distance</th>\n",
       "      <th>passenger_load</th>\n",
       "      <th>meat_plants</th>\n",
       "      <th>median_household_income</th>\n",
       "      <th>%insured</th>\n",
       "      <th>deaths_per_100000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>55601</td>\n",
       "      <td>0.514487</td>\n",
       "      <td>594.44</td>\n",
       "      <td>93.535092</td>\n",
       "      <td>32.539527</td>\n",
       "      <td>-86.644082</td>\n",
       "      <td>...</td>\n",
       "      <td>18.081557</td>\n",
       "      <td>11.1</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.450536</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0</td>\n",
       "      <td>59338</td>\n",
       "      <td>91.278314</td>\n",
       "      <td>964.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>218022</td>\n",
       "      <td>0.515384</td>\n",
       "      <td>1589.78</td>\n",
       "      <td>137.139730</td>\n",
       "      <td>30.727750</td>\n",
       "      <td>-87.722071</td>\n",
       "      <td>...</td>\n",
       "      <td>17.489033</td>\n",
       "      <td>10.7</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.533710</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>2</td>\n",
       "      <td>57588</td>\n",
       "      <td>88.666596</td>\n",
       "      <td>1081.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>24881</td>\n",
       "      <td>0.472168</td>\n",
       "      <td>884.88</td>\n",
       "      <td>28.117937</td>\n",
       "      <td>31.868263</td>\n",
       "      <td>-85.387129</td>\n",
       "      <td>...</td>\n",
       "      <td>21.999985</td>\n",
       "      <td>17.6</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>106.344161</td>\n",
       "      <td>0.011937</td>\n",
       "      <td>2</td>\n",
       "      <td>34382</td>\n",
       "      <td>87.757208</td>\n",
       "      <td>1254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>22400</td>\n",
       "      <td>0.467813</td>\n",
       "      <td>622.58</td>\n",
       "      <td>35.979312</td>\n",
       "      <td>32.996421</td>\n",
       "      <td>-87.125115</td>\n",
       "      <td>...</td>\n",
       "      <td>19.114200</td>\n",
       "      <td>14.5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.990010</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>0</td>\n",
       "      <td>46064</td>\n",
       "      <td>89.793747</td>\n",
       "      <td>1232.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>57840</td>\n",
       "      <td>0.507261</td>\n",
       "      <td>644.78</td>\n",
       "      <td>89.705016</td>\n",
       "      <td>33.982109</td>\n",
       "      <td>-86.567906</td>\n",
       "      <td>...</td>\n",
       "      <td>19.208672</td>\n",
       "      <td>17.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>49.528971</td>\n",
       "      <td>0.003389</td>\n",
       "      <td>5</td>\n",
       "      <td>50412</td>\n",
       "      <td>86.639241</td>\n",
       "      <td>1191.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>5083</td>\n",
       "      <td>5</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>Logan County</td>\n",
       "      <td>21737</td>\n",
       "      <td>0.500759</td>\n",
       "      <td>708.13</td>\n",
       "      <td>30.696341</td>\n",
       "      <td>35.214132</td>\n",
       "      <td>-93.719510</td>\n",
       "      <td>...</td>\n",
       "      <td>20.399456</td>\n",
       "      <td>18.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>130.024885</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>2</td>\n",
       "      <td>41471</td>\n",
       "      <td>90.377787</td>\n",
       "      <td>1288.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>5085</td>\n",
       "      <td>5</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>Lonoke County</td>\n",
       "      <td>73657</td>\n",
       "      <td>0.507148</td>\n",
       "      <td>770.73</td>\n",
       "      <td>95.567838</td>\n",
       "      <td>34.753922</td>\n",
       "      <td>-91.887424</td>\n",
       "      <td>...</td>\n",
       "      <td>18.585973</td>\n",
       "      <td>12.8</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.944702</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0</td>\n",
       "      <td>56651</td>\n",
       "      <td>92.884815</td>\n",
       "      <td>913.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>5087</td>\n",
       "      <td>5</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>Madison County</td>\n",
       "      <td>16481</td>\n",
       "      <td>0.500698</td>\n",
       "      <td>834.26</td>\n",
       "      <td>19.755232</td>\n",
       "      <td>36.010382</td>\n",
       "      <td>-93.725249</td>\n",
       "      <td>...</td>\n",
       "      <td>20.813868</td>\n",
       "      <td>12.1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.375308</td>\n",
       "      <td>0.004369</td>\n",
       "      <td>7</td>\n",
       "      <td>42674</td>\n",
       "      <td>88.537940</td>\n",
       "      <td>1316.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>5089</td>\n",
       "      <td>5</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>Marion County</td>\n",
       "      <td>16722</td>\n",
       "      <td>0.509090</td>\n",
       "      <td>597.01</td>\n",
       "      <td>28.009581</td>\n",
       "      <td>36.268445</td>\n",
       "      <td>-92.684519</td>\n",
       "      <td>...</td>\n",
       "      <td>19.922011</td>\n",
       "      <td>17.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>145.519675</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>1</td>\n",
       "      <td>36581</td>\n",
       "      <td>90.279330</td>\n",
       "      <td>1608.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>5091</td>\n",
       "      <td>5</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>Miller County</td>\n",
       "      <td>43592</td>\n",
       "      <td>0.508832</td>\n",
       "      <td>625.58</td>\n",
       "      <td>69.682535</td>\n",
       "      <td>33.314034</td>\n",
       "      <td>-93.892853</td>\n",
       "      <td>...</td>\n",
       "      <td>21.478718</td>\n",
       "      <td>12.3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>96.657214</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>1</td>\n",
       "      <td>42162</td>\n",
       "      <td>91.923729</td>\n",
       "      <td>1222.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     county_fips  state_fips state_name     county_name  total_population  \\\n",
       "0           1001           1    Alabama  Autauga County             55601   \n",
       "1           1003           1    Alabama  Baldwin County            218022   \n",
       "2           1005           1    Alabama  Barbour County             24881   \n",
       "3           1007           1    Alabama     Bibb County             22400   \n",
       "4           1009           1    Alabama   Blount County             57840   \n",
       "..           ...         ...        ...             ...               ...   \n",
       "152         5083           5   Arkansas    Logan County             21737   \n",
       "153         5085           5   Arkansas   Lonoke County             73657   \n",
       "154         5087           5   Arkansas  Madison County             16481   \n",
       "155         5089           5   Arkansas   Marion County             16722   \n",
       "156         5091           5   Arkansas   Miller County             43592   \n",
       "\n",
       "     female-percent     area  population_density   latitude  longitude  ...  \\\n",
       "0          0.514487   594.44           93.535092  32.539527 -86.644082  ...   \n",
       "1          0.515384  1589.78          137.139730  30.727750 -87.722071  ...   \n",
       "2          0.472168   884.88           28.117937  31.868263 -85.387129  ...   \n",
       "3          0.467813   622.58           35.979312  32.996421 -87.125115  ...   \n",
       "4          0.507261   644.78           89.705016  33.982109 -86.567906  ...   \n",
       "..              ...      ...                 ...        ...        ...  ...   \n",
       "152        0.500759   708.13           30.696341  35.214132 -93.719510  ...   \n",
       "153        0.507148   770.73           95.567838  34.753922 -91.887424  ...   \n",
       "154        0.500698   834.26           19.755232  36.010382 -93.725249  ...   \n",
       "155        0.509090   597.01           28.009581  36.268445 -92.684519  ...   \n",
       "156        0.508832   625.58           69.682535  33.314034 -93.892853  ...   \n",
       "\n",
       "       smokers  diabetes  Religious  party  airport_distance  passenger_load  \\\n",
       "0    18.081557      11.1       68.0      0         35.450536        0.005342   \n",
       "1    17.489033      10.7       53.0      0         58.533710        0.000656   \n",
       "2    21.999985      17.6       55.0      0        106.344161        0.011937   \n",
       "3    19.114200      14.5       50.0      0         71.990010        0.008750   \n",
       "4    19.208672      17.0       65.0      0         49.528971        0.003389   \n",
       "..         ...       ...        ...    ...               ...             ...   \n",
       "152  20.399456      18.0       80.0      0        130.024885        0.003312   \n",
       "153  18.585973      12.8       45.0      0         30.944702        0.000950   \n",
       "154  20.813868      12.1       37.0      0         60.375308        0.004369   \n",
       "155  19.922011      17.0       38.0      0        145.519675        0.004306   \n",
       "156  21.478718      12.3       60.0      0         96.657214        0.001996   \n",
       "\n",
       "     meat_plants  median_household_income   %insured  deaths_per_100000  \n",
       "0              0                    59338  91.278314              964.0  \n",
       "1              2                    57588  88.666596             1081.1  \n",
       "2              2                    34382  87.757208             1254.0  \n",
       "3              0                    46064  89.793747             1232.1  \n",
       "4              5                    50412  86.639241             1191.2  \n",
       "..           ...                      ...        ...                ...  \n",
       "152            2                    41471  90.377787             1288.1  \n",
       "153            0                    56651  92.884815              913.7  \n",
       "154            7                    42674  88.537940             1316.7  \n",
       "155            1                    36581  90.279330             1608.7  \n",
       "156            1                    42162  91.923729             1222.7  \n",
       "\n",
       "[157 rows x 33 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_fips</th>\n",
       "      <th>date</th>\n",
       "      <th>social-distancing-total-grade</th>\n",
       "      <th>social-distancing-encounters-grade</th>\n",
       "      <th>social-distancing-travel-distance-grade</th>\n",
       "      <th>weekend</th>\n",
       "      <th>daily-state-test</th>\n",
       "      <th>confirmed</th>\n",
       "      <th>death</th>\n",
       "      <th>virus-pressure</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>01/22/20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>01/23/20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>01/24/20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>01/25/20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>01/26/20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23702</th>\n",
       "      <td>5091</td>\n",
       "      <td>06/16/20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1428.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>48.0</td>\n",
       "      <td>26.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23703</th>\n",
       "      <td>5091</td>\n",
       "      <td>06/17/20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15133.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23704</th>\n",
       "      <td>5091</td>\n",
       "      <td>06/18/20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7735.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23705</th>\n",
       "      <td>5091</td>\n",
       "      <td>06/19/20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6338.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23706</th>\n",
       "      <td>5091</td>\n",
       "      <td>06/20/20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7462.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23707 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       county_fips      date  social-distancing-total-grade  \\\n",
       "0             1001  01/22/20                            1.0   \n",
       "1             1001  01/23/20                            1.0   \n",
       "2             1001  01/24/20                            1.0   \n",
       "3             1001  01/25/20                            1.0   \n",
       "4             1001  01/26/20                            1.0   \n",
       "...            ...       ...                            ...   \n",
       "23702         5091  06/16/20                            1.0   \n",
       "23703         5091  06/17/20                            1.0   \n",
       "23704         5091  06/18/20                            1.0   \n",
       "23705         5091  06/19/20                            1.0   \n",
       "23706         5091  06/20/20                            1.0   \n",
       "\n",
       "       social-distancing-encounters-grade  \\\n",
       "0                                     1.0   \n",
       "1                                     1.0   \n",
       "2                                     1.0   \n",
       "3                                     1.0   \n",
       "4                                     1.0   \n",
       "...                                   ...   \n",
       "23702                                 2.0   \n",
       "23703                                 2.0   \n",
       "23704                                 2.0   \n",
       "23705                                 1.0   \n",
       "23706                                 1.0   \n",
       "\n",
       "       social-distancing-travel-distance-grade  weekend  daily-state-test  \\\n",
       "0                                          1.0        0               NaN   \n",
       "1                                          1.0        0               NaN   \n",
       "2                                          1.0        1               NaN   \n",
       "3                                          1.0        2               NaN   \n",
       "4                                          1.0        1               NaN   \n",
       "...                                        ...      ...               ...   \n",
       "23702                                      1.0        0            1428.0   \n",
       "23703                                      1.0        0           15133.0   \n",
       "23704                                      1.0        0            7735.0   \n",
       "23705                                      1.0        1            6338.0   \n",
       "23706                                      1.0        2            7462.0   \n",
       "\n",
       "       confirmed  death  virus-pressure  precipitation  temperature  \n",
       "0            0.0    0.0             0.0            0.0          NaN  \n",
       "1            0.0    0.0             0.0          104.0          NaN  \n",
       "2            0.0    0.0             0.0          163.0          NaN  \n",
       "3            0.0    0.0             0.0            0.0          NaN  \n",
       "4            0.0    0.0             0.0            0.0          NaN  \n",
       "...          ...    ...             ...            ...          ...  \n",
       "23702        2.0    1.0             4.5           48.0         26.4  \n",
       "23703        4.0    0.0            10.5            0.0         25.3  \n",
       "23704        0.0    0.0             2.5            0.0         27.2  \n",
       "23705        1.0    0.0            64.5            3.0         27.8  \n",
       "23706        9.0    0.0            33.0            5.0         27.2  \n",
       "\n",
       "[23707 rows x 12 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# my way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from makeHistoricalData import makeHistoricalData\n",
    "from models import GBM, GLM, KNN, NN, MM_GLM, GBM_grid_search, NN_grid_search\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from matplotlib import colors as mcolors\n",
    "from pexecute.process import ProcessLoom\n",
    "import time\n",
    "from sys import argv\n",
    "import sys\n",
    "from math import floor, sqrt\n",
    "import os\n",
    "#import dill\n",
    "import glob\n",
    "import shutil\n",
    "import zipfile\n",
    "import email, smtplib, ssl\n",
    "from email import encoders\n",
    "from email.mime.base import MIMEBase\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "import mimetypes\n",
    "import subprocess as cmd\n",
    "import shelve\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "import statistics\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "r = 21  # the following day to predict\n",
    "numberOfSelectedCounties = 2\n",
    "target_mode = 'regular'\n",
    "spatial_mode = 'county'\n",
    "numberOfSelectedCountiesname = 1535\n",
    "iteration = 1\n",
    "######################################################### split data to train, val, test\n",
    "def splitData(numberOfCounties, main_data, target, spatial_mode, mode ):\n",
    "\n",
    "    numberOfCounties = len(main_data['county_fips'].unique())\n",
    "\n",
    "    if mode == 'val':\n",
    "      main_data = main_data.sort_values(by=['date of day t' , 'county_fips'])\n",
    "      target = target.sort_values(by=['date of day t' , 'county_fips'])\n",
    "      X_train_train = main_data.iloc[:-2*(r*numberOfCounties),:].sort_values(by=['county_fips' , 'date of day t'])\n",
    "      # X_train_train = X_train_train.drop(['date of day t', 'county_fips'], axis=1)\n",
    "      X_train_val = main_data.iloc[-2*(r*numberOfCounties):-(r*numberOfCounties),:].sort_values(by=['county_fips' , 'date of day t'])\n",
    "      # X_train_val = X_train_val.drop(['date of day t', 'county_fips'], axis=1)\n",
    "      X_test = main_data.tail(r*numberOfCounties).sort_values(by=['county_fips' , 'date of day t'])\n",
    "      # X_test = X_test.drop(['date of day t', 'county_fips'], axis=1)\n",
    "\n",
    "      y_train_train = target.iloc[:-2*(r*numberOfCounties),:].sort_values(by=['county_fips' , 'date of day t'])\n",
    "      y_train_val = target.iloc[-2*(r*numberOfCounties):-(r*numberOfCounties),:].sort_values(by=['county_fips' , 'date of day t'])\n",
    "      y_test = target.tail(r*numberOfCounties).sort_values(by=['county_fips' , 'date of day t'])\n",
    "\n",
    "      return X_train_train , X_train_val , X_test , y_train_train , y_train_val , y_test\n",
    "\n",
    "    if mode == 'test':\n",
    "      main_data = main_data.sort_values(by=['date of day t' , 'county_fips'])\n",
    "      target = target.sort_values(by=['date of day t' , 'county_fips'])\n",
    "\n",
    "      X_train = main_data.iloc[:-(r*numberOfCounties),:].sort_values(by=['county_fips' , 'date of day t'])\n",
    "      # X_train = X_train.drop(['date of day t', 'county_fips'], axis=1)\n",
    "      X_test = main_data.tail(r*numberOfCounties).sort_values(by=['county_fips' , 'date of day t'])\n",
    "      # X_test = X_test.drop(['date of day t', 'county_fips'], axis=1)\n",
    "\n",
    "      y_train = target.iloc[:-(r*numberOfCounties),:].sort_values(by=['county_fips' , 'date of day t'])\n",
    "      y_test = target.tail(r*numberOfCounties).sort_values(by=['county_fips' , 'date of day t'])\n",
    "\n",
    "      return X_train , X_test , y_train , y_test\n",
    "\n",
    "\n",
    "\n",
    "########################################################### clean data\n",
    "def clean_data(data, numberOfSelectedCounties):\n",
    "    global numberOfDays\n",
    "    data = data.sort_values(by=['county_fips', 'date of day t'])\n",
    "    # select the number of counties we want to use\n",
    "    # numberOfSelectedCounties = numberOfCounties\n",
    "    if numberOfSelectedCounties == -1:\n",
    "        numberOfSelectedCounties = len(data['county_fips'].unique())\n",
    "\n",
    "    using_data = data[(data['county_fips'] <= data['county_fips'].unique()[numberOfSelectedCounties - 1])]\n",
    "    using_data = using_data.reset_index(drop=True)\n",
    "    main_data = using_data.drop(['county_name', 'state_fips', 'state_name'],\n",
    "                                axis=1)  # , 'date of day t'\n",
    "    # target = pd.DataFrame(main_data['Target'])\n",
    "    # main_data = main_data.drop(['Target'], axis=1)\n",
    "    # numberOfCounties = len(using_data['county_fips'].unique())\n",
    "    numberOfDays = len(using_data['date of day t'].unique())\n",
    "\n",
    "    return main_data\n",
    "\n",
    "\n",
    "########################################################### preprocess\n",
    "def preprocess(main_data, spatial_mode, validationFlag):\n",
    "\n",
    "    target = pd.DataFrame(main_data[['date of day t', 'county_fips', 'Target']])\n",
    "    main_data = main_data.drop(['Target'], axis=1)\n",
    "    # specify the size of train, validation and test sets\n",
    "    t1 = time.time()\n",
    "    # produce train, validation and test data in parallel\n",
    "\n",
    "    if validationFlag:     # validationFlag is 1 if we want to have a validation set and 0 otherwise\n",
    "        # add the functions to the multiprocessing object, loom\n",
    "\n",
    "        X_train_train , X_train_val , X_test , y_train_train , y_train_val , y_test = splitData(numberOfSelectedCounties, main_data, target, spatial_mode,'val')\n",
    "        return X_train_train, X_train_val, X_test, y_train_train, y_train_val, y_test\n",
    "\n",
    "    else:\n",
    "\n",
    "        X_train , X_test , y_train , y_test = splitData(numberOfSelectedCounties, main_data, target, spatial_mode,'test')\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################ MASE_denominator\n",
    "def mase_denominator(r, target_name, target_mode ,numberOfSelectedCounties):\n",
    "\n",
    "    data = makeHistoricalData(0, r, target_name, 'mrmr', 'country', target_mode, './')\n",
    "    if numberOfSelectedCounties == -1 :\n",
    "      numberOfSelectedCounties = len(data['county_fips'].unique())\n",
    "    data = clean_data(data, numberOfSelectedCounties)\n",
    "    X_train_train, X_train_val, X_test, y_train_train_date, y_train_val_date, y_test_date = preprocess(data, 'country', 1)\n",
    "\n",
    "    train_train = (y_train_train_date.reset_index(drop=True)).sort_values(by=['date of day t', 'county_fips'])\n",
    "    train_val = (y_train_val_date.reset_index(drop=True)).sort_values(by=['date of day t', 'county_fips'])\n",
    "    test = (y_test_date.reset_index(drop=True)).sort_values(by=['date of day t', 'county_fips'])\n",
    "\n",
    "    train_lag = train_train.copy().iloc[:-(numberOfSelectedCounties*r), :].tail(numberOfSelectedCounties*r).rename(\n",
    "        columns={'Target': 'train-lag-Target'})\n",
    "    train_train = train_train.tail(numberOfSelectedCounties*r).rename(columns={'Target': 'train-Target'}) #[['train-Target']]\n",
    "    train_val = train_val.tail(numberOfSelectedCounties*r).rename(columns={'Target': 'val-Target'}) #[['val-Target']]\n",
    "    test = test.tail(numberOfSelectedCounties*r).rename(columns={'Target': 'test-Target'}) #[['test-Target']]\n",
    "\n",
    "    df_for_train_lag_MASE_denominator=pd.concat([train_train.reset_index(drop=True), train_lag.reset_index(drop=True)], axis=1)\n",
    "    #[['county_fips','date of day t','Target']]\n",
    "    df_for_train_lag_MASE_denominator['absolute-error']=abs(df_for_train_lag_MASE_denominator['train-Target'] -\n",
    "                                                            df_for_train_lag_MASE_denominator['train-lag-Target'])\n",
    "\n",
    "    df_for_train_val_MASE_denominator = pd.concat([train_train.reset_index(drop=True),train_val.reset_index(drop=True)], axis=1)\n",
    "    #[['county_fips','date of day t','Target']]\n",
    "    df_for_train_val_MASE_denominator['absolute-error']=abs(df_for_train_val_MASE_denominator['val-Target'] -\n",
    "                                                            df_for_train_val_MASE_denominator['train-Target'])\n",
    "\n",
    "    df_for_val_test_MASE_denominator=pd.concat([train_val.reset_index(drop=True),test.reset_index(drop=True)], axis=1)\n",
    "    #[['county_fips','date of day t','Target']]\n",
    "    df_for_val_test_MASE_denominator['absolute-error']=abs(df_for_val_test_MASE_denominator['test-Target'] -\n",
    "                                                           df_for_val_test_MASE_denominator['val-Target'])\n",
    "\n",
    "    train_val_MASE_denominator = df_for_train_val_MASE_denominator['absolute-error'].mean()\n",
    "    val_test_MASE_denominator = df_for_val_test_MASE_denominator['absolute-error'].mean()\n",
    "    train_lag_MASE_denominator = df_for_train_lag_MASE_denominator['absolute-error'].mean()\n",
    "\n",
    "    return train_val_MASE_denominator, val_test_MASE_denominator, train_lag_MASE_denominator\n",
    "\n",
    "\n",
    "########################################################### run non-mixed methods in parallel\n",
    "def parallel_run(method, X_train_train, X_train_val, y_train_train, y_train_val, best_loss, c):\n",
    "\n",
    "    y_prediction, y_prediction_train = None, None\n",
    "    if method == 'GBM':\n",
    "        y_prediction, y_prediction_train = GBM(X_train_train, X_train_val, y_train_train, best_loss['GBM'])\n",
    "    elif method == 'GLM':\n",
    "        y_prediction, y_prediction_train = GLM(X_train_train, X_train_val, y_train_train)\n",
    "    elif method == 'KNN':\n",
    "        y_prediction, y_prediction_train = KNN(X_train_train, X_train_val, y_train_train)\n",
    "    elif method == 'NN':\n",
    "        y_prediction, y_prediction_train = NN(X_train_train, X_train_val, y_train_train, y_train_val, best_loss['NN'])\n",
    "\n",
    "    return y_prediction, y_prediction_train\n",
    "\n",
    "\n",
    "########################################################### run mixed methods in parallel\n",
    "def mixed_parallel_run(method, X_train, X_test, y_train, y_test, best_loss):\n",
    "\n",
    "    y_prediction, y_prediction_train = None, None\n",
    "    if method == 'MM_GLM':\n",
    "        y_prediction, y_prediction_train = MM_GLM(X_train, X_test, y_train)\n",
    "    elif method == 'MM_NN':\n",
    "        y_prediction, y_prediction_train = NN(X_train, X_test, y_train, y_test, best_loss[method])\n",
    "\n",
    "    return y_prediction, y_prediction_train\n",
    "\n",
    "\n",
    "########################################################### run algorithms in parallel except mixed models\n",
    "def run_algorithms(X_train_dict, X_val_dict, y_train_dict, y_val_dict, best_loss, c , spatial_mode, county_fips):\n",
    "    from models import GBM, GLM, KNN, NN\n",
    "    t1 = time.time()\n",
    "    methods = ['GBM','GLM','KNN','NN']\n",
    "    X_train = {method : None for method in methods}\n",
    "    X_val = {method : None for method in methods}\n",
    "    y_train = {method : None for method in methods}\n",
    "    y_val = {method : None for method in methods}\n",
    "    loom = ProcessLoom(max_runner_cap=4)\n",
    "    # add the functions to the multiprocessing object, loom\n",
    "    if spatial_mode == 'country':\n",
    "      for method in methods:\n",
    "        X_train[method] = X_train_dict[method].drop(['county_fips','date of day t'],axis=1)\n",
    "        X_val[method] = X_val_dict[method].drop(['county_fips','date of day t'],axis=1)\n",
    "        y_train[method] = np.array(y_train_dict[method]['Target']).reshape(-1)\n",
    "        y_val[method] = np.array(y_val_dict[method]['Target']).reshape(-1)\n",
    "      loom.add_function(GBM, [X_train['GBM'], X_val['GBM'], y_train['GBM'], best_loss['GBM']], {})\n",
    "      loom.add_function(GLM, [X_train['GLM'], X_val['GLM'], y_train['GLM']], {})\n",
    "      loom.add_function(KNN, [X_train['KNN'], X_val['KNN'], y_train['KNN']], {})\n",
    "      loom.add_function(NN, [X_train['NN'], X_val['NN'], y_train['NN'], y_val['NN'], best_loss['NN']], {})\n",
    "    if spatial_mode == 'county':\n",
    "      for method in methods:\n",
    "        X_train[method] = X_train_dict[method]\n",
    "        X_train[method] = X_train[method][X_train[method]['county_fips']==county_fips].drop(['county_fips','date of day t'],axis=1)\n",
    "        print('run_alg X_train[method].shape',X_train[method].shape)\n",
    "        X_val[method] = X_val_dict[method]\n",
    "        X_val[method] = X_val[method][X_val[method]['county_fips']==county_fips].drop(['county_fips','date of day t'],axis=1)\n",
    "        y_train[method] = y_train_dict[method]\n",
    "        y_train[method] = y_train[method][y_train[method]['county_fips']==county_fips].drop(['county_fips','date of day t'],axis=1)\n",
    "        y_val[method] = y_val_dict[method]\n",
    "        y_val[method] = y_val[method][y_val[method]['county_fips']==county_fips].drop(['county_fips','date of day t'],axis=1)\n",
    "        y_train[method] = np.array(y_train[method]['Target']).reshape(-1)\n",
    "        y_val[method] = np.array(y_val[method]['Target']).reshape(-1)\n",
    "      loom.add_function(GBM, [X_train['GBM'], X_val['GBM'], y_train['GBM'], best_loss['GBM']], {})\n",
    "      loom.add_function(GLM, [X_train['GLM'], X_val['GLM'], y_train['GLM']], {})\n",
    "      loom.add_function(KNN, [X_train['KNN'], X_val['KNN'], y_train['KNN']], {})\n",
    "      loom.add_function(NN, [X_train['NN'], X_val['NN'], y_train['NN'], y_val['NN'], best_loss['NN']], {})\n",
    "\n",
    "    # run the processes in parallel\n",
    "    output = loom.execute()\n",
    "    t2 = time.time()\n",
    "    print('total time - run algorithms: ', t2 - t1)\n",
    "\n",
    "    return output[0]['output'], output[1]['output'], output[2]['output'], output[3]['output']\n",
    "\n",
    "\n",
    "########################################################### run mixed models in parallel\n",
    "def run_mixed_models(X_train_MM, X_test_MM, y_train_MM, y_test_MM, best_loss):\n",
    "\n",
    "    from models import GBM, GLM, KNN, NN, MM_GLM\n",
    "    t1 = time.time()\n",
    "    loom = ProcessLoom(max_runner_cap=2)\n",
    "    # add the functions to the multiprocessing object, loom\n",
    "    loom.add_function(MM_GLM, [X_train_MM['MM_GLM'], X_test_MM['MM_GLM'], y_train_MM['MM_GLM']], {})\n",
    "    loom.add_function(NN, [X_train_MM['MM_NN'], X_test_MM['MM_NN'], y_train_MM['MM_NN'], y_test_MM['MM_NN'], best_loss['MM_NN']], {})\n",
    "    # run the processes in parallel\n",
    "    output = loom.execute()\n",
    "    t2 = time.time()\n",
    "    print('total time - run mixed models: ', t2 - t1)\n",
    "\n",
    "    return output[0]['output'], output[1]['output']\n",
    "\n",
    "####################################################################### update best loss\n",
    "\n",
    "def update_best_loss(model_type ,spatial_mode ,county_fips,best_loss,X_train_train_to_use,X_train_val_to_use,y_train_train,\\\n",
    "                     y_train_val,y_prediction_train,y_prediction,covariates,\\\n",
    "                     numberOfCovariates,max_c):\n",
    "    h = 1\n",
    "    if model_type == 'mixed_model':\n",
    "          loom = ProcessLoom(max_runner_cap=1)\n",
    "          c = numberOfCovariates\n",
    "          if numberOfCovariates > max_c :\n",
    "            c = max_c\n",
    "          y_predictions_test, y_predictions_train = [], []\n",
    "          if spatial_mode == 'county':\n",
    "            # Construct the outputs for the testing dataset of the 'MM' methods\n",
    "            y_predictions_test.extend([y_prediction[county_fips]['GBM'][(h, c)], y_prediction[county_fips]['GLM'][(h, c)],\n",
    "                                        y_prediction[county_fips]['KNN'][(h, c)], y_prediction[county_fips]['NN'][(h, c)]])\n",
    "            \n",
    "          elif spatial_mode == 'country':\n",
    "            y_predictions_test.extend([y_prediction['GBM'][(h, c)], y_prediction['GLM'][(h, c)],\n",
    "                                        y_prediction['KNN'][(h, c)], y_prediction['NN'][(h, c)]])\n",
    "          y_prediction_test_np = np.array(y_predictions_test).reshape(len(y_predictions_test), -1)\n",
    "          X_test_mixedModel = pd.DataFrame(y_prediction_test_np.transpose())\n",
    "          if spatial_mode == 'county':\n",
    "            # Construct the outputs for the training dataset of the 'MM' methods\n",
    "            y_predictions_train.extend([y_prediction_train[county_fips]['GBM'][(h, c)], y_prediction_train[county_fips]['GLM'][(h, c)],\n",
    "                                        y_prediction_train[county_fips]['KNN'][(h, c)], y_prediction_train[county_fips]['NN'][(h, c)]])\n",
    "          elif spatial_mode == 'country':\n",
    "            y_predictions_train.extend([y_prediction_train['GBM'][(h, c)], y_prediction_train['GLM'][(h, c)],\n",
    "                                        y_prediction_train['KNN'][(h, c)], y_prediction_train['NN'][(h, c)]])\n",
    "          y_prediction_train_np = np.array(y_predictions_train).reshape(len(y_predictions_train), -1)\n",
    "          X_train_mixedModel = pd.DataFrame(y_prediction_train_np.transpose())\n",
    "          loom.add_function(NN_grid_search, [X_train_mixedModel,y_train_train , X_test_mixedModel,y_train_val])\n",
    "          best_loss_output = loom.execute()\n",
    "          best_loss['MM_NN'] = best_loss_output[0]['output']\n",
    "          \n",
    "    if model_type == 'none_mixed_model':\n",
    "          loom = ProcessLoom(max_runner_cap= 2)\n",
    "          if spatial_mode == 'country':\n",
    "            loom.add_function(GBM_grid_search, [X_train_train_to_use['GBM'][covariates],\n",
    "                                                    y_train_train , X_train_val_to_use['GBM'][covariates],\n",
    "                                                    y_train_val])\n",
    "            loom.add_function(NN_grid_search, [X_train_train_to_use['NN'][covariates],\n",
    "                                                    y_train_train , X_train_val_to_use['NN'][covariates],\n",
    "                                                    y_train_val])\n",
    "          if spatial_mode == 'county':\n",
    "            loom.add_function(GBM_grid_search, [X_train_train_to_use[county_fips][h]['GBM'][covariates],\n",
    "                                                    y_train_train , X_train_val_to_use[county_fips][h]['GBM'][covariates],\n",
    "                                                    y_train_val])\n",
    "            loom.add_function(NN_grid_search, [X_train_train_to_use[county_fips][h]['NN'][covariates],\n",
    "                                                    y_train_train , X_train_val_to_use[county_fips][h]['NN'][covariates],\n",
    "                                                    y_train_val])\n",
    "          best_loss_output=loom.execute()\n",
    "          best_loss['GBM'],best_loss['NN'] = best_loss_output[0]['output'],best_loss_output[1]['output']\n",
    "    return best_loss\n",
    "\n",
    "########################################################### \n",
    "\n",
    "def get_best_loss_mode(counties_best_loss_list):\n",
    "\n",
    "  methods_with_loss=['GBM', 'NN', 'MM_NN']\n",
    "  best_loss = {method: None for method in methods_with_loss}\n",
    "  for method in methods_with_loss:\n",
    "\n",
    "    counties_best_loss_array=np.array(counties_best_loss_list[method])\n",
    "    # when we choose number_of_selected_counties smaller than number of different losses\n",
    "    # some times its not possibel to find mode\n",
    "    if len(np.unique(counties_best_loss_array))==len(counties_best_loss_array):\n",
    "      best_loss[method] = random.choice(counties_best_loss_list[method])\n",
    "    else:\n",
    "      best_loss[method] = statistics.mode(counties_best_loss_list[method])\n",
    "  return(best_loss)\n",
    "\n",
    "########################################################### generate data for best h and c\n",
    "\n",
    "def generate_data(h, numberOfCovariates, covariates_names, numberOfSelectedCounties):\n",
    "\n",
    "    data = makeHistoricalData(h, r, 'confirmed', 'mrmr', spatial_mode, target_mode, './')\n",
    "    data = clean_data(data, numberOfSelectedCounties)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = preprocess(data, spatial_mode, 0)\n",
    "    covariates = [covariates_names[i] for i in range(numberOfCovariates)]\n",
    "    best_covariates = []\n",
    "    indx_c = 0\n",
    "    for covar in covariates:  # iterate through sorted covariates\n",
    "        indx_c += 1\n",
    "        for covariate in data.columns:  # add all historical covariates of this covariate and create a feature\n",
    "            if covar.split(' ')[0] in covariate:\n",
    "                best_covariates.append(covariate)\n",
    "\n",
    "    best_covariates += ['county_fips','date of day t'] # we add this two columns to use when we want break data to county_data\n",
    "    X_train = X_train[best_covariates]\n",
    "    X_test = X_test[best_covariates]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "########################################################### plot the results\n",
    "\n",
    "def plot_results(row, col, numberOfCovariates, methods, history, errors, mode):\n",
    "    print(history)\n",
    "    mpl.style.use('seaborn')\n",
    "    plt.rc('font', size=20)\n",
    "    fig, ax = plt.subplots(row, col, figsize=(40, 40))\n",
    "    colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "    # Sort colors by hue, saturation, value and name.\n",
    "    by_hsv = sorted((tuple(mcolors.rgb_to_hsv(mcolors.to_rgba(color)[:3])), name)\n",
    "                    for name, color in colors.items())\n",
    "    sorted_names = [name for hsv, name in by_hsv]\n",
    "    colorset = set(sorted_names[::-1])\n",
    "    for item in colorset:\n",
    "        if ('white' in item) or ('light' in item):\n",
    "            colorset = colorset - {item}\n",
    "    colors = list(colorset - {'lavenderblush',  'aliceblue', 'lavender', 'azure',\n",
    "         'mintcream', 'honeydew', 'beige', 'ivory', 'snow', 'w'})\n",
    "    #colors = ['tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan',\n",
    "     #         'tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "    ind = 0\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            color = 0\n",
    "            for h in history:\n",
    "                errors_h = []\n",
    "                # x label: covariates\n",
    "                covariates_list = [c for c in range(1, numberOfCovariates + 1)][:maxC]\n",
    "                # y label: errors\n",
    "                for c in range(1, numberOfCovariates + 1):\n",
    "                    errors_h.append(errors[methods[ind]][(h, c)])\n",
    "                    if c == maxC:\n",
    "                      break\n",
    "                ax[i, j].plot(covariates_list, errors_h, colors[color * 2], label=\"h = \" + str(h))\n",
    "                ax[i, j].set_xlabel(\"Number Of Covariates\")\n",
    "                ax[i, j].set_ylabel(mode)\n",
    "                ax[i, j].set_title(str(methods[ind]))\n",
    "                ax[i, j].legend()\n",
    "                ax[i, j].set_xticks(covariates_list)\n",
    "                color += 1\n",
    "            ind += 1\n",
    "    address = validation_address + 'plots_of_errors/'\n",
    "    if not os.path.exists(address):\n",
    "        os.makedirs(address)\n",
    "    plt.savefig(address + str(mode)+'.png')\n",
    "\n",
    "\n",
    "########################################################### plot table for final results\n",
    "def plot_table(table_data, col_labels, row_labels, name, mode):\n",
    "    fig = plt.figure() #dpi=50 figsize=(30, 10)\n",
    "    ax = fig.add_subplot(111)\n",
    "    colWidths = [0.1, 0.1, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
    "    address = ''\n",
    "    if mode == 'val':\n",
    "        # colWidths.pop()\n",
    "        address = validation_address + 'tables/'\n",
    "        if not os.path.exists(address):\n",
    "            os.makedirs(address)\n",
    "    else:\n",
    "        address = test_address + 'tables/'\n",
    "        if not os.path.exists(address):\n",
    "            os.makedirs(address)\n",
    "    the_table = plt.table(cellText=table_data,\n",
    "                          colWidths=colWidths,\n",
    "                          rowLabels=row_labels,\n",
    "                          colLabels=col_labels,\n",
    "                          loc='center',\n",
    "                          cellLoc='center')\n",
    "    the_table.auto_set_font_size(False)\n",
    "    the_table.set_fontsize(9)\n",
    "    the_table.scale(1.5, 1.5)\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.savefig(address + name + '.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "########################################################### plotting mean errors (first error)\n",
    "def plot_targets(method, x_axis, df, main_address):\n",
    "    mpl.style.use('default')\n",
    "    plt.rc('font', size=40)\n",
    "    fig, ax = plt.subplots(figsize=(60, 20))\n",
    "    ax.plot(x_axis, df['average of targets'], label='Target')\n",
    "    ax.plot(x_axis, df['average of predictions'], label='Prediction')\n",
    "    ax.set_xlabel('date', fontsize=40)\n",
    "    ax.set_ylabel('real and predicted targets for ' + str(method), fontsize=40)\n",
    "    ax.legend()\n",
    "    address = main_address + 'procedure_of_prediction/'\n",
    "    if not os.path.exists(address):\n",
    "        os.makedirs(address)\n",
    "    plt.savefig(address +'procedure_'+ str(method) +'.png')\n",
    "\n",
    "\n",
    "########################################################### box plots and violin plots\n",
    "def box_violin_plot(X, Y, figsizes, fontsizes, name, address):\n",
    "    mpl.style.use('default')\n",
    "    # box plot\n",
    "    fig = plt.figure(figsize=figsizes['box'])\n",
    "    plt.rc('font', size=fontsizes['box'])\n",
    "    plt.locator_params(axis='y', nbins=20)\n",
    "    sns.boxplot(x=X, y=Y)\n",
    "    plt.savefig(address + str(name) + 'boxplot.png')\n",
    "    plt.close()\n",
    "    # violin plot\n",
    "    fig = plt.figure(figsize=figsizes['violin'])\n",
    "    plt.rc('font', size=fontsizes['violin'])\n",
    "    plt.locator_params(axis='y', nbins=20)\n",
    "    sns.violinplot(x=X, y=Y)\n",
    "    plt.savefig(address + str(name) + 'violinplot.png')\n",
    "    plt.close()\n",
    "########################################################### plot prediction and real values\n",
    "\n",
    "def real_prediction_plot(df,r,target_name,best_h,spatial_mode,methods,numberOfSelectedCounties):\n",
    "\n",
    "    address = test_address + 'plots_of_real_prediction_values/'\n",
    "    if not os.path.exists(address):\n",
    "        os.makedirs(address)\n",
    "\n",
    "    for method in methods:\n",
    "       \n",
    "        method_prediction_df = df[method] # this df contain real and predicted target values\n",
    "        county_name_df=pd.read_csv('./'+'fixed-data.csv')[['county_fips','county_name']] # we need county names for plot label\n",
    "        df_for_plot = pd.merge(method_prediction_df,county_name_df,how='left')\n",
    "\n",
    "        df_for_plot['date'] = df_for_plot['date of day t'].apply(lambda x:datetime.datetime.strptime(x,'%m/%d/%y')+datetime.timedelta(days=r))\n",
    "        df_for_plot['date'] = df_for_plot['date'].apply(lambda x:datetime.datetime.strftime(x,'%m/%d/%y'))\n",
    "\n",
    "        counties = [36061]+random.sample(df_for_plot['county_fips'].unique().tolist(),2) # newyork + two random county\n",
    "\n",
    "        length=list()\n",
    "        for county in counties:\n",
    "          length.append(len(df_for_plot[df_for_plot['county_fips']==county]))\n",
    "\n",
    "        plot_with=max(length)+20\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(plot_with,75))\n",
    "        mpl.style.use('default')\n",
    "\n",
    "        for index,county in enumerate(counties):\n",
    "\n",
    "            plt.subplot(311+index)\n",
    "            plt.rc('font', size=45)\n",
    "            plt.plot(df_for_plot.loc[df_for_plot['county_fips']==county,'date'],df_for_plot.loc[df_for_plot['county_fips']==county,method],label='Prediction',linewidth=2.0)\n",
    "            plt.plot(df_for_plot.loc[df_for_plot['county_fips']==county,'date'],df_for_plot.loc[df_for_plot['county_fips']==county,'Target'],label='Real values',linewidth=2.0)\n",
    "            plt.xticks(rotation=65)\n",
    "            fig.subplots_adjust(hspace=0.4)\n",
    "            plt.ylabel('Number of confirmed')\n",
    "            countyname = df_for_plot.loc[df_for_plot['county_fips']==county,'county_name'].unique()\n",
    "            if len(countyname)>0 : # it is False when newyork is not in selected counties and make error\n",
    "              plt.title(df_for_plot.loc[df_for_plot['county_fips']==county,'county_name'].unique()[0])\n",
    "            plt.legend()\n",
    "        plt.xlabel('Date')\n",
    "        plt.savefig(address + str(method) + ' real_prediction_values.jpg')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "########################################################### get errors for each model in each h and c\n",
    "def get_errors(h, c, method, y_prediction, y_prediction_train, y_test_date, MASE_denominator, numberOfSelectedCounties, mode):\n",
    "    # make predictions rounded to their closest number and make the negatives ones zero\n",
    "    y_prediction = np.round(y_prediction)\n",
    "    y_prediction[y_prediction < 0] = 0\n",
    "    # write outputs into a file\n",
    "    orig_stdout = sys.stdout\n",
    "    f = open(env_address+'out.txt', 'a')\n",
    "    sys.stdout = f\n",
    "    # if mode == 'val': y_test_date would be an np.array with the target\n",
    "    # if mode == 'test': y_test_date would be a dataframe with columns ['date of day t', 'county_fips', 'Target']\n",
    "    y_test = y_test_date\n",
    "    if mode == 'test':  # use the 'Target' column for computing main errors\n",
    "        y_test = np.array(y_test_date['Target']).reshape(-1)\n",
    "\n",
    "    meanAbsoluteError = mean_absolute_error(y_test, y_prediction)\n",
    "    print(\"Mean Absolute Error of \", method, \" for h =\", h, \"and #covariates =\", c, \": %.2f\" % meanAbsoluteError)\n",
    "    sumOfAbsoluteError = sum(abs(y_test - y_prediction))\n",
    "    percentageOfAbsoluteError = (sumOfAbsoluteError / sum(y_test)) * 100\n",
    "    # we change zero targets into 1 and add 1 to their predictions\n",
    "    y_test_temp = y_test.copy()\n",
    "    y_test_temp[y_test == 0] = 1\n",
    "    y_prediction_temp = y_prediction.copy()\n",
    "    y_prediction_temp[y_test == 0] += 1\n",
    "    # meanPercentageOfAbsoluteError = sum((abs(y_prediction_temp - y_test_temp) / y_test_temp) * 100) / len(y_test)\n",
    "    print(\"Percentage of Absolute Error of \", method, \" for h =\", h, \"and #covariates =\", c,\n",
    "          \": %.2f\" % percentageOfAbsoluteError)\n",
    "    rootMeanSquaredError = sqrt(mean_squared_error(y_test, y_prediction))\n",
    "    print(\"Root Mean Squared Error of \", method, \" for h =\", h, \"and #covariates =\", c, \": %.2f\" % rootMeanSquaredError)\n",
    "\n",
    "    second_error = sum(abs(y_prediction - y_test))\n",
    "    ### compute adjusted R squared error\n",
    "    SS_Residual = sum((y_test - y_prediction.reshape(-1)) ** 2)\n",
    "    SS_Total = sum((y_test - np.mean(y_test)) ** 2)\n",
    "    r_squared = 1 - (float(SS_Residual)) / SS_Total\n",
    "    adj_r_squared = 1 - (1 - r_squared) * (len(y_test) - 1) / (len(y_test) - c - 1)\n",
    "    print(\"Adjusted R Squared Error of \", method, \" for h =\", h, \"and #covariates =\", c, \": %.2f\" % adj_r_squared)\n",
    "\n",
    "\n",
    "    #################################################################################################### MASE\n",
    "\n",
    "    if  target_mode=='cumulative':\n",
    "\n",
    "        # for cumulative data form we need to change MASE error and we need new case data to calclate this error so in next lines we build new case\n",
    "        # data from cumulative data form\n",
    "        data_new_case = makeHistoricalData(h, r, target_name, 'mrmr', spatial_mode, 'regular', './')\n",
    "        if numberOfSelectedCounties == -1 :\n",
    "          numberOfSelectedCounties = len(data_new_case['county_fips'].unique())\n",
    "        data_new_case = clean_data(data_new_case, numberOfSelectedCounties)\n",
    "        reverse_dates=data_new_case['date of day t'].unique()[::-1]\n",
    "        for i,j in enumerate(reverse_dates[1:]):\n",
    "            data_new_case.loc[data_new_case['date of day t']==reverse_dates[i],target_name+' t']=list(np.array(data_new_case.loc[data_new_case['date of day t']==reverse_dates[i],target_name+' t'])-np.array(data_new_case.loc[data_new_case['date of day t']==j,target_name+' t']))\n",
    "\n",
    "        if mode == 'val':\n",
    "\n",
    "            y_test_val = y_test_date\n",
    "            X_train_train_new_case, X_train_val_new_case, X_test_new_case, y_train_train_date_new_case, y_train_val_date_new_case, y_test_date_new_case = preprocess(data_new_case, 1)\n",
    "\n",
    "            train_train_new_case=pd.concat([y_train_train_date_new_case.copy().reset_index(drop=True),X_train_train_new_case.copy().reset_index(drop=True)],axis=1)\n",
    "            train_train_template=train_train_new_case[['date of day t','county_fips']]\n",
    "            cumul_train_train_predict = train_train_template\n",
    "            cumul_train_train_predict['cumul_train_naive_predict'] = y_prediction_train.tolist()\n",
    "            cumul_train_train_predict.sort_values(by=['date of day t','county_fips'],inplace=True)\n",
    "            cumul_train_train_predict=cumul_train_train_predict.tail(numberOfSelectedCounties*r)\n",
    "            cumul_train_train_predict.sort_values(by=['county_fips','date of day t'],inplace=True)\n",
    "            for i in range(r):\n",
    "                X_train_train_new_case_temp = train_train_new_case.copy().sort_values(by=['date of day t','county_fips'])\n",
    "                if i==0:\n",
    "                  X_train_train_new_case_temp = X_train_train_new_case_temp.tail(numberOfSelectedCounties*r)\n",
    "                else:\n",
    "                  X_train_train_new_case_temp = X_train_train_new_case_temp.iloc[:-(numberOfSelectedCounties*(i)), :].tail(numberOfSelectedCounties*r)\n",
    "                X_train_train_new_case_temp.sort_values(by=['county_fips','date of day t'])\n",
    "                cumul_train_train_predict['cumul_train_naive_predict']=list(np.array(cumul_train_train_predict['cumul_train_naive_predict'])+np.array(X_train_train_new_case_temp[target_name+' t']))\n",
    "            train_val_mase_denom = pd.DataFrame(y_test_date.copy(),columns=['Target'])\n",
    "            train_val_mase_denom['cumul_train_naive_predict'] = cumul_train_train_predict['cumul_train_naive_predict'].tolist()\n",
    "            train_val_mase_denom['absolute_error'] = abs(train_val_mase_denom['Target'] - train_val_mase_denom['cumul_train_naive_predict'])\n",
    "            train_val_MASE_denominator = train_val_mase_denom['absolute_error'].mean()\n",
    "            MASE_numerator = sum(abs(y_prediction_temp - y_test_temp))/len(y_test_val)\n",
    "            MASE = MASE_numerator/train_val_MASE_denominator\n",
    "\n",
    "        if mode == 'test':\n",
    "\n",
    "            X_train_new_case, X_test_new_case, y_train_date_new_case, y_test_date_new_case = preprocess(data_new_case, 0)\n",
    "\n",
    "            train_new_case=pd.concat([y_train_date_new_case.copy().reset_index(drop=True),X_train_new_case.copy().reset_index(drop=True)],axis=1)\n",
    "            train_template=train_new_case[['date of day t','county_fips']]\n",
    "            cumul_train_predict = train_template\n",
    "            cumul_train_predict['cumul_train_naive_predict'] = y_prediction_train.tolist()\n",
    "            cumul_train_predict.sort_values(by=['date of day t','county_fips'],inplace=True)\n",
    "            cumul_train_predict=cumul_train_predict.tail(numberOfSelectedCounties*r)\n",
    "            cumul_train_predict.sort_values(by=['county_fips','date of day t'],inplace=True)\n",
    "            for i in range(r):\n",
    "                X_train_new_case_temp = train_new_case.copy().sort_values(by=['date of day t','county_fips'])\n",
    "                if i==0:\n",
    "                  X_train_new_case_temp = X_train_new_case_temp.tail(numberOfSelectedCounties*repr)\n",
    "                else:\n",
    "                  X_train_new_case_temp = X_train_new_case_temp.iloc[:-(numberOfSelectedCounties*(i)), :].tail(numberOfSelectedCounties*r)\n",
    "                X_train_new_case_temp.sort_values(by=['county_fips','date of day t'])\n",
    "                cumul_train_predict['cumul_train_naive_predict']=list(np.array(cumul_train_predict['cumul_train_naive_predict'])+np.array(X_train_new_case_temp[target_name+' t']))\n",
    "            train_test_mase_denom = y_test_date.copy()\n",
    "            train_test_mase_denom['cumul_train_naive_predict'] = cumul_train_predict['cumul_train_naive_predict'].tolist()\n",
    "            train_test_mase_denom['absolute_error'] = abs(train_test_mase_denom['Target'] - train_test_mase_denom['cumul_train_naive_predict'])\n",
    "            train_test_MASE_denominator = train_test_mase_denom['absolute_error'].mean()\n",
    "            MASE_numerator = sum(abs(y_prediction - y_test))/len(y_test)\n",
    "            MASE = MASE_numerator/train_test_MASE_denominator\n",
    "\n",
    "    else:\n",
    "        MASE_numerator = sum(abs(y_prediction_temp - y_test_temp))/len(y_test)\n",
    "        MASE = MASE_numerator/MASE_denominator\n",
    "    print(\"MASE Error of \", method, \" for h =\", h, \"and #covariates =\", c, \": %.2f\" % MASE)\n",
    "\n",
    "\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "    # save outputs in 'out.txt'\n",
    "    sys.stdout = orig_stdout\n",
    "    f.close()\n",
    "    # for the test mode we compute some additional errors, we need 'date of day t' column so we use the main dataframe\n",
    "    # we add our prediction, the difference between prediction and target ('error' column),\n",
    "    # the absolute difference between prediction and target ('absolute_error' column),\n",
    "    # the precentage of this difference (('percentage_error' column) -> we change zero targets into 1 and add 1 to their predictions),\n",
    "    # and second_error as follows and save these in 'all_errors' file\n",
    "    # then we compute the average of percentage_errors (and other values) in each day and save them in\n",
    "    # 'first_error' file\n",
    "\n",
    "    if mode == 'test':\n",
    "        # write outputs into a file\n",
    "        orig_stdout = sys.stdout\n",
    "        f = open(env_address + 'out.txt', 'a')\n",
    "        sys.stdout = f\n",
    "\n",
    "        first_error_address = test_address + 'averages_of_errors_in_each_day/'\n",
    "        all_errors_address = test_address + 'all_errors/' + str(method) + '/'\n",
    "        if not os.path.exists(first_error_address):\n",
    "            os.makedirs(first_error_address)\n",
    "        if not os.path.exists(all_errors_address):\n",
    "            os.makedirs(all_errors_address)\n",
    "        dataframe = pd.DataFrame(y_test_date, copy=True)\n",
    "        dataframe['prediction'] = y_prediction\n",
    "        dataframe['error'] = y_prediction - y_test\n",
    "        dataframe['absoulte_error'] = abs(y_prediction - y_test)\n",
    "        y_test_temp = y_test.copy()\n",
    "        y_test_temp[y_test == 0] = 1\n",
    "        y_prediction_temp = y_prediction.copy()\n",
    "        y_prediction_temp[y_test == 0] += 1\n",
    "        dataframe['percentage_error'] = ((abs(y_prediction_temp - y_test_temp)) / y_test_temp) * 100\n",
    "        second_error = (sum(dataframe['error']) / sum(y_test)) * 100\n",
    "        dataframe.to_csv(all_errors_address + 'all_errors_' + str(method) + '.csv')\n",
    "        box_violin_plot(dataframe['date of day t'], dataframe['percentage_error'], figsizes={'box': (60, 30), 'violin': (100, 50)},\n",
    "                        fontsizes={'box' : 40, 'violin': 60}, name=str(method) + '_percentage_errors_in_each_day_',\n",
    "                        address=all_errors_address)\n",
    "        box_violin_plot(dataframe['date of day t'], dataframe['error'], figsizes={'box': (20, 10), 'violin': (50, 30)},\n",
    "                        fontsizes={'box': 15, 'violin': 30}, name=str(method) + '_pure_errors_in_each_day_',\n",
    "                        address=all_errors_address)\n",
    "        dataframe['county_fips']=dataframe['county_fips'].astype(float)\n",
    "        if numberOfSelectedCounties == -1:\n",
    "          numberOfSelectedCounties = len(dataframe['county_fips'])\n",
    "        first_error = pd.DataFrame((dataframe.groupby(['date of day t']).sum() / numberOfSelectedCounties))\n",
    "        first_error.columns = ['fips','average of targets', 'average of predictions', 'average of errors',\n",
    "                               'average of absoulte_errors', 'average of percentage_errors']\n",
    "        first_error = first_error.drop(['fips'], axis=1)\n",
    "        first_error.to_csv(first_error_address + 'first_error_' + str(method) + '.csv')\n",
    "        plot_targets(method, first_error.index, first_error, first_error_address)\n",
    "\n",
    "        # save outputs in 'out.txt'\n",
    "        sys.stdout = orig_stdout\n",
    "        f.close()\n",
    "    return meanAbsoluteError, percentageOfAbsoluteError, adj_r_squared, second_error, MASE\n",
    "\n",
    "\n",
    "########################################################### push results to github\n",
    "def push(message):\n",
    "    try:\n",
    "        cmd.run(\"git pull\", check=True, shell=True)\n",
    "        print(\"everything has been pulled\")\n",
    "        cmd.run(\"git add .\", check=True, shell=True)\n",
    "        cmd.run(f\"git commit -m '{message}'\", check=True, shell=True)\n",
    "        cmd.run(\"git push\", check=True, shell=True)\n",
    "        print('pushed.')\n",
    "\n",
    "    except:\n",
    "        print('could not push')\n",
    "\n",
    "\n",
    "########################################################### zip some of the results\n",
    "def make_zip(selected_for_email, subject):\n",
    "\n",
    "    for source_root in selected_for_email:\n",
    "        for i in [x[0] for x in os.walk(source_root)]:\n",
    "            address = mail_address  + '//'+ '/'.join(i.split('/')[3:])\n",
    "            # print(address)\n",
    "            if not os.path.exists(address):\n",
    "                    os.makedirs(address)\n",
    "            for jpgfile in glob.iglob(os.path.join(i, \"*.png\")):\n",
    "                shutil.copy(jpgfile, address)\n",
    "    shutil.make_archive(subject, 'zip', mail_address)\n",
    "\n",
    "\n",
    "########################################################### mail some of the results\n",
    "def send_email(*attachments):\n",
    "    subject = \"Server results\"\n",
    "    body = \" \"\n",
    "    sender_email = \"covidserver1@gmail.com\"\n",
    "    receiver_email = [\"arezo.h1371@yahoo.com\"]#,\"arashmarioriyad@gmail.com\"\n",
    "    CC_email = []#\"p.ramazi@gmail.com\"\n",
    "    password = \"S.123456.S\"\n",
    "\n",
    "    # Create a multipart message and set headers\n",
    "    message = MIMEMultipart()\n",
    "    message[\"From\"] = sender_email\n",
    "    message[\"To\"] = ','.join(receiver_email)#receiver_email\n",
    "    message[\"Subject\"] = subject\n",
    "    message[\"CC\"] = ','.join(CC_email) # Recommended for mass emails\n",
    "\n",
    "    # Add body to email\n",
    "    message.attach(MIMEText(body, \"plain\"))\n",
    "\n",
    "    # Add attachments\n",
    "    for file_name in attachments:\n",
    "            f = open(file_name, 'rb')\n",
    "            ctype, encoding = mimetypes.guess_type(file_name)\n",
    "            if ctype is None or encoding is not None:\n",
    "                ctype = 'application/octet-stream'\n",
    "            maintype, subtype = ctype.split('/', 1)\n",
    "            # in case of a text file\n",
    "            if maintype == 'text':\n",
    "                part = MIMEText(f.read(), _subtype=subtype)\n",
    "            # any other file\n",
    "            else:\n",
    "                part = MIMEBase(maintype, subtype)\n",
    "                part.set_payload(f.read())\n",
    "            encoders.encode_base64(part)\n",
    "            part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' % os.path.basename(file_name))\n",
    "            message.attach(part)\n",
    "            f.close()\n",
    "            text = message.as_string()\n",
    "\n",
    "    # Log in to server using secure context and send email\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email+CC_email , text)\n",
    "\n",
    "\n",
    "############################################################ test process\n",
    "def test_process(h, r, target_name,spatial_mode, target_mode,best_h,best_c,historical_X_train,\\\n",
    "                 historical_X_test, historical_y_train_date, historical_y_test_date, best_loss,\\\n",
    "                 numberOfSelectedCounties, covariates_names, maxHistory, test_address, env_address, mail_address):\n",
    "\n",
    "    \n",
    "    columns_table_t = ['best_h', 'best_c', 'mean absolute error', 'percentage of absolute error', 'adjusted R squared error',\n",
    "                      'second error', 'mean absolute scaled error']\n",
    "    columns_table = ['best_h', 'best_c', 'mean absolute error', 'percentage of absolute error',\n",
    "                      'adjusted R squared error',\n",
    "                      'sum of absolute error', 'mean absolute scaled error']\n",
    "    methods = ['GBM', 'GLM', 'KNN', 'NN', 'MM_GLM', 'MM_NN']\n",
    "    none_mixed_methods = ['GBM', 'GLM', 'KNN', 'NN']\n",
    "    mixed_methods = ['MM_GLM', 'MM_NN']\n",
    "\n",
    "    train_val_MASE_denominator, val_test_MASE_denominator, train_lag_MASE_denominator = mase_denominator(r, target_name, target_mode, numberOfSelectedCounties)\n",
    "    df_for_prediction_plot = {method : None for method in methods}\n",
    "\n",
    "    all_data = makeHistoricalData(h, r, target_name, 'mrmr', spatial_mode, target_mode, './')\n",
    "    all_data = clean_data(all_data, numberOfSelectedCounties)\n",
    "    print(all_data.shape)\n",
    "    all_counties = all_data['county_fips'].unique()\n",
    "    y_prediction = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    y_prediction_train = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    # run non-mixed methods on the whole training set with their best h and c\n",
    "    X_train_dict, X_test_dict, y_train_dict, y_test_dict = {}, {}, {}, {}\n",
    "\n",
    "\n",
    "    for county_fips in all_counties:\n",
    "\n",
    "        GBM, GLM, KNN, NN = run_algorithms(historical_X_train, historical_X_test, historical_y_train_date, historical_y_test_date, best_loss, 0, spatial_mode, county_fips)\n",
    "\n",
    "        y_prediction[county_fips]['GBM'], y_prediction_train[county_fips]['GBM'] = GBM\n",
    "        y_prediction[county_fips]['GLM'], y_prediction_train[county_fips]['GLM'] = GLM\n",
    "        y_prediction[county_fips]['KNN'], y_prediction_train[county_fips]['KNN'] = KNN\n",
    "        y_prediction[county_fips]['NN'], y_prediction_train[county_fips]['NN'] = NN\n",
    "\n",
    "\n",
    "    table_data = []\n",
    "\n",
    "    for method in none_mixed_methods:\n",
    "        meanAbsoluteError, percentageOfAbsoluteError, adj_r_squared, second_error, meanAbsoluteScaledError = get_errors(best_h[method]['MAPE'],\n",
    "        best_c[method]['MAPE'], method, flatten(data=y_prediction, h=h, c=None, method=method, state=6), flatten(data=y_prediction_train, h=h, c=None, method=method, state=6), historical_y_test_date[method],\n",
    "         val_test_MASE_denominator, numberOfSelectedCounties, mode='test')\n",
    "        \n",
    "        table_data.append([best_h[method]['MAPE'], best_c[method]['MAPE'],  round(meanAbsoluteError, 2),\n",
    "                            round(percentageOfAbsoluteError, 2), round(adj_r_squared, 2), round(second_error, 2), round(meanAbsoluteScaledError, 2)])\n",
    "\n",
    "    push('a new table added')\n",
    "\n",
    "    for method in none_mixed_methods:\n",
    "      method_real_pred_df = historical_y_train_date[method].append(historical_y_test_date[method])\n",
    "      prediction=list(flatten(data=y_prediction_train, h=h, c=None, method=method, state=6))+list(flatten(data=y_prediction, h=h, c=None, method=method, state=6))\n",
    "      method_real_pred_df[method] = prediction\n",
    "      df_for_prediction_plot[method] = method_real_pred_df\n",
    "\n",
    "    # generate data for non-mixed methods with the best h and c of mixed models and fit mixed models on them\n",
    "    # (with the whole training set)\n",
    "    y_predictions = {'MM_GLM': [], 'MM_NN': []}\n",
    "    y_prediction = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    y_prediction_train = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    #table_data = []\n",
    "    X_train_MM_dict = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    X_test_MM_dict = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    y_train_MM_dict = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    y_test_MM_dict = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    y_train, y_test = {}, {}\n",
    "    y_test_date = {}\n",
    "\n",
    "    # we make mixed_model train_data in this loop\n",
    "\n",
    "    for mixed_method in mixed_methods:\n",
    "        X_train, X_test, y_train_date, y_test_date[mixed_method] = generate_data(best_h[mixed_method]['MAPE'], best_c[mixed_method]['MAPE'],\n",
    "                                                                                  covariates_names, numberOfSelectedCounties)\n",
    "        y_test_date_temp = y_test_date[mixed_method]\n",
    "        y_train[mixed_method] = y_train_date#np.array(['Target']).reshape(-1)\n",
    "        y_test[mixed_method] = y_test_date_temp# np.array(['Target']).reshape(-1)\n",
    "        mixed_model_covariates_names = list(X_train.columns)\n",
    "        X_train_to_use = {method: None for method in methods}\n",
    "        X_test_to_use = {method: None for method in methods}\n",
    "        for method in none_mixed_methods:\n",
    "            X_train_to_use[method] = X_train.copy()\n",
    "            X_test_to_use[method] = X_test.copy()\n",
    "            if method in models_to_log:\n",
    "                # make temporal and some fixed covariates logarithmic\n",
    "                negative_features=['temperature']\n",
    "                for covar in mixed_model_covariates_names:\n",
    "                    if (' t' in covar) and (covar.split(' ')[0] not in negative_features) and (covar != 'date of day t'):\n",
    "                        X_train_to_use[method][covar] = np.log((X_train_to_use[method][covar] + 1).astype(float))\n",
    "                        X_test_to_use[method][covar] = np.log((X_test_to_use[method][covar] + 1).astype(float))\n",
    "\n",
    "                fix_log_list = ['total_population', 'population_density', 'area', 'median_household_income',\n",
    "                                'houses_density', 'airport_distance','deaths_per_100000']\n",
    "                for covar in fix_log_list:\n",
    "                    if covar in mixed_model_covariates_names:\n",
    "                        X_train_to_use[method][covar] = np.log((X_train_to_use[method][covar] + 1).astype(float))\n",
    "                        X_test_to_use[method][covar] = np.log((X_test_to_use[method][covar] + 1).astype(float))\n",
    "\n",
    "            X_train_dict[method] = X_train_to_use[method]\n",
    "            X_test_dict[method] = X_test_to_use[method]\n",
    "            y_train_dict[method] = y_train[mixed_method]\n",
    "            y_test_dict[method] = y_test[mixed_method]\n",
    "\n",
    "\n",
    "        #########################################################################################\n",
    "\n",
    "        # we run mixed model for each county in this loop\n",
    "\n",
    "        # for mixed_method in mixed_methods:\n",
    "\n",
    "        for county_fips in all_counties:\n",
    "          \n",
    "            GBM, GLM, KNN, NN = run_algorithms(X_train_dict, X_test_dict, y_train_dict, y_test_dict, best_loss, 0, spatial_mode, county_fips)\n",
    "            y_prediction[county_fips]['GBM'], y_prediction_train[county_fips]['GBM'] = GBM\n",
    "            y_prediction[county_fips]['GLM'], y_prediction_train[county_fips]['GLM'] = GLM\n",
    "            y_prediction[county_fips]['KNN'], y_prediction_train[county_fips]['KNN'] = KNN\n",
    "            y_prediction[county_fips]['NN'], y_prediction_train[county_fips]['NN'] = NN\n",
    "            y_predictions_test, y_predictions_train = [], []\n",
    "            # Construct the outputs for the testing dataset of the 'MM' methods\n",
    "            y_predictions_test.extend([y_prediction[county_fips]['GBM'], y_prediction[county_fips]['GLM'], y_prediction[county_fips]['KNN'], y_prediction[county_fips]['NN']])\n",
    "            y_prediction_test_np = np.array(y_predictions_test).reshape(len(y_predictions_test), -1)\n",
    "            X_test_mixedModel = pd.DataFrame(y_prediction_test_np.transpose())\n",
    "            # Construct the outputs for the training dataset of the 'MM' methods\n",
    "            y_predictions_train.extend([y_prediction_train[county_fips]['GBM'], y_prediction_train[county_fips]['GLM'], y_prediction_train[county_fips]['KNN'], y_prediction_train[county_fips]['NN']])\n",
    "            y_prediction_train_np = np.array(y_predictions_train).reshape(len(y_predictions_train), -1)\n",
    "            X_train_mixedModel = pd.DataFrame(y_prediction_train_np.transpose())\n",
    "            X_train_MM_dict[county_fips][mixed_method] = X_train_mixedModel\n",
    "            X_test_MM_dict[county_fips][mixed_method] = X_test_mixedModel\n",
    "            y_train_MM_dict[county_fips][mixed_method] = y_train[mixed_method][y_train[mixed_method]['county_fips']==county_fips]\n",
    "            y_test_MM_dict[county_fips][mixed_method] = y_test[mixed_method][y_test[mixed_method]['county_fips']==county_fips]\n",
    "\n",
    "\n",
    "            y_test_MM_dict[county_fips][mixed_method] = np.array(y_test_MM_dict[county_fips][mixed_method]['Target']).reshape(-1)\n",
    "            y_train_MM_dict[county_fips][mixed_method] = np.array(y_train_MM_dict[county_fips][mixed_method]['Target']).reshape(-1)\n",
    "\n",
    "    for county_fips in all_counties:\n",
    "\n",
    "        # mixed model with linear regression and neural network\n",
    "        MM_GLM, MM_NN = run_mixed_models(X_train_MM_dict[county_fips], X_test_MM_dict[county_fips], y_train_MM_dict[county_fips], y_test_MM_dict[county_fips] ,best_loss)\n",
    "        y_prediction[county_fips]['MM_GLM'], y_prediction_train[county_fips]['MM_GLM'] = MM_GLM\n",
    "        y_prediction[county_fips]['MM_NN'], y_prediction_train[county_fips]['MM_NN'] = MM_NN\n",
    "\n",
    "    # save the entire session\n",
    "    filename = env_address + 'test.out'\n",
    "    my_shelf = shelve.open(filename, 'n')  # 'n' for new\n",
    "    for key in dir():\n",
    "        try:\n",
    "            my_shelf[key] = locals()[key]\n",
    "        except:\n",
    "            print('ERROR shelving: {0}'.format(key))\n",
    "    my_shelf.close()\n",
    "\n",
    "    ############################################################################################\n",
    "    for mixed_method in mixed_methods:\n",
    "        meanAbsoluteError, percentageOfAbsoluteError, adj_r_squared, second_error, meanAbsoluteScaledError = get_errors(best_h[mixed_method]['MAPE'],\n",
    "        best_c[mixed_method]['MAPE'], mixed_method, flatten(data=y_prediction, h=h, c=None, method=mixed_method, state=6), flatten(data=y_prediction_train, h=h, c=None, method=mixed_method, state=6), y_test_date[mixed_method],\n",
    "                                    val_test_MASE_denominator, numberOfSelectedCounties, mode='test')\n",
    "        table_data.append([best_h[mixed_method]['MAPE'], best_c[mixed_method]['MAPE'], round(meanAbsoluteError, 2), round(percentageOfAbsoluteError, 2),\n",
    "                            round(adj_r_squared, 2), round(second_error, 2), round(meanAbsoluteScaledError, 2)])\n",
    "\n",
    "    table_name = 'table_of_best_test_results'\n",
    "    plot_table(table_data, columns_table_t, methods, table_name, mode='test')\n",
    "    # push('a new table added')\n",
    "\n",
    "    for method in mixed_methods:\n",
    "      method_real_pred_df=y_train[method].append(y_test[method])\n",
    "      prediction=list(flatten(data=y_prediction_train, h=h, c=None, method=method, state=6))+list(flatten(data=y_prediction, h=h, c=None, method=method, state=6))\n",
    "      method_real_pred_df[method] = prediction\n",
    "      df_for_prediction_plot[method] = method_real_pred_df\n",
    "\n",
    "    real_prediction_plot(df_for_prediction_plot,r,target_name,best_h,spatial_mode,methods, numberOfSelectedCounties)\n",
    "\n",
    "    # mail the test results\n",
    "    selected_for_email = [test_address + '/tables', test_address + '/all_errors/NN', test_address + '/all_errors/KNN' , test_address + '/plots_of_real_prediction_values']\n",
    "    zip_file_name = 'test results for h =' + str(maxHistory) + ' #counties=' + str(numberOfSelectedCountiesname)\n",
    "    make_zip(selected_for_email, zip_file_name)\n",
    "    send_email(zip_file_name + '.zip')\n",
    "\n",
    "    # save the entire session\n",
    "    filename = env_address + 'test.out'\n",
    "    my_shelf = shelve.open(filename, 'n')  # 'n' for new\n",
    "    for key in dir():\n",
    "        try:\n",
    "            my_shelf[key] = locals()[key]\n",
    "        except:\n",
    "            print('ERROR shelving: {0}'.format(key))\n",
    "    my_shelf.close()\n",
    "\n",
    "\n",
    "########################################################## flatten\n",
    "def flatten(data=None, h=None, c=None, method=None, covariates_list=None, state=1):\n",
    "    if state == 1:\n",
    "        result = []\n",
    "        for county_fips in data:\n",
    "            result += list(data[county_fips][method][(h, c)])\n",
    "    elif state == 2:\n",
    "        result = []\n",
    "        for county_fips in data:\n",
    "            result += list(data[county_fips][(h, c)])\n",
    "    elif state == 3:\n",
    "        result = pd.DataFrame(columns=covariates_list)\n",
    "        for county_fips in data:\n",
    "            result = pd.concat([result, data[county_fips][h][method][covariates_list]], ignore_index=True)\n",
    "    elif state == 4:\n",
    "        for county_fips in data:\n",
    "            result = pd.DataFrame(columns=data[county_fips].columns.values)\n",
    "            break\n",
    "        for county_fips in data:\n",
    "            result = pd.concat([result, data[county_fips]], ignore_index=True)\n",
    "    elif state == 5:\n",
    "        result = []\n",
    "        for county_fips in data:\n",
    "            result += list(data[county_fips])\n",
    "        result = np.array(result)\n",
    "    elif state == 6:\n",
    "        result = []\n",
    "        for county_fips in data:\n",
    "            result += list(data[county_fips][method])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################### main\n",
    "def main(maxHistory, maxC):\n",
    "    history = [i for i in range(1, maxHistory + 1)]\n",
    "    methods = ['GBM', 'GLM', 'KNN', 'NN', 'MM_GLM', 'MM_NN']\n",
    "    none_mixed_methods = ['GBM', 'GLM', 'KNN', 'NN']\n",
    "    mixed_methods = ['MM_GLM', 'MM_NN']\n",
    "    target_name = 'confirmed'\n",
    "    base_data = makeHistoricalData(0, r, target_name, 'mrmr', spatial_mode, target_mode, './')\n",
    "    base_data = clean_data(base_data, numberOfSelectedCounties)\n",
    "    covariates_names = list(base_data.columns)\n",
    "    covariates_names.remove('Target')\n",
    "    covariates_names.remove('date of day t')\n",
    "    covariates_names.remove('county_fips')\n",
    "    numberOfCovariates = len(covariates_names)\n",
    "    y_prediction = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in base_data['county_fips'].unique()}\n",
    "    y_prediction_train = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in base_data['county_fips'].unique()}\n",
    "    y_val = {county_fips: {}\n",
    "                    for county_fips in base_data['county_fips'].unique()}\n",
    "    error_names = ['MAPE', 'MAE', 'adj-R2', 'sec', 'MASE']\n",
    "    complete_error_names = {'MAPE': 'Percentage Of Absolute Error', 'MAE': 'Mean Absolute Error',\n",
    "                            'MASE': 'Mean Absolute Scaled Error', 'adj-R2': 'Adjusted R Squared Error',\n",
    "                            'sec': 'Sum Of Absolute Error'}\n",
    "    validation_errors = {error: {method: {} for method in methods} for error in error_names}\n",
    "    minError = {method: {error: int(1e10) for error in error_names} for method in methods}\n",
    "    best_h = {method: {error: 0 for error in error_names} for method in methods}\n",
    "    best_c = {method: {error: 0 for error in error_names} for method in methods}\n",
    "    # best_loss = {'GBM': 'poisson', 'MM_NN': 'poisson', 'NN': 'MeanAbsoluteError'}\n",
    "    best_loss = {method: None for method in ['GBM', 'NN', 'MM_NN']}\n",
    "    counties_best_loss_list = {method: list() for method in ['GBM', 'NN', 'MM_NN']}\n",
    "    df_for_prediction_plot = pd.DataFrame(columns = methods)\n",
    "    columns_table_t = ['best_h', 'best_c', 'mean absolute error', 'percentage of absolute error', 'adjusted R squared error',\n",
    "                      'second error', 'mean absolute scaled error']\n",
    "    columns_table = ['best_h', 'best_c', 'mean absolute error', 'percentage of absolute error',\n",
    "                      'adjusted R squared error',\n",
    "                      'sum of absolute error', 'mean absolute scaled error']\n",
    "    historical_X_train = {}\n",
    "    historical_X_test = {}\n",
    "    historical_y_train = {}\n",
    "    historical_y_test = {}\n",
    "    historical_y_train_date = {}\n",
    "    historical_y_test_date = {}\n",
    "    X_train_train_to_use = {county_fips: {h: {method: None for method in methods} for h in history} for county_fips in base_data['county_fips'].unique()}\n",
    "    X_train_val_to_use = {county_fips: {h: {method: None for method in methods} for h in history} for county_fips in base_data['county_fips'].unique()}\n",
    "    X_test_to_use = {county_fips: {h: {method: None for method in methods} for h in history} for county_fips in base_data['county_fips'].unique()}\n",
    "    train_val_MASE_denominator, val_test_MASE_denominator, train_lag_MASE_denominator = mase_denominator(r, target_name, target_mode, numberOfSelectedCounties)\n",
    "    \n",
    "    for h in history:\n",
    "        print(\"h = \", h)\n",
    "        all_data = makeHistoricalData(h, r, target_name, 'mrmr', spatial_mode, target_mode, './')\n",
    "        all_data = clean_data(all_data, numberOfSelectedCounties)\n",
    "        print(all_data.shape)\n",
    "        if (all_data.shape[0] < 1) :\n",
    "            history = history[:h-1]\n",
    "            break\n",
    "        all_counties = all_data['county_fips'].unique()\n",
    "        y_test_date = {county_fips: None for county_fips in all_counties}\n",
    "        y_train_date = {county_fips: None for county_fips in all_counties}\n",
    "        y_train = {county_fips: None for county_fips in all_counties}\n",
    "        y_test = {county_fips: None for county_fips in all_counties}\n",
    "        for county_fips in all_counties:\n",
    "            print(\"county_fips = \", county_fips)\n",
    "            data = all_data[all_data['county_fips']==county_fips]\n",
    "            print(data.shape)\n",
    "            parallel_outputs = {}\n",
    "            X_train_train, X_train_val, X_test, y_train_train_date, y_train_val_date, y_test_date[county_fips] = preprocess(data, spatial_mode, 1)\n",
    "            indx_c = 0\n",
    "            for c in covariates_names:\n",
    "                indx_c += 1\n",
    "                y_val[county_fips][(h, indx_c)] = np.array(y_train_val_date['Target']).reshape(-1)\n",
    "                if indx_c == maxC:\n",
    "                    break\n",
    "            for method in methods:\n",
    "                X_train_train_to_use[county_fips][h][method] = X_train_train.copy()\n",
    "                X_train_val_to_use[county_fips][h][method]= X_train_val.copy()\n",
    "                X_test_to_use[county_fips][h][method] = X_test.copy()\n",
    "                if method in models_to_log:\n",
    "                    negative_features=['temperature']\n",
    "                    for covar in covariates_names:\n",
    "                        if (' t' in covar) and (covar.split(' ')[0] not in negative_features):\n",
    "                            X_train_train_to_use[county_fips][h][method][covar] = np.log((X_train_train_to_use[county_fips][h][method][covar] + 1).astype(float))\n",
    "                            X_train_val_to_use[county_fips][h][method][covar] = np.log((X_train_val_to_use[county_fips][h][method][covar] + 1).astype(float))\n",
    "                            X_test_to_use[county_fips][h][method][covar] = np.log((X_test_to_use[county_fips][h][method][covar] + 1).astype(float))\n",
    "                    fix_log_list = ['total_population', 'population_density', 'area', 'median_household_income',\n",
    "                                    'houses_density', 'airport_distance','deaths_per_100000']\n",
    "                    for covar in fix_log_list:\n",
    "                        if covar in covariates_names:\n",
    "                            X_train_train_to_use[county_fips][h][method][covar] = np.log((X_train_train_to_use[county_fips][h][method][covar] + 1).astype(float))\n",
    "                            X_train_val_to_use[county_fips][h][method][covar] = np.log((X_train_val_to_use[county_fips][h][method][covar] + 1).astype(float))\n",
    "                            X_test_to_use[county_fips][h][method][covar] = np.log((X_test_to_use[county_fips][h][method][covar] + 1).astype(float))\n",
    "            \n",
    "            y_train_date[county_fips] = (pd.DataFrame(y_train_train_date).append(pd.DataFrame(y_train_val_date))).reset_index(drop=True)\n",
    "            y_train_train = np.array(y_train_train_date['Target']).reshape(-1)\n",
    "            y_train_val = np.array(y_train_val_date['Target']).reshape(-1)\n",
    "            y_test[county_fips] = np.array(y_test_date[county_fips]['Target']).reshape(-1)\n",
    "            y_train[county_fips] = np.array((pd.DataFrame(y_train_train).append(pd.DataFrame(y_train_val))).reset_index(drop=True)).reshape(-1)\n",
    "\n",
    "            # find best loss\n",
    "            if (h==1):\n",
    "              best_loss = update_best_loss('none_mixed_model', spatial_mode ,county_fips,best_loss,X_train_train_to_use,X_train_val_to_use,\\\n",
    "                                          y_train_train,y_train_val,None,None,data.columns.drop(['Target','date of day t','county_fips']),\\\n",
    "                                            numberOfCovariates,maxC)\n",
    "              # update list of county losses (mode of this list will be used as best loss)\n",
    "              for method in ['GBM', 'NN']:\n",
    "                counties_best_loss_list[method].append(best_loss[method])\n",
    "\n",
    "\n",
    "            covariates_list = []\n",
    "            loom = ProcessLoom(max_runner_cap=len(base_data.columns) * len(none_mixed_methods) + 5)\n",
    "            indx_c = 0\n",
    "            for c in covariates_names:\n",
    "                indx_c += 1\n",
    "                for covariate in data.columns:\n",
    "                    if c.split(' ')[0] in covariate:\n",
    "                        covariates_list.append(covariate)\n",
    "                for method in none_mixed_methods:\n",
    "                    X_train_train_temp = X_train_train_to_use[county_fips][h][method][covariates_list]\n",
    "                    X_train_val_temp = X_train_val_to_use[county_fips][h][method][covariates_list]\n",
    "                    loom.add_function(parallel_run, [method, X_train_train_temp, X_train_val_temp, y_train_train, y_train_val, best_loss, indx_c])\n",
    "                if indx_c == maxC:\n",
    "                    break\n",
    "            parallel_outputs['non_mixed'] = loom.execute()\n",
    "            ind = 0\n",
    "            for c in range(1, numberOfCovariates + 1):\n",
    "                for method in none_mixed_methods:\n",
    "                    y_prediction[county_fips][method][(h, c)], y_prediction_train[county_fips][method][(h, c)] = parallel_outputs['non_mixed'][ind]['output']\n",
    "                    ind += 1\n",
    "                if c == maxC:\n",
    "                    break\n",
    "            filename = env_address + 'validation.out'\n",
    "            my_shelf = shelve.open(filename, 'n')\n",
    "            for key in dir():\n",
    "                try:\n",
    "                    my_shelf[key] = locals()[key]\n",
    "                except:\n",
    "                    print('ERROR shelving: {0}'.format(key))\n",
    "            my_shelf.close()\n",
    "\n",
    "            # find best loss\n",
    "            if h == 1 :\n",
    "              best_loss = update_best_loss('mixed_model', spatial_mode, county_fips,best_loss,None,None,y_train_train,\\\n",
    "                        y_train_val,y_prediction_train,y_prediction,None,\\\n",
    "                        numberOfCovariates,maxC)\n",
    "              # update list of county losses (mode of this list will be used as best loss)\n",
    "              counties_best_loss_list['MM_NN'].append(best_loss['MM_NN'])\n",
    "\n",
    "            loom = ProcessLoom(max_runner_cap=len(base_data.columns) * len(mixed_methods) + 5)\n",
    "            indx_c = 0\n",
    "            for c in range(1, numberOfCovariates + 1):\n",
    "                indx_c += 1\n",
    "                for mixed_method in mixed_methods:\n",
    "                    y_predictions_test, y_predictions_train = [], []\n",
    "                    y_predictions_test.extend([y_prediction[county_fips]['GBM'][(h, c)], y_prediction[county_fips]['GLM'][(h, c)],\n",
    "                                                y_prediction[county_fips]['KNN'][(h, c)], y_prediction[county_fips]['NN'][(h, c)]])\n",
    "                    y_prediction_test_np = np.array(y_predictions_test).reshape(len(y_predictions_test), -1)\n",
    "                    X_test_mixedModel = pd.DataFrame(y_prediction_test_np.transpose())\n",
    "                    y_predictions_train.extend([y_prediction_train[county_fips]['GBM'][(h, c)], y_prediction_train[county_fips]['GLM'][(h, c)],\n",
    "                                                y_prediction_train[county_fips]['KNN'][(h, c)], y_prediction_train[county_fips]['NN'][(h, c)]])\n",
    "                    y_prediction_train_np = np.array(y_predictions_train).reshape(len(y_predictions_train), -1)\n",
    "                    X_train_mixedModel = pd.DataFrame(y_prediction_train_np.transpose())\n",
    "                    loom.add_function(mixed_parallel_run, [mixed_method, X_train_mixedModel, X_test_mixedModel, y_train_train, y_train_val, best_loss])\n",
    "                if indx_c == maxC:\n",
    "                    break\n",
    "            parallel_outputs['mixed'] = loom.execute()\n",
    "            ind = 0\n",
    "            for c in range(1, numberOfCovariates + 1):\n",
    "                for mixed_method in mixed_methods:\n",
    "                    y_prediction[county_fips][mixed_method][(h, c)], y_prediction_train[county_fips][mixed_method][(h, c)] = parallel_outputs['mixed'][ind]['output']\n",
    "                    y_prediction[county_fips][mixed_method][(h, c)] = np.array(y_prediction[county_fips][mixed_method][(h, c)]).ravel()\n",
    "                    y_prediction_train[county_fips][mixed_method][(h, c)] = np.array(y_prediction_train[county_fips][mixed_method][(h, c)]).ravel()\n",
    "                    ind += 1\n",
    "                if c == maxC:\n",
    "                    break\n",
    "            filename = env_address + 'validation.out'\n",
    "            my_shelf = shelve.open(filename, 'n')\n",
    "            for key in dir():\n",
    "                try:\n",
    "                    my_shelf[key] = locals()[key]\n",
    "                except:\n",
    "                    print('ERROR shelving: {0}'.format(key))\n",
    "            my_shelf.close()\n",
    "        print(\"########################################################################################################\")\n",
    "        number_of_improved_methods = 0\n",
    "        indx_c = 0\n",
    "        covariates_list=['county_fips','date of day t']\n",
    "        for c in covariates_names:\n",
    "            print(indx_c)\n",
    "            indx_c += 1\n",
    "            for covariate in data.columns:\n",
    "                if c.split(' ')[0] in covariate:\n",
    "                    covariates_list.append(covariate)\n",
    "            for method in methods:\n",
    "                X_train_train_temp = flatten(data=X_train_train_to_use, h=h, method=method, covariates_list=covariates_list, state=3)\n",
    "                X_train_val_temp = flatten(data=X_train_val_to_use, h=h, method=method, covariates_list=covariates_list, state=3)\n",
    "                X_test_temp = flatten(data=X_test_to_use, h=h, method=method, covariates_list=covariates_list, state=3)\n",
    "                validation_errors['MAE'][method][(h, indx_c)], validation_errors['MAPE'][method][(h, indx_c)], \\\n",
    "                validation_errors['adj-R2'][method][(h, indx_c)], validation_errors['sec'][method][(h, indx_c)], \\\n",
    "                validation_errors['MASE'][method][(h, indx_c)] = \\\n",
    "                    get_errors(h, indx_c, method, flatten(data=y_prediction, h=h, c=indx_c, method=method, state=1), flatten(data=y_prediction_train, h=h, c=indx_c, method=method, state=1), flatten(data=y_val, h=h, c=indx_c, state=2),\n",
    "                                train_val_MASE_denominator, numberOfSelectedCounties, mode='val')\n",
    "                for error in error_names:\n",
    "                    if validation_errors[error][method][(h, indx_c)] < minError[method][error]:\n",
    "                        minError[method][error] = validation_errors[error][method][(h, indx_c)]\n",
    "                        best_h[method][error] = h\n",
    "                        best_c[method][error] = indx_c\n",
    "                        if error == 'MAPE':\n",
    "                            number_of_improved_methods += 1\n",
    "                            print(method+' improved')\n",
    "                        if error == 'MAPE' and method != 'MM_GLM' and method != 'MM_NN':\n",
    "                            historical_X_train[method] = (X_train_train_temp.append(X_train_val_temp)).reset_index(\n",
    "                                drop=True)\n",
    "                            historical_X_test[method] = X_test_temp\n",
    "                            historical_y_train[method] = flatten(data=y_train, state=5)\n",
    "                            historical_y_test[method] = flatten(data=y_test, state=5)\n",
    "                            historical_y_train_date[method] = flatten(data=y_train_date, state=4)\n",
    "                            historical_y_test_date[method] = flatten(data=y_test_date, state=4)\n",
    "            filename = env_address + 'validation.out'\n",
    "            my_shelf = shelve.open(filename, 'n')\n",
    "            for key in dir():\n",
    "                try:\n",
    "                    my_shelf[key] = locals()[key]\n",
    "                except:\n",
    "                    print('ERROR shelving: {0}'.format(key))\n",
    "            my_shelf.close()\n",
    "            if indx_c == maxC:\n",
    "                break\n",
    "        if h == 1:\n",
    "          best_loss = get_best_loss_mode(counties_best_loss_list)\n",
    "        \n",
    "        filename = env_address + 'validation.out'\n",
    "        my_shelf = shelve.open(filename, 'n')  # 'n' for new\n",
    "        for key in dir():\n",
    "            try:\n",
    "                my_shelf[key] = locals()[key]\n",
    "            except:\n",
    "                print('ERROR shelving: {0}'.format(key))\n",
    "        my_shelf.close()\n",
    "        push('logs of h=' + str(h) + ' added')\n",
    "        if (number_of_improved_methods == 0) or (h == maxHistory//2) :\n",
    "          print('number of improved methods for h=',h,':',number_of_improved_methods)\n",
    "          print('jump to test process')\n",
    "          test_process(h, r, target_name,spatial_mode, target_mode,best_h,best_c,historical_X_train,\\\n",
    "                 historical_X_test, historical_y_train_date, historical_y_test_date, best_loss,\\\n",
    "                 numberOfSelectedCounties, covariates_names, maxHistory, test_address, env_address, mail_address)\n",
    "\n",
    "\n",
    "\n",
    "    # plot table for best results\n",
    "    table_data = []\n",
    "    for method in methods:\n",
    "        table_data.append([best_h[method]['MAPE'], best_c[method]['MAPE'], round(minError[method]['MAE'], 2),\n",
    "                            round(minError[method]['MAPE'], 2), round(minError[method]['adj-R2'], 2),\n",
    "                            round(minError[method]['sec'], 2), round(minError[method]['MASE'], 2)])\n",
    "    table_name = 'tabel_of_best_validation_results'\n",
    "    plot_table(table_data, columns_table, methods, table_name, mode='val')\n",
    "    # plot the results of methods on validation set\n",
    "\n",
    "    for error in error_names:\n",
    "        plot_results(3, 2, numberOfCovariates, methods, history, validation_errors[error], complete_error_names[error])\n",
    "\n",
    "    # mail the validation results\n",
    "    selected_for_email = [validation_address]\n",
    "    zip_file_name = 'validation results for h =' + str(maxHistory) + ' #counties=' + str(numberOfSelectedCountiesname)\n",
    "    make_zip(selected_for_email, zip_file_name)\n",
    "    send_email(zip_file_name + '.zip')\n",
    "    push('plots added')\n",
    "    test_process(h, r, target_name,spatial_mode, target_mode,best_h,best_c,historical_X_train,\\\n",
    "                 historical_X_test, historical_y_train_date, historical_y_test_date, best_loss,\\\n",
    "                 numberOfSelectedCounties, covariates_names, maxHistory, test_address, env_address, mail_address)\n",
    "\n",
    "    print(\"y_prediction\", y_prediction)\n",
    "    print(\"y_val\", y_val)\n",
    "    print(validation_errors)\n",
    "    print(\"best_h\", best_h)\n",
    "    print(\"best_c\", best_c)\n",
    "    print(historical_X_train['GBM'].columns.values)\n",
    "    print(historical_X_train['GBM'].shape)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    begin = time.time()\n",
    "    maxHistory = 5\n",
    "    maxC = 2\n",
    "    validation_address = './'+str(iteration)+' results/counties=' + str(numberOfSelectedCountiesname) + ' max_history=' + str(maxHistory) + '/validation/'\n",
    "    test_address = './' +str(iteration)+ ' results/counties=' + str(numberOfSelectedCountiesname) + ' max_history=' + str(maxHistory) + '/test/'\n",
    "    env_address = './' +str(iteration)+ ' results/counties=' + str(numberOfSelectedCountiesname) + ' max_history=' + str(maxHistory) + '/session_parameters/'\n",
    "    mail_address = './'+str(iteration)+' results/counties=' + str(numberOfSelectedCountiesname) + ' max_history=' + str(maxHistory) + '/email/'\n",
    "    if not os.path.exists(mail_address):\n",
    "        os.makedirs(mail_address)\n",
    "    if not os.path.exists(test_address):\n",
    "        os.makedirs(test_address)\n",
    "    if not os.path.exists(validation_address):\n",
    "        os.makedirs(validation_address)\n",
    "    if not os.path.exists(env_address):\n",
    "        os.makedirs(env_address)\n",
    "    push('new folders added')\n",
    "    models_to_log = ['NN', 'GLM', 'GBM']\n",
    "    main(maxHistory, maxC)\n",
    "    end = time.time()\n",
    "    push('final results added')\n",
    "    print(\"The total time of execution in minutes: \", round((end - begin) / 60, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shit way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from makeHistoricalData import makeHistoricalData\n",
    "from models import GBM, GLM, KNN, NN, MM_GLM, GBM_grid_search, NN_grid_search\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from matplotlib import colors as mcolors\n",
    "from pexecute.process import ProcessLoom\n",
    "import time\n",
    "from sys import argv\n",
    "import sys\n",
    "from math import floor, sqrt\n",
    "import os\n",
    "#import dill\n",
    "import glob\n",
    "import shutil\n",
    "import zipfile\n",
    "import email, smtplib, ssl\n",
    "from email import encoders\n",
    "from email.mime.base import MIMEBase\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "import mimetypes\n",
    "import subprocess as cmd\n",
    "import shelve\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "import statistics\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "r = 21  # the following day to predict\n",
    "numberOfSelectedCounties = -1\n",
    "target_mode = 'regular'\n",
    "spatial_mode = 'county'\n",
    "numberOfSelectedCountiesname = 1535\n",
    "iteration = 20\n",
    "\n",
    "######################################################### split data to train, val, test\n",
    "def splitData(numberOfCounties, main_data, target, spatial_mode, mode ):\n",
    "\n",
    "    numberOfCounties = len(main_data['county_fips'].unique())\n",
    "\n",
    "    if mode == 'val':\n",
    "      main_data = main_data.sort_values(by=['date of day t' , 'county_fips'])\n",
    "      target = target.sort_values(by=['date of day t' , 'county_fips'])\n",
    "      X_train_train = main_data.iloc[:-2*(r*numberOfCounties),:].sort_values(by=['county_fips' , 'date of day t'])\n",
    "      # X_train_train = X_train_train.drop(['date of day t', 'county_fips'], axis=1)\n",
    "      X_train_val = main_data.iloc[-2*(r*numberOfCounties):-(r*numberOfCounties),:].sort_values(by=['county_fips' , 'date of day t'])\n",
    "      # X_train_val = X_train_val.drop(['date of day t', 'county_fips'], axis=1)\n",
    "      X_test = main_data.tail(r*numberOfCounties).sort_values(by=['county_fips' , 'date of day t'])\n",
    "      # X_test = X_test.drop(['date of day t', 'county_fips'], axis=1)\n",
    "\n",
    "      y_train_train = target.iloc[:-2*(r*numberOfCounties),:].sort_values(by=['county_fips' , 'date of day t'])\n",
    "      y_train_val = target.iloc[-2*(r*numberOfCounties):-(r*numberOfCounties),:].sort_values(by=['county_fips' , 'date of day t'])\n",
    "      y_test = target.tail(r*numberOfCounties).sort_values(by=['county_fips' , 'date of day t'])\n",
    "\n",
    "      return X_train_train , X_train_val , X_test , y_train_train , y_train_val , y_test\n",
    "\n",
    "    if mode == 'test':\n",
    "      main_data = main_data.sort_values(by=['date of day t' , 'county_fips'])\n",
    "      target = target.sort_values(by=['date of day t' , 'county_fips'])\n",
    "\n",
    "      X_train = main_data.iloc[:-(r*numberOfCounties),:].sort_values(by=['county_fips' , 'date of day t'])\n",
    "      # X_train = X_train.drop(['date of day t', 'county_fips'], axis=1)\n",
    "      X_test = main_data.tail(r*numberOfCounties).sort_values(by=['county_fips' , 'date of day t'])\n",
    "      # X_test = X_test.drop(['date of day t', 'county_fips'], axis=1)\n",
    "\n",
    "      y_train = target.iloc[:-(r*numberOfCounties),:].sort_values(by=['county_fips' , 'date of day t'])\n",
    "      y_test = target.tail(r*numberOfCounties).sort_values(by=['county_fips' , 'date of day t'])\n",
    "\n",
    "      return X_train , X_test , y_train , y_test\n",
    "\n",
    "\n",
    "\n",
    "########################################################### clean data\n",
    "def clean_data(data, numberOfSelectedCounties):\n",
    "    global numberOfDays\n",
    "    data = data.sort_values(by=['county_fips', 'date of day t'])\n",
    "    # select the number of counties we want to use\n",
    "    # numberOfSelectedCounties = numberOfCounties\n",
    "    if numberOfSelectedCounties == -1:\n",
    "        numberOfSelectedCounties = len(data['county_fips'].unique())\n",
    "\n",
    "    using_data = data[(data['county_fips'] <= data['county_fips'].unique()[numberOfSelectedCounties - 1])]\n",
    "    using_data = using_data.reset_index(drop=True)\n",
    "    main_data = using_data.drop(['county_name', 'state_fips', 'state_name'],\n",
    "                                axis=1)  # , 'date of day t'\n",
    "    # target = pd.DataFrame(main_data['Target'])\n",
    "    # main_data = main_data.drop(['Target'], axis=1)\n",
    "    # numberOfCounties = len(using_data['county_fips'].unique())\n",
    "    numberOfDays = len(using_data['date of day t'].unique())\n",
    "\n",
    "    return main_data\n",
    "\n",
    "\n",
    "########################################################### preprocess\n",
    "def preprocess(main_data, spatial_mode, validationFlag):\n",
    "\n",
    "    target = pd.DataFrame(main_data[['date of day t', 'county_fips', 'Target']])\n",
    "    main_data = main_data.drop(['Target'], axis=1)\n",
    "    # specify the size of train, validation and test sets\n",
    "    t1 = time.time()\n",
    "    # produce train, validation and test data in parallel\n",
    "\n",
    "    if validationFlag:     # validationFlag is 1 if we want to have a validation set and 0 otherwise\n",
    "        # add the functions to the multiprocessing object, loom\n",
    "\n",
    "        X_train_train , X_train_val , X_test , y_train_train , y_train_val , y_test = splitData(numberOfSelectedCounties, main_data, target, spatial_mode,'val')\n",
    "        return X_train_train, X_train_val, X_test, y_train_train, y_train_val, y_test\n",
    "\n",
    "    else:\n",
    "\n",
    "        X_train , X_test , y_train , y_test = splitData(numberOfSelectedCounties, main_data, target, spatial_mode,'test')\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################ MASE_denominator\n",
    "def mase_denominator(r, target_name, target_mode ,numberOfSelectedCounties):\n",
    "\n",
    "    data = makeHistoricalData(0, r, target_name, 'mrmr', 'country', target_mode, './', iteration)\n",
    "    if numberOfSelectedCounties == -1 :\n",
    "      numberOfSelectedCounties = len(data['county_fips'].unique())\n",
    "    data = clean_data(data, numberOfSelectedCounties)\n",
    "    X_train_train, X_train_val, X_test, y_train_train_date, y_train_val_date, y_test_date = preprocess(data, 'country', 1)\n",
    "\n",
    "    train_train = (y_train_train_date.reset_index(drop=True)).sort_values(by=['date of day t', 'county_fips'])\n",
    "    train_val = (y_train_val_date.reset_index(drop=True)).sort_values(by=['date of day t', 'county_fips'])\n",
    "    test = (y_test_date.reset_index(drop=True)).sort_values(by=['date of day t', 'county_fips'])\n",
    "\n",
    "    train_lag = train_train.copy().iloc[:-(numberOfSelectedCounties*r), :].tail(numberOfSelectedCounties*r).rename(\n",
    "        columns={'Target': 'train-lag-Target'})\n",
    "    train_train = train_train.tail(numberOfSelectedCounties*r).rename(columns={'Target': 'train-Target'}) #[['train-Target']]\n",
    "    train_val = train_val.tail(numberOfSelectedCounties*r).rename(columns={'Target': 'val-Target'}) #[['val-Target']]\n",
    "    test = test.tail(numberOfSelectedCounties*r).rename(columns={'Target': 'test-Target'}) #[['test-Target']]\n",
    "\n",
    "    df_for_train_lag_MASE_denominator=pd.concat([train_train.reset_index(drop=True), train_lag.reset_index(drop=True)], axis=1)\n",
    "    #[['county_fips','date of day t','Target']]\n",
    "    df_for_train_lag_MASE_denominator['absolute-error']=abs(df_for_train_lag_MASE_denominator['train-Target'] -\n",
    "                                                            df_for_train_lag_MASE_denominator['train-lag-Target'])\n",
    "\n",
    "    df_for_train_val_MASE_denominator = pd.concat([train_train.reset_index(drop=True),train_val.reset_index(drop=True)], axis=1)\n",
    "    #[['county_fips','date of day t','Target']]\n",
    "    df_for_train_val_MASE_denominator['absolute-error']=abs(df_for_train_val_MASE_denominator['val-Target'] -\n",
    "                                                            df_for_train_val_MASE_denominator['train-Target'])\n",
    "\n",
    "    df_for_val_test_MASE_denominator=pd.concat([train_val.reset_index(drop=True),test.reset_index(drop=True)], axis=1)\n",
    "    #[['county_fips','date of day t','Target']]\n",
    "    df_for_val_test_MASE_denominator['absolute-error']=abs(df_for_val_test_MASE_denominator['test-Target'] -\n",
    "                                                           df_for_val_test_MASE_denominator['val-Target'])\n",
    "\n",
    "    train_val_MASE_denominator = df_for_train_val_MASE_denominator['absolute-error'].mean()\n",
    "    val_test_MASE_denominator = df_for_val_test_MASE_denominator['absolute-error'].mean()\n",
    "    train_lag_MASE_denominator = df_for_train_lag_MASE_denominator['absolute-error'].mean()\n",
    "\n",
    "    return train_val_MASE_denominator, val_test_MASE_denominator, train_lag_MASE_denominator\n",
    "\n",
    "\n",
    "########################################################### run non-mixed methods in parallel\n",
    "def parallel_run(method, X_train_train, X_train_val, y_train_train, y_train_val, best_loss, c):\n",
    "\n",
    "    y_prediction, y_prediction_train = None, None\n",
    "    if method == 'GBM':\n",
    "        y_prediction, y_prediction_train = GBM(X_train_train, X_train_val, y_train_train, best_loss['GBM'])\n",
    "    elif method == 'GLM':\n",
    "        y_prediction, y_prediction_train = GLM(X_train_train, X_train_val, y_train_train)\n",
    "    elif method == 'KNN':\n",
    "        y_prediction, y_prediction_train = KNN(X_train_train, X_train_val, y_train_train)\n",
    "    elif method == 'NN':\n",
    "        y_prediction, y_prediction_train = NN(X_train_train, X_train_val, y_train_train, y_train_val, best_loss['NN'])\n",
    "\n",
    "    return y_prediction, y_prediction_train\n",
    "\n",
    "\n",
    "########################################################### run mixed methods in parallel\n",
    "def mixed_parallel_run(method, X_train, X_test, y_train, y_test, best_loss):\n",
    "\n",
    "    y_prediction, y_prediction_train = None, None\n",
    "    if method == 'MM_GLM':\n",
    "        y_prediction, y_prediction_train = MM_GLM(X_train, X_test, y_train)\n",
    "    elif method == 'MM_NN':\n",
    "        y_prediction, y_prediction_train = NN(X_train, X_test, y_train, y_test, best_loss[method])\n",
    "\n",
    "    return y_prediction, y_prediction_train\n",
    "\n",
    "\n",
    "########################################################### run algorithms in parallel except mixed models\n",
    "def run_algorithms(X_train_dict, X_val_dict, y_train_dict, y_val_dict, best_loss, c , spatial_mode, county_fips):\n",
    "    from models import GBM, GLM, KNN, NN\n",
    "    t1 = time.time()\n",
    "    methods = ['GBM','GLM','KNN','NN']\n",
    "    X_train = {method : None for method in methods}\n",
    "    X_val = {method : None for method in methods}\n",
    "    y_train = {method : None for method in methods}\n",
    "    y_val = {method : None for method in methods}\n",
    "    loom = ProcessLoom(max_runner_cap=4)\n",
    "    # add the functions to the multiprocessing object, loom\n",
    "    if spatial_mode == 'country':\n",
    "      for method in methods:\n",
    "        X_train[method] = X_train_dict[method].drop(['county_fips','date of day t'],axis=1)\n",
    "        X_val[method] = X_val_dict[method].drop(['county_fips','date of day t'],axis=1)\n",
    "        y_train[method] = np.array(y_train_dict[method]['Target']).reshape(-1)\n",
    "        y_val[method] = np.array(y_val_dict[method]['Target']).reshape(-1)\n",
    "      loom.add_function(GBM, [X_train['GBM'], X_val['GBM'], y_train['GBM'], best_loss['GBM']], {})\n",
    "      loom.add_function(GLM, [X_train['GLM'], X_val['GLM'], y_train['GLM']], {})\n",
    "      loom.add_function(KNN, [X_train['KNN'], X_val['KNN'], y_train['KNN']], {})\n",
    "      loom.add_function(NN, [X_train['NN'], X_val['NN'], y_train['NN'], y_val['NN'], best_loss['NN']], {})\n",
    "    if spatial_mode == 'county':\n",
    "      for method in methods:\n",
    "        X_train[method] = X_train_dict[method]\n",
    "        X_train[method] = X_train[method][X_train[method]['county_fips']==county_fips].drop(['county_fips','date of day t'],axis=1)\n",
    "        print('run_alg X_train[method].shape',X_train[method].shape)\n",
    "        X_val[method] = X_val_dict[method]\n",
    "        X_val[method] = X_val[method][X_val[method]['county_fips']==county_fips].drop(['county_fips','date of day t'],axis=1)\n",
    "        y_train[method] = y_train_dict[method]\n",
    "        y_train[method] = y_train[method][y_train[method]['county_fips']==county_fips].drop(['county_fips','date of day t'],axis=1)\n",
    "        y_val[method] = y_val_dict[method]\n",
    "        y_val[method] = y_val[method][y_val[method]['county_fips']==county_fips].drop(['county_fips','date of day t'],axis=1)\n",
    "        y_train[method] = np.array(y_train[method]['Target']).reshape(-1)\n",
    "        y_val[method] = np.array(y_val[method]['Target']).reshape(-1)\n",
    "      loom.add_function(GBM, [X_train['GBM'], X_val['GBM'], y_train['GBM'], best_loss['GBM']], {})\n",
    "      loom.add_function(GLM, [X_train['GLM'], X_val['GLM'], y_train['GLM']], {})\n",
    "      loom.add_function(KNN, [X_train['KNN'], X_val['KNN'], y_train['KNN']], {})\n",
    "      loom.add_function(NN, [X_train['NN'], X_val['NN'], y_train['NN'], y_val['NN'], best_loss['NN']], {})\n",
    "\n",
    "    # run the processes in parallel\n",
    "    output = loom.execute()\n",
    "    t2 = time.time()\n",
    "    print('total time - run algorithms: ', t2 - t1)\n",
    "\n",
    "    return output[0]['output'], output[1]['output'], output[2]['output'], output[3]['output']\n",
    "\n",
    "\n",
    "########################################################### run mixed models in parallel\n",
    "def run_mixed_models(X_train_MM, X_test_MM, y_train_MM, y_test_MM, best_loss):\n",
    "\n",
    "    from models import GBM, GLM, KNN, NN, MM_GLM\n",
    "    t1 = time.time()\n",
    "    loom = ProcessLoom(max_runner_cap=2)\n",
    "    # add the functions to the multiprocessing object, loom\n",
    "    loom.add_function(MM_GLM, [X_train_MM['MM_GLM'], X_test_MM['MM_GLM'], y_train_MM['MM_GLM']], {})\n",
    "    loom.add_function(NN, [X_train_MM['MM_NN'], X_test_MM['MM_NN'], y_train_MM['MM_NN'], y_test_MM['MM_NN'], best_loss['MM_NN']], {})\n",
    "    # run the processes in parallel\n",
    "    output = loom.execute()\n",
    "    t2 = time.time()\n",
    "    print('total time - run mixed models: ', t2 - t1)\n",
    "\n",
    "    return output[0]['output'], output[1]['output']\n",
    "\n",
    "####################################################################### update best loss\n",
    "\n",
    "def update_best_loss(model_type ,spatial_mode ,county_fips,best_loss,X_train_train_to_use,X_train_val_to_use,y_train_train,\\\n",
    "                     y_train_val,y_prediction_train,y_prediction,covariates,\\\n",
    "                     numberOfCovariates,max_c):\n",
    "    h = 1\n",
    "    if model_type == 'mixed_model':\n",
    "          loom = ProcessLoom(max_runner_cap=1)\n",
    "          c = numberOfCovariates\n",
    "          if numberOfCovariates > max_c :\n",
    "            c = max_c\n",
    "          y_predictions_test, y_predictions_train = [], []\n",
    "          if spatial_mode == 'county':\n",
    "            # Construct the outputs for the testing dataset of the 'MM' methods\n",
    "            y_predictions_test.extend([y_prediction[county_fips]['GBM'][(h, c)], y_prediction[county_fips]['GLM'][(h, c)],\n",
    "                                        y_prediction[county_fips]['KNN'][(h, c)], y_prediction[county_fips]['NN'][(h, c)]])\n",
    "            \n",
    "          elif spatial_mode == 'country':\n",
    "            y_predictions_test.extend([y_prediction['GBM'][(h, c)], y_prediction['GLM'][(h, c)],\n",
    "                                        y_prediction['KNN'][(h, c)], y_prediction['NN'][(h, c)]])\n",
    "          y_prediction_test_np = np.array(y_predictions_test).reshape(len(y_predictions_test), -1)\n",
    "          X_test_mixedModel = pd.DataFrame(y_prediction_test_np.transpose())\n",
    "          if spatial_mode == 'county':\n",
    "            # Construct the outputs for the training dataset of the 'MM' methods\n",
    "            y_predictions_train.extend([y_prediction_train[county_fips]['GBM'][(h, c)], y_prediction_train[county_fips]['GLM'][(h, c)],\n",
    "                                        y_prediction_train[county_fips]['KNN'][(h, c)], y_prediction_train[county_fips]['NN'][(h, c)]])\n",
    "          elif spatial_mode == 'country':\n",
    "            y_predictions_train.extend([y_prediction_train['GBM'][(h, c)], y_prediction_train['GLM'][(h, c)],\n",
    "                                        y_prediction_train['KNN'][(h, c)], y_prediction_train['NN'][(h, c)]])\n",
    "          y_prediction_train_np = np.array(y_predictions_train).reshape(len(y_predictions_train), -1)\n",
    "          X_train_mixedModel = pd.DataFrame(y_prediction_train_np.transpose())\n",
    "          loom.add_function(NN_grid_search, [X_train_mixedModel,y_train_train , X_test_mixedModel,y_train_val])\n",
    "          best_loss_output = loom.execute()\n",
    "          best_loss['MM_NN'] = best_loss_output[0]['output']\n",
    "          \n",
    "    if model_type == 'none_mixed_model':\n",
    "          loom = ProcessLoom(max_runner_cap= 2)\n",
    "          if spatial_mode == 'country':\n",
    "            loom.add_function(GBM_grid_search, [X_train_train_to_use['GBM'][covariates],\n",
    "                                                    y_train_train , X_train_val_to_use['GBM'][covariates],\n",
    "                                                    y_train_val])\n",
    "            loom.add_function(NN_grid_search, [X_train_train_to_use['NN'][covariates],\n",
    "                                                    y_train_train , X_train_val_to_use['NN'][covariates],\n",
    "                                                    y_train_val])\n",
    "          if spatial_mode == 'county':\n",
    "            loom.add_function(GBM_grid_search, [X_train_train_to_use[county_fips][h]['GBM'][covariates],\n",
    "                                                    y_train_train , X_train_val_to_use[county_fips][h]['GBM'][covariates],\n",
    "                                                    y_train_val])\n",
    "            loom.add_function(NN_grid_search, [X_train_train_to_use[county_fips][h]['NN'][covariates],\n",
    "                                                    y_train_train , X_train_val_to_use[county_fips][h]['NN'][covariates],\n",
    "                                                    y_train_val])\n",
    "          best_loss_output=loom.execute()\n",
    "          best_loss['GBM'],best_loss['NN'] = best_loss_output[0]['output'],best_loss_output[1]['output']\n",
    "    return best_loss\n",
    "\n",
    "########################################################### \n",
    "\n",
    "def get_best_loss_mode(counties_best_loss_list):\n",
    "\n",
    "  methods_with_loss=['GBM', 'NN', 'MM_NN']\n",
    "  best_loss = {method: None for method in methods_with_loss}\n",
    "  for method in methods_with_loss:\n",
    "\n",
    "    counties_best_loss_array=np.array(counties_best_loss_list[method])\n",
    "    # when we choose number_of_selected_counties smaller than number of different losses\n",
    "    # some times its not possibel to find mode\n",
    "    if len(np.unique(counties_best_loss_array))==len(counties_best_loss_array):\n",
    "      best_loss[method] = random.choice(counties_best_loss_list[method])\n",
    "    else:\n",
    "      best_loss[method] = statistics.mode(counties_best_loss_list[method])\n",
    "  return(best_loss)\n",
    "\n",
    "########################################################### generate data for best h and c\n",
    "\n",
    "def generate_data(h, numberOfCovariates, covariates_names, numberOfSelectedCounties):\n",
    "\n",
    "    data = makeHistoricalData(h, r, 'confirmed', 'mrmr', spatial_mode, target_mode, './', iteration)\n",
    "    data = clean_data(data, numberOfSelectedCounties)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = preprocess(data, spatial_mode, 0)\n",
    "    covariates = [covariates_names[i] for i in range(numberOfCovariates)]\n",
    "    best_covariates = []\n",
    "    indx_c = 0\n",
    "    for covar in covariates:  # iterate through sorted covariates\n",
    "        indx_c += 1\n",
    "        for covariate in data.columns:  # add all historical covariates of this covariate and create a feature\n",
    "            if covar.split(' ')[0] in covariate:\n",
    "                best_covariates.append(covariate)\n",
    "\n",
    "    best_covariates += ['county_fips','date of day t'] # we add this two columns to use when we want break data to county_data\n",
    "    X_train = X_train[best_covariates]\n",
    "    X_test = X_test[best_covariates]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "########################################################### plot the results\n",
    "\n",
    "def plot_results(row, col, numberOfCovariates, methods, history, errors, mode):\n",
    "    print(history)\n",
    "    mpl.style.use('seaborn')\n",
    "    plt.rc('font', size=20)\n",
    "    fig, ax = plt.subplots(row, col, figsize=(40, 40))\n",
    "    colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "    # Sort colors by hue, saturation, value and name.\n",
    "    by_hsv = sorted((tuple(mcolors.rgb_to_hsv(mcolors.to_rgba(color)[:3])), name)\n",
    "                    for name, color in colors.items())\n",
    "    sorted_names = [name for hsv, name in by_hsv]\n",
    "    colorset = set(sorted_names[::-1])\n",
    "    for item in colorset:\n",
    "        if ('white' in item) or ('light' in item):\n",
    "            colorset = colorset - {item}\n",
    "    colors = list(colorset - {'lavenderblush',  'aliceblue', 'lavender', 'azure',\n",
    "         'mintcream', 'honeydew', 'beige', 'ivory', 'snow', 'w'})\n",
    "    #colors = ['tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan',\n",
    "     #         'tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n",
    "    ind = 0\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            color = 0\n",
    "            for h in history:\n",
    "                errors_h = []\n",
    "                # x label: covariates\n",
    "                covariates_list = [c for c in range(1, numberOfCovariates + 1)][:maxC]\n",
    "                # y label: errors\n",
    "                for c in range(1, numberOfCovariates + 1):\n",
    "                    errors_h.append(errors[methods[ind]][(h, c)])\n",
    "                    if c == maxC:\n",
    "                      break\n",
    "                ax[i, j].plot(covariates_list, errors_h, colors[color * 2], label=\"h = \" + str(h))\n",
    "                ax[i, j].set_xlabel(\"Number Of Covariates\")\n",
    "                ax[i, j].set_ylabel(mode)\n",
    "                ax[i, j].set_title(str(methods[ind]))\n",
    "                ax[i, j].legend()\n",
    "                ax[i, j].set_xticks(covariates_list)\n",
    "                color += 1\n",
    "            ind += 1\n",
    "    address = validation_address + 'plots_of_errors/'\n",
    "    if not os.path.exists(address):\n",
    "        os.makedirs(address)\n",
    "    plt.savefig(address + str(mode)+'.png')\n",
    "\n",
    "\n",
    "########################################################### plot table for final results\n",
    "def plot_table(table_data, col_labels, row_labels, name, mode):\n",
    "    fig = plt.figure() #dpi=50 figsize=(30, 10)\n",
    "    ax = fig.add_subplot(111)\n",
    "    colWidths = [0.1, 0.1, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
    "    address = ''\n",
    "    if mode == 'val':\n",
    "        # colWidths.pop()\n",
    "        address = validation_address + 'tables/'\n",
    "        if not os.path.exists(address):\n",
    "            os.makedirs(address)\n",
    "    else:\n",
    "        address = test_address + 'tables/'\n",
    "        if not os.path.exists(address):\n",
    "            os.makedirs(address)\n",
    "    the_table = plt.table(cellText=table_data,\n",
    "                          colWidths=colWidths,\n",
    "                          rowLabels=row_labels,\n",
    "                          colLabels=col_labels,\n",
    "                          loc='center',\n",
    "                          cellLoc='center')\n",
    "    the_table.auto_set_font_size(False)\n",
    "    the_table.set_fontsize(9)\n",
    "    the_table.scale(1.5, 1.5)\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.savefig(address + name + '.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "########################################################### plotting mean errors (first error)\n",
    "def plot_targets(method, x_axis, df, main_address):\n",
    "    mpl.style.use('default')\n",
    "    plt.rc('font', size=40)\n",
    "    fig, ax = plt.subplots(figsize=(60, 20))\n",
    "    ax.plot(x_axis, df['average of targets'], label='Target')\n",
    "    ax.plot(x_axis, df['average of predictions'], label='Prediction')\n",
    "    ax.set_xlabel('date', fontsize=40)\n",
    "    ax.set_ylabel('real and predicted targets for ' + str(method), fontsize=40)\n",
    "    ax.legend()\n",
    "    address = main_address + 'procedure_of_prediction/'\n",
    "    if not os.path.exists(address):\n",
    "        os.makedirs(address)\n",
    "    plt.savefig(address +'procedure_'+ str(method) +'.png')\n",
    "\n",
    "\n",
    "########################################################### box plots and violin plots\n",
    "def box_violin_plot(X, Y, figsizes, fontsizes, name, address):\n",
    "    mpl.style.use('default')\n",
    "    # box plot\n",
    "    fig = plt.figure(figsize=figsizes['box'])\n",
    "    plt.rc('font', size=fontsizes['box'])\n",
    "    plt.locator_params(axis='y', nbins=20)\n",
    "    sns.boxplot(x=X, y=Y)\n",
    "    plt.savefig(address + str(name) + 'boxplot.png')\n",
    "    plt.close()\n",
    "    # violin plot\n",
    "    fig = plt.figure(figsize=figsizes['violin'])\n",
    "    plt.rc('font', size=fontsizes['violin'])\n",
    "    plt.locator_params(axis='y', nbins=20)\n",
    "    sns.violinplot(x=X, y=Y)\n",
    "    plt.savefig(address + str(name) + 'violinplot.png')\n",
    "    plt.close()\n",
    "########################################################### plot prediction and real values\n",
    "\n",
    "def real_prediction_plot(df,r,target_name,best_h,spatial_mode,methods,numberOfSelectedCounties):\n",
    "\n",
    "    address = test_address + 'plots_of_real_prediction_values/'\n",
    "    if not os.path.exists(address):\n",
    "        os.makedirs(address)\n",
    "\n",
    "    for method in methods:\n",
    "       \n",
    "        method_prediction_df = df[method] # this df contain real and predicted target values\n",
    "        county_name_df=pd.read_csv('./'+'fixed-data.csv')[['county_fips','county_name']] # we need county names for plot label\n",
    "        df_for_plot = pd.merge(method_prediction_df,county_name_df,how='left')\n",
    "\n",
    "        df_for_plot['date'] = df_for_plot['date of day t'].apply(lambda x:datetime.datetime.strptime(x,'%m/%d/%y')+datetime.timedelta(days=r))\n",
    "        df_for_plot['date'] = df_for_plot['date'].apply(lambda x:datetime.datetime.strftime(x,'%m/%d/%y'))\n",
    "\n",
    "        counties = [36061]+random.sample(df_for_plot['county_fips'].unique().tolist(),2) # newyork + two random county\n",
    "\n",
    "        length=list()\n",
    "        for county in counties:\n",
    "          length.append(len(df_for_plot[df_for_plot['county_fips']==county]))\n",
    "\n",
    "        plot_with=max(length)+20\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(plot_with,75))\n",
    "        mpl.style.use('default')\n",
    "\n",
    "        for index,county in enumerate(counties):\n",
    "\n",
    "            plt.subplot(311+index)\n",
    "            plt.rc('font', size=45)\n",
    "            plt.plot(df_for_plot.loc[df_for_plot['county_fips']==county,'date'],df_for_plot.loc[df_for_plot['county_fips']==county,method],label='Prediction',linewidth=2.0)\n",
    "            plt.plot(df_for_plot.loc[df_for_plot['county_fips']==county,'date'],df_for_plot.loc[df_for_plot['county_fips']==county,'Target'],label='Real values',linewidth=2.0)\n",
    "            plt.xticks(rotation=65)\n",
    "            fig.subplots_adjust(hspace=0.4)\n",
    "            plt.ylabel('Number of confirmed')\n",
    "            countyname = df_for_plot.loc[df_for_plot['county_fips']==county,'county_name'].unique()\n",
    "            if len(countyname)>0 : # it is False when newyork is not in selected counties and make error\n",
    "              plt.title(df_for_plot.loc[df_for_plot['county_fips']==county,'county_name'].unique()[0])\n",
    "            plt.legend()\n",
    "        plt.xlabel('Date')\n",
    "        plt.savefig(address + str(method) + ' real_prediction_values.jpg')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "########################################################### get errors for each model in each h and c\n",
    "def get_errors(h, c, method, y_prediction, y_prediction_train, y_test_date, MASE_denominator, numberOfSelectedCounties, mode):\n",
    "    # make predictions rounded to their closest number and make the negatives ones zero\n",
    "    y_prediction = np.round(y_prediction)\n",
    "    y_prediction[y_prediction < 0] = 0\n",
    "    # write outputs into a file\n",
    "    orig_stdout = sys.stdout\n",
    "    f = open(env_address+'out.txt', 'a')\n",
    "    sys.stdout = f\n",
    "    # if mode == 'val': y_test_date would be an np.array with the target\n",
    "    # if mode == 'test': y_test_date would be a dataframe with columns ['date of day t', 'county_fips', 'Target']\n",
    "    y_test = y_test_date\n",
    "    if mode == 'test':  # use the 'Target' column for computing main errors\n",
    "        y_test = np.array(y_test_date['Target']).reshape(-1)\n",
    "\n",
    "    meanAbsoluteError = mean_absolute_error(y_test, y_prediction)\n",
    "    print(\"Mean Absolute Error of \", method, \" for h =\", h, \"and #covariates =\", c, \": %.2f\" % meanAbsoluteError)\n",
    "    sumOfAbsoluteError = sum(abs(y_test - y_prediction))\n",
    "    percentageOfAbsoluteError = (sumOfAbsoluteError / sum(y_test)) * 100\n",
    "    # we change zero targets into 1 and add 1 to their predictions\n",
    "    y_test_temp = y_test.copy()\n",
    "    y_test_temp[y_test == 0] = 1\n",
    "    y_prediction_temp = y_prediction.copy()\n",
    "    y_prediction_temp[y_test == 0] += 1\n",
    "    # meanPercentageOfAbsoluteError = sum((abs(y_prediction_temp - y_test_temp) / y_test_temp) * 100) / len(y_test)\n",
    "    print(\"Percentage of Absolute Error of \", method, \" for h =\", h, \"and #covariates =\", c,\n",
    "          \": %.2f\" % percentageOfAbsoluteError)\n",
    "    rootMeanSquaredError = sqrt(mean_squared_error(y_test, y_prediction))\n",
    "    print(\"Root Mean Squared Error of \", method, \" for h =\", h, \"and #covariates =\", c, \": %.2f\" % rootMeanSquaredError)\n",
    "\n",
    "    second_error = sum(abs(y_prediction - y_test))\n",
    "    ### compute adjusted R squared error\n",
    "    SS_Residual = sum((y_test - y_prediction.reshape(-1)) ** 2)\n",
    "    SS_Total = sum((y_test - np.mean(y_test)) ** 2)\n",
    "    r_squared = 1 - (float(SS_Residual)) / SS_Total\n",
    "    adj_r_squared = 1 - (1 - r_squared) * (len(y_test) - 1) / (len(y_test) - c - 1)\n",
    "    print(\"Adjusted R Squared Error of \", method, \" for h =\", h, \"and #covariates =\", c, \": %.2f\" % adj_r_squared)\n",
    "\n",
    "\n",
    "    #################################################################################################### MASE\n",
    "\n",
    "    if  target_mode=='cumulative':\n",
    "\n",
    "        # for cumulative data form we need to change MASE error and we need new case data to calclate this error so in next lines we build new case\n",
    "        # data from cumulative data form\n",
    "        data_new_case = makeHistoricalData(h, r, target_name, 'mrmr', spatial_mode, 'regular', './', iteration)\n",
    "        if numberOfSelectedCounties == -1 :\n",
    "          numberOfSelectedCounties = len(data_new_case['county_fips'].unique())\n",
    "        data_new_case = clean_data(data_new_case, numberOfSelectedCounties)\n",
    "        reverse_dates=data_new_case['date of day t'].unique()[::-1]\n",
    "        for i,j in enumerate(reverse_dates[1:]):\n",
    "            data_new_case.loc[data_new_case['date of day t']==reverse_dates[i],target_name+' t']=list(np.array(data_new_case.loc[data_new_case['date of day t']==reverse_dates[i],target_name+' t'])-np.array(data_new_case.loc[data_new_case['date of day t']==j,target_name+' t']))\n",
    "\n",
    "        if mode == 'val':\n",
    "\n",
    "            y_test_val = y_test_date\n",
    "            X_train_train_new_case, X_train_val_new_case, X_test_new_case, y_train_train_date_new_case, y_train_val_date_new_case, y_test_date_new_case = preprocess(data_new_case, 1)\n",
    "\n",
    "            train_train_new_case=pd.concat([y_train_train_date_new_case.copy().reset_index(drop=True),X_train_train_new_case.copy().reset_index(drop=True)],axis=1)\n",
    "            train_train_template=train_train_new_case[['date of day t','county_fips']]\n",
    "            cumul_train_train_predict = train_train_template\n",
    "            cumul_train_train_predict['cumul_train_naive_predict'] = y_prediction_train.tolist()\n",
    "            cumul_train_train_predict.sort_values(by=['date of day t','county_fips'],inplace=True)\n",
    "            cumul_train_train_predict=cumul_train_train_predict.tail(numberOfSelectedCounties*r)\n",
    "            cumul_train_train_predict.sort_values(by=['county_fips','date of day t'],inplace=True)\n",
    "            for i in range(r):\n",
    "                X_train_train_new_case_temp = train_train_new_case.copy().sort_values(by=['date of day t','county_fips'])\n",
    "                if i==0:\n",
    "                  X_train_train_new_case_temp = X_train_train_new_case_temp.tail(numberOfSelectedCounties*r)\n",
    "                else:\n",
    "                  X_train_train_new_case_temp = X_train_train_new_case_temp.iloc[:-(numberOfSelectedCounties*(i)), :].tail(numberOfSelectedCounties*r)\n",
    "                X_train_train_new_case_temp.sort_values(by=['county_fips','date of day t'])\n",
    "                cumul_train_train_predict['cumul_train_naive_predict']=list(np.array(cumul_train_train_predict['cumul_train_naive_predict'])+np.array(X_train_train_new_case_temp[target_name+' t']))\n",
    "            train_val_mase_denom = pd.DataFrame(y_test_date.copy(),columns=['Target'])\n",
    "            train_val_mase_denom['cumul_train_naive_predict'] = cumul_train_train_predict['cumul_train_naive_predict'].tolist()\n",
    "            train_val_mase_denom['absolute_error'] = abs(train_val_mase_denom['Target'] - train_val_mase_denom['cumul_train_naive_predict'])\n",
    "            train_val_MASE_denominator = train_val_mase_denom['absolute_error'].mean()\n",
    "            MASE_numerator = sum(abs(y_prediction_temp - y_test_temp))/len(y_test_val)\n",
    "            MASE = MASE_numerator/train_val_MASE_denominator\n",
    "\n",
    "        if mode == 'test':\n",
    "\n",
    "            X_train_new_case, X_test_new_case, y_train_date_new_case, y_test_date_new_case = preprocess(data_new_case, 0)\n",
    "\n",
    "            train_new_case=pd.concat([y_train_date_new_case.copy().reset_index(drop=True),X_train_new_case.copy().reset_index(drop=True)],axis=1)\n",
    "            train_template=train_new_case[['date of day t','county_fips']]\n",
    "            cumul_train_predict = train_template\n",
    "            cumul_train_predict['cumul_train_naive_predict'] = y_prediction_train.tolist()\n",
    "            cumul_train_predict.sort_values(by=['date of day t','county_fips'],inplace=True)\n",
    "            cumul_train_predict=cumul_train_predict.tail(numberOfSelectedCounties*r)\n",
    "            cumul_train_predict.sort_values(by=['county_fips','date of day t'],inplace=True)\n",
    "            for i in range(r):\n",
    "                X_train_new_case_temp = train_new_case.copy().sort_values(by=['date of day t','county_fips'])\n",
    "                if i==0:\n",
    "                  X_train_new_case_temp = X_train_new_case_temp.tail(numberOfSelectedCounties*repr)\n",
    "                else:\n",
    "                  X_train_new_case_temp = X_train_new_case_temp.iloc[:-(numberOfSelectedCounties*(i)), :].tail(numberOfSelectedCounties*r)\n",
    "                X_train_new_case_temp.sort_values(by=['county_fips','date of day t'])\n",
    "                cumul_train_predict['cumul_train_naive_predict']=list(np.array(cumul_train_predict['cumul_train_naive_predict'])+np.array(X_train_new_case_temp[target_name+' t']))\n",
    "            train_test_mase_denom = y_test_date.copy()\n",
    "            train_test_mase_denom['cumul_train_naive_predict'] = cumul_train_predict['cumul_train_naive_predict'].tolist()\n",
    "            train_test_mase_denom['absolute_error'] = abs(train_test_mase_denom['Target'] - train_test_mase_denom['cumul_train_naive_predict'])\n",
    "            train_test_MASE_denominator = train_test_mase_denom['absolute_error'].mean()\n",
    "            MASE_numerator = sum(abs(y_prediction - y_test))/len(y_test)\n",
    "            MASE = MASE_numerator/train_test_MASE_denominator\n",
    "\n",
    "    else:\n",
    "        MASE_numerator = sum(abs(y_prediction_temp - y_test_temp))/len(y_test)\n",
    "        MASE = MASE_numerator/MASE_denominator\n",
    "    print(\"MASE Error of \", method, \" for h =\", h, \"and #covariates =\", c, \": %.2f\" % MASE)\n",
    "\n",
    "\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "    # save outputs in 'out.txt'\n",
    "    sys.stdout = orig_stdout\n",
    "    f.close()\n",
    "    # for the test mode we compute some additional errors, we need 'date of day t' column so we use the main dataframe\n",
    "    # we add our prediction, the difference between prediction and target ('error' column),\n",
    "    # the absolute difference between prediction and target ('absolute_error' column),\n",
    "    # the precentage of this difference (('percentage_error' column) -> we change zero targets into 1 and add 1 to their predictions),\n",
    "    # and second_error as follows and save these in 'all_errors' file\n",
    "    # then we compute the average of percentage_errors (and other values) in each day and save them in\n",
    "    # 'first_error' file\n",
    "\n",
    "    if mode == 'test':\n",
    "        # write outputs into a file\n",
    "        orig_stdout = sys.stdout\n",
    "        f = open(env_address + 'out.txt', 'a')\n",
    "        sys.stdout = f\n",
    "\n",
    "        first_error_address = test_address + 'averages_of_errors_in_each_day/'\n",
    "        all_errors_address = test_address + 'all_errors/' + str(method) + '/'\n",
    "        if not os.path.exists(first_error_address):\n",
    "            os.makedirs(first_error_address)\n",
    "        if not os.path.exists(all_errors_address):\n",
    "            os.makedirs(all_errors_address)\n",
    "        dataframe = pd.DataFrame(y_test_date, copy=True)\n",
    "        dataframe['prediction'] = y_prediction\n",
    "        dataframe['error'] = y_prediction - y_test\n",
    "        dataframe['absoulte_error'] = abs(y_prediction - y_test)\n",
    "        y_test_temp = y_test.copy()\n",
    "        y_test_temp[y_test == 0] = 1\n",
    "        y_prediction_temp = y_prediction.copy()\n",
    "        y_prediction_temp[y_test == 0] += 1\n",
    "        dataframe['percentage_error'] = ((abs(y_prediction_temp - y_test_temp)) / y_test_temp) * 100\n",
    "        second_error = (sum(dataframe['error']) / sum(y_test)) * 100\n",
    "        dataframe.to_csv(all_errors_address + 'all_errors_' + str(method) + '.csv')\n",
    "        box_violin_plot(dataframe['date of day t'], dataframe['percentage_error'], figsizes={'box': (60, 30), 'violin': (100, 50)},\n",
    "                        fontsizes={'box' : 40, 'violin': 60}, name=str(method) + '_percentage_errors_in_each_day_',\n",
    "                        address=all_errors_address)\n",
    "        box_violin_plot(dataframe['date of day t'], dataframe['error'], figsizes={'box': (20, 10), 'violin': (50, 30)},\n",
    "                        fontsizes={'box': 15, 'violin': 30}, name=str(method) + '_pure_errors_in_each_day_',\n",
    "                        address=all_errors_address)\n",
    "        dataframe['county_fips']=dataframe['county_fips'].astype(float)\n",
    "        if numberOfSelectedCounties == -1:\n",
    "          numberOfSelectedCounties = len(dataframe['county_fips'])\n",
    "        first_error = pd.DataFrame((dataframe.groupby(['date of day t']).sum() / numberOfSelectedCounties))\n",
    "        first_error.columns = ['fips','average of targets', 'average of predictions', 'average of errors',\n",
    "                               'average of absoulte_errors', 'average of percentage_errors']\n",
    "        first_error = first_error.drop(['fips'], axis=1)\n",
    "        first_error.to_csv(first_error_address + 'first_error_' + str(method) + '.csv')\n",
    "        plot_targets(method, first_error.index, first_error, first_error_address)\n",
    "\n",
    "        # save outputs in 'out.txt'\n",
    "        sys.stdout = orig_stdout\n",
    "        f.close()\n",
    "    return meanAbsoluteError, percentageOfAbsoluteError, adj_r_squared, second_error, MASE\n",
    "\n",
    "\n",
    "########################################################### push results to github\n",
    "def push(message):\n",
    "    try:\n",
    "        cmd.run(\"git pull\", check=True, shell=True)\n",
    "        print(\"everything has been pulled\")\n",
    "        cmd.run(\"git add .\", check=True, shell=True)\n",
    "        cmd.run(f\"git commit -m '{message}'\", check=True, shell=True)\n",
    "        cmd.run(\"git push\", check=True, shell=True)\n",
    "        print('pushed.')\n",
    "\n",
    "    except:\n",
    "        print('could not push')\n",
    "\n",
    "\n",
    "########################################################### zip some of the results\n",
    "def make_zip(selected_for_email, subject):\n",
    "\n",
    "    for source_root in selected_for_email:\n",
    "        for i in [x[0] for x in os.walk(source_root)]:\n",
    "            address = mail_address  + '//'+ '/'.join(i.split('/')[3:])\n",
    "            # print(address)\n",
    "            if not os.path.exists(address):\n",
    "                    os.makedirs(address)\n",
    "            for jpgfile in glob.iglob(os.path.join(i, \"*.png\")):\n",
    "                shutil.copy(jpgfile, address)\n",
    "    shutil.make_archive(subject, 'zip', mail_address)\n",
    "\n",
    "\n",
    "########################################################### mail some of the results\n",
    "def send_email(*attachments):\n",
    "    subject = \"Server results\"\n",
    "    body = \" \"\n",
    "    sender_email = \"covidserver1@gmail.com\"\n",
    "    receiver_email = [\"arezo.h1371@yahoo.com\"]#,\"arashmarioriyad@gmail.com\"\n",
    "    CC_email = [\"p.ramazi@gmail.com\"]#\n",
    "    password = \"S.123456.S\"\n",
    "\n",
    "    # Create a multipart message and set headers\n",
    "    message = MIMEMultipart()\n",
    "    message[\"From\"] = sender_email\n",
    "    message[\"To\"] = ','.join(receiver_email)#receiver_email\n",
    "    message[\"Subject\"] = subject\n",
    "    message[\"CC\"] = ','.join(CC_email) # Recommended for mass emails\n",
    "\n",
    "    # Add body to email\n",
    "    message.attach(MIMEText(body, \"plain\"))\n",
    "\n",
    "    # Add attachments\n",
    "    for file_name in attachments:\n",
    "            f = open(file_name, 'rb')\n",
    "            ctype, encoding = mimetypes.guess_type(file_name)\n",
    "            if ctype is None or encoding is not None:\n",
    "                ctype = 'application/octet-stream'\n",
    "            maintype, subtype = ctype.split('/', 1)\n",
    "            # in case of a text file\n",
    "            if maintype == 'text':\n",
    "                part = MIMEText(f.read(), _subtype=subtype)\n",
    "            # any other file\n",
    "            else:\n",
    "                part = MIMEBase(maintype, subtype)\n",
    "                part.set_payload(f.read())\n",
    "            encoders.encode_base64(part)\n",
    "            part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' % os.path.basename(file_name))\n",
    "            message.attach(part)\n",
    "            f.close()\n",
    "            text = message.as_string()\n",
    "\n",
    "    # Log in to server using secure context and send email\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email+CC_email , text)\n",
    "\n",
    "\n",
    "############################################################ test process\n",
    "def test_process(h, r, target_name,spatial_mode, target_mode,best_h,best_c,historical_X_train,\\\n",
    "                 historical_X_test, historical_y_train_date, historical_y_test_date, best_loss,\\\n",
    "                 numberOfSelectedCounties, covariates_names, maxHistory, test_address, env_address, mail_address):\n",
    "\n",
    "    \n",
    "    columns_table_t = ['best_h', 'best_c', 'mean absolute error', 'percentage of absolute error', 'adjusted R squared error',\n",
    "                      'second error', 'mean absolute scaled error']\n",
    "    columns_table = ['best_h', 'best_c', 'mean absolute error', 'percentage of absolute error',\n",
    "                      'adjusted R squared error',\n",
    "                      'sum of absolute error', 'mean absolute scaled error']\n",
    "    methods = ['GBM', 'GLM', 'KNN', 'NN', 'MM_GLM', 'MM_NN']\n",
    "    none_mixed_methods = ['GBM', 'GLM', 'KNN', 'NN']\n",
    "    mixed_methods = ['MM_GLM', 'MM_NN']\n",
    "\n",
    "    train_val_MASE_denominator, val_test_MASE_denominator, train_lag_MASE_denominator = mase_denominator(r, target_name, target_mode, numberOfSelectedCounties)\n",
    "    df_for_prediction_plot = {method : None for method in methods}\n",
    "\n",
    "    all_data = makeHistoricalData(h, r, target_name, 'mrmr', spatial_mode, target_mode, './', iteration)\n",
    "    all_data = clean_data(all_data, numberOfSelectedCounties)\n",
    "    print(all_data.shape)\n",
    "    all_counties = all_data['county_fips'].unique()\n",
    "    y_prediction = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    y_prediction_train = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    # run non-mixed methods on the whole training set with their best h and c\n",
    "    X_train_dict, X_test_dict, y_train_dict, y_test_dict = {}, {}, {}, {}\n",
    "\n",
    "\n",
    "    for county_fips in all_counties:\n",
    "\n",
    "        GBM, GLM, KNN, NN = run_algorithms(historical_X_train, historical_X_test, historical_y_train_date, historical_y_test_date, best_loss, 0, spatial_mode, county_fips)\n",
    "\n",
    "        y_prediction[county_fips]['GBM'], y_prediction_train[county_fips]['GBM'] = GBM\n",
    "        y_prediction[county_fips]['GLM'], y_prediction_train[county_fips]['GLM'] = GLM\n",
    "        y_prediction[county_fips]['KNN'], y_prediction_train[county_fips]['KNN'] = KNN\n",
    "        y_prediction[county_fips]['NN'], y_prediction_train[county_fips]['NN'] = NN\n",
    "\n",
    "\n",
    "    table_data = []\n",
    "\n",
    "    for method in none_mixed_methods:\n",
    "        meanAbsoluteError, percentageOfAbsoluteError, adj_r_squared, second_error, meanAbsoluteScaledError = get_errors(best_h[method]['MAPE'],\n",
    "        best_c[method]['MAPE'], method, flatten(data=y_prediction, h=h, c=None, method=method, state=6), flatten(data=y_prediction_train, h=h, c=None, method=method, state=6), historical_y_test_date[method],\n",
    "         val_test_MASE_denominator, numberOfSelectedCounties, mode='test')\n",
    "        \n",
    "        table_data.append([best_h[method]['MAPE'], best_c[method]['MAPE'],  round(meanAbsoluteError, 2),\n",
    "                            round(percentageOfAbsoluteError, 2), round(adj_r_squared, 2), round(second_error, 2), round(meanAbsoluteScaledError, 2)])\n",
    "\n",
    "    push('a new table added')\n",
    "\n",
    "    for method in none_mixed_methods:\n",
    "      method_real_pred_df = historical_y_train_date[method].append(historical_y_test_date[method])\n",
    "      prediction=list(flatten(data=y_prediction_train, h=h, c=None, method=method, state=6))+list(flatten(data=y_prediction, h=h, c=None, method=method, state=6))\n",
    "      method_real_pred_df[method] = prediction\n",
    "      df_for_prediction_plot[method] = method_real_pred_df\n",
    "\n",
    "    # generate data for non-mixed methods with the best h and c of mixed models and fit mixed models on them\n",
    "    # (with the whole training set)\n",
    "    y_predictions = {'MM_GLM': [], 'MM_NN': []}\n",
    "    y_prediction = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    y_prediction_train = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    #table_data = []\n",
    "    X_train_MM_dict = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    X_test_MM_dict = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    y_train_MM_dict = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    y_test_MM_dict = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in all_counties}\n",
    "    y_train, y_test = {}, {}\n",
    "    y_test_date = {}\n",
    "\n",
    "    # we make mixed_model train_data in this loop\n",
    "\n",
    "    for mixed_method in mixed_methods:\n",
    "        X_train, X_test, y_train_date, y_test_date[mixed_method] = generate_data(best_h[mixed_method]['MAPE'], best_c[mixed_method]['MAPE'],\n",
    "                                                                                  covariates_names, numberOfSelectedCounties)\n",
    "        y_test_date_temp = y_test_date[mixed_method]\n",
    "        y_train[mixed_method] = y_train_date#np.array(['Target']).reshape(-1)\n",
    "        y_test[mixed_method] = y_test_date_temp# np.array(['Target']).reshape(-1)\n",
    "        mixed_model_covariates_names = list(X_train.columns)\n",
    "        X_train_to_use = {method: None for method in methods}\n",
    "        X_test_to_use = {method: None for method in methods}\n",
    "        for method in none_mixed_methods:\n",
    "            X_train_to_use[method] = X_train.copy()\n",
    "            X_test_to_use[method] = X_test.copy()\n",
    "            if method in models_to_log:\n",
    "                # make temporal and some fixed covariates logarithmic\n",
    "                negative_features=['temperature']\n",
    "                for covar in mixed_model_covariates_names:\n",
    "                    if (' t' in covar) and (covar.split(' ')[0] not in negative_features) and (covar != 'date of day t'):\n",
    "                        X_train_to_use[method][covar] = np.log((X_train_to_use[method][covar] + 1).astype(float))\n",
    "                        X_test_to_use[method][covar] = np.log((X_test_to_use[method][covar] + 1).astype(float))\n",
    "\n",
    "                fix_log_list = ['total_population', 'population_density', 'area', 'median_household_income',\n",
    "                                'houses_density', 'airport_distance','deaths_per_100000']\n",
    "                for covar in fix_log_list:\n",
    "                    if covar in mixed_model_covariates_names:\n",
    "                        X_train_to_use[method][covar] = np.log((X_train_to_use[method][covar] + 1).astype(float))\n",
    "                        X_test_to_use[method][covar] = np.log((X_test_to_use[method][covar] + 1).astype(float))\n",
    "\n",
    "            X_train_dict[method] = X_train_to_use[method]\n",
    "            X_test_dict[method] = X_test_to_use[method]\n",
    "            y_train_dict[method] = y_train[mixed_method]\n",
    "            y_test_dict[method] = y_test[mixed_method]\n",
    "\n",
    "\n",
    "        #########################################################################################\n",
    "\n",
    "        # we run mixed model for each county in this loop\n",
    "\n",
    "        # for mixed_method in mixed_methods:\n",
    "\n",
    "        for county_fips in all_counties:\n",
    "          \n",
    "            GBM, GLM, KNN, NN = run_algorithms(X_train_dict, X_test_dict, y_train_dict, y_test_dict, best_loss, 0, spatial_mode, county_fips)\n",
    "            y_prediction[county_fips]['GBM'], y_prediction_train[county_fips]['GBM'] = GBM\n",
    "            y_prediction[county_fips]['GLM'], y_prediction_train[county_fips]['GLM'] = GLM\n",
    "            y_prediction[county_fips]['KNN'], y_prediction_train[county_fips]['KNN'] = KNN\n",
    "            y_prediction[county_fips]['NN'], y_prediction_train[county_fips]['NN'] = NN\n",
    "            y_predictions_test, y_predictions_train = [], []\n",
    "            # Construct the outputs for the testing dataset of the 'MM' methods\n",
    "            y_predictions_test.extend([y_prediction[county_fips]['GBM'], y_prediction[county_fips]['GLM'], y_prediction[county_fips]['KNN'], y_prediction[county_fips]['NN']])\n",
    "            y_prediction_test_np = np.array(y_predictions_test).reshape(len(y_predictions_test), -1)\n",
    "            X_test_mixedModel = pd.DataFrame(y_prediction_test_np.transpose())\n",
    "            # Construct the outputs for the training dataset of the 'MM' methods\n",
    "            y_predictions_train.extend([y_prediction_train[county_fips]['GBM'], y_prediction_train[county_fips]['GLM'], y_prediction_train[county_fips]['KNN'], y_prediction_train[county_fips]['NN']])\n",
    "            y_prediction_train_np = np.array(y_predictions_train).reshape(len(y_predictions_train), -1)\n",
    "            X_train_mixedModel = pd.DataFrame(y_prediction_train_np.transpose())\n",
    "            X_train_MM_dict[county_fips][mixed_method] = X_train_mixedModel\n",
    "            X_test_MM_dict[county_fips][mixed_method] = X_test_mixedModel\n",
    "            y_train_MM_dict[county_fips][mixed_method] = y_train[mixed_method][y_train[mixed_method]['county_fips']==county_fips]\n",
    "            y_test_MM_dict[county_fips][mixed_method] = y_test[mixed_method][y_test[mixed_method]['county_fips']==county_fips]\n",
    "\n",
    "\n",
    "            y_test_MM_dict[county_fips][mixed_method] = np.array(y_test_MM_dict[county_fips][mixed_method]['Target']).reshape(-1)\n",
    "            y_train_MM_dict[county_fips][mixed_method] = np.array(y_train_MM_dict[county_fips][mixed_method]['Target']).reshape(-1)\n",
    "\n",
    "    for county_fips in all_counties:\n",
    "\n",
    "        # mixed model with linear regression and neural network\n",
    "        MM_GLM, MM_NN = run_mixed_models(X_train_MM_dict[county_fips], X_test_MM_dict[county_fips], y_train_MM_dict[county_fips], y_test_MM_dict[county_fips] ,best_loss)\n",
    "        y_prediction[county_fips]['MM_GLM'], y_prediction_train[county_fips]['MM_GLM'] = MM_GLM\n",
    "        y_prediction[county_fips]['MM_NN'], y_prediction_train[county_fips]['MM_NN'] = MM_NN\n",
    "\n",
    "    # save the entire session\n",
    "    filename = env_address + 'test.out'\n",
    "    my_shelf = shelve.open(filename, 'n')  # 'n' for new\n",
    "    for key in dir():\n",
    "        try:\n",
    "            my_shelf[key] = locals()[key]\n",
    "        except:\n",
    "            print('ERROR shelving: {0}'.format(key))\n",
    "    my_shelf.close()\n",
    "\n",
    "    ############################################################################################\n",
    "    for mixed_method in mixed_methods:\n",
    "        meanAbsoluteError, percentageOfAbsoluteError, adj_r_squared, second_error, meanAbsoluteScaledError = get_errors(best_h[mixed_method]['MAPE'],\n",
    "        best_c[mixed_method]['MAPE'], mixed_method, flatten(data=y_prediction, h=h, c=None, method=mixed_method, state=6), flatten(data=y_prediction_train, h=h, c=None, method=mixed_method, state=6), y_test_date[mixed_method],\n",
    "                                    val_test_MASE_denominator, numberOfSelectedCounties, mode='test')\n",
    "        table_data.append([best_h[mixed_method]['MAPE'], best_c[mixed_method]['MAPE'], round(meanAbsoluteError, 2), round(percentageOfAbsoluteError, 2),\n",
    "                            round(adj_r_squared, 2), round(second_error, 2), round(meanAbsoluteScaledError, 2)])\n",
    "\n",
    "    table_name = 'table_of_best_test_results'\n",
    "    plot_table(table_data, columns_table_t, methods, table_name, mode='test')\n",
    "    # push('a new table added')\n",
    "\n",
    "    for method in mixed_methods:\n",
    "      method_real_pred_df=y_train[method].append(y_test[method])\n",
    "      prediction=list(flatten(data=y_prediction_train, h=h, c=None, method=method, state=6))+list(flatten(data=y_prediction, h=h, c=None, method=method, state=6))\n",
    "      method_real_pred_df[method] = prediction\n",
    "      df_for_prediction_plot[method] = method_real_pred_df\n",
    "\n",
    "    real_prediction_plot(df_for_prediction_plot,r,target_name,best_h,spatial_mode,methods, numberOfSelectedCounties)\n",
    "\n",
    "    # mail the test results\n",
    "    selected_for_email = [test_address + '/tables', test_address + '/all_errors/NN', test_address + '/all_errors/KNN' , test_address + '/plots_of_real_prediction_values']\n",
    "    zip_file_name = 'test results for h =' + str(maxHistory) + ' #counties=' + str(numberOfSelectedCountiesname)\n",
    "    make_zip(selected_for_email, zip_file_name)\n",
    "    send_email(zip_file_name + '.zip')\n",
    "\n",
    "    # save the entire session\n",
    "    filename = env_address + 'test.out'\n",
    "    my_shelf = shelve.open(filename, 'n')  # 'n' for new\n",
    "    for key in dir():\n",
    "        try:\n",
    "            my_shelf[key] = locals()[key]\n",
    "        except:\n",
    "            print('ERROR shelving: {0}'.format(key))\n",
    "    my_shelf.close()\n",
    "\n",
    "\n",
    "########################################################## flatten\n",
    "def flatten(data=None, h=None, c=None, method=None, covariates_list=None, state=1):\n",
    "    if state == 1:\n",
    "        result = []\n",
    "        for county_fips in data:\n",
    "            result += list(data[county_fips][method][(h, c)])\n",
    "    elif state == 2:\n",
    "        result = []\n",
    "        for county_fips in data:\n",
    "            result += list(data[county_fips][(h, c)])\n",
    "    elif state == 3:\n",
    "        result = pd.DataFrame(columns=covariates_list)\n",
    "        for county_fips in data:\n",
    "            result = pd.concat([result, data[county_fips][h][method][covariates_list]], ignore_index=True)\n",
    "    elif state == 4:\n",
    "        for county_fips in data:\n",
    "            result = pd.DataFrame(columns=data[county_fips].columns.values)\n",
    "            break\n",
    "        for county_fips in data:\n",
    "            result = pd.concat([result, data[county_fips]], ignore_index=True)\n",
    "    elif state == 5:\n",
    "        result = []\n",
    "        for county_fips in data:\n",
    "            result += list(data[county_fips])\n",
    "        result = np.array(result)\n",
    "    elif state == 6:\n",
    "        result = []\n",
    "        for county_fips in data:\n",
    "            result += list(data[county_fips][method])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################### main\n",
    "def main(maxHistory, maxC):\n",
    "    history = [i for i in range(1, maxHistory + 1)]\n",
    "    methods = ['GBM', 'GLM', 'KNN', 'NN', 'MM_GLM', 'MM_NN']\n",
    "    none_mixed_methods = ['GBM', 'GLM', 'KNN', 'NN']\n",
    "    mixed_methods = ['MM_GLM', 'MM_NN']\n",
    "    target_name = 'confirmed'\n",
    "    base_data = makeHistoricalData(0, r, target_name, 'mrmr', spatial_mode, target_mode, './', iteration)\n",
    "    base_data = clean_data(base_data, numberOfSelectedCounties)\n",
    "    covariates_names = list(base_data.columns)\n",
    "    covariates_names.remove('Target')\n",
    "    covariates_names.remove('date of day t')\n",
    "    covariates_names.remove('county_fips')\n",
    "    numberOfCovariates = len(covariates_names)\n",
    "    y_prediction = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in base_data['county_fips'].unique()}\n",
    "    y_prediction_train = {county_fips: {'GBM': {}, 'GLM': {}, 'KNN': {}, 'NN': {}, 'MM_GLM': {}, 'MM_NN': {}}\n",
    "                    for county_fips in base_data['county_fips'].unique()}\n",
    "    y_val = {county_fips: {}\n",
    "                    for county_fips in base_data['county_fips'].unique()}\n",
    "    error_names = ['MAPE', 'MAE', 'adj-R2', 'sec', 'MASE']\n",
    "    complete_error_names = {'MAPE': 'Percentage Of Absolute Error', 'MAE': 'Mean Absolute Error',\n",
    "                            'MASE': 'Mean Absolute Scaled Error', 'adj-R2': 'Adjusted R Squared Error',\n",
    "                            'sec': 'Sum Of Absolute Error'}\n",
    "    validation_errors = {error: {method: {} for method in methods} for error in error_names}\n",
    "    minError = {method: {error: int(1e10) for error in error_names} for method in methods}\n",
    "    best_h = {method: {error: 0 for error in error_names} for method in methods}\n",
    "    best_c = {method: {error: 0 for error in error_names} for method in methods}\n",
    "    best_loss = {'GBM': 'poisson', 'MM_NN': 'poisson', 'NN': 'MeanAbsoluteError'}\n",
    "    # best_loss = {method: None for method in ['GBM', 'NN', 'MM_NN']}\n",
    "    counties_best_loss_list = {method: list() for method in ['GBM', 'NN', 'MM_NN']}\n",
    "    df_for_prediction_plot = pd.DataFrame(columns = methods)\n",
    "    columns_table_t = ['best_h', 'best_c', 'mean absolute error', 'percentage of absolute error', 'adjusted R squared error',\n",
    "                      'second error', 'mean absolute scaled error']\n",
    "    columns_table = ['best_h', 'best_c', 'mean absolute error', 'percentage of absolute error',\n",
    "                      'adjusted R squared error',\n",
    "                      'sum of absolute error', 'mean absolute scaled error']\n",
    "    historical_X_train = {}\n",
    "    historical_X_test = {}\n",
    "    historical_y_train = {}\n",
    "    historical_y_test = {}\n",
    "    historical_y_train_date = {}\n",
    "    historical_y_test_date = {}\n",
    "    X_train_train_to_use = {county_fips: {h: {method: None for method in methods} for h in history} for county_fips in base_data['county_fips'].unique()}\n",
    "    X_train_val_to_use = {county_fips: {h: {method: None for method in methods} for h in history} for county_fips in base_data['county_fips'].unique()}\n",
    "    X_test_to_use = {county_fips: {h: {method: None for method in methods} for h in history} for county_fips in base_data['county_fips'].unique()}\n",
    "    train_val_MASE_denominator, val_test_MASE_denominator, train_lag_MASE_denominator = mase_denominator(r, target_name, target_mode, numberOfSelectedCounties)\n",
    "    \n",
    "    for h in history:\n",
    "        print(\"h = \", h)\n",
    "        all_data = makeHistoricalData(h, r, target_name, 'mrmr', spatial_mode, target_mode, './', iteration)\n",
    "        all_data = clean_data(all_data, numberOfSelectedCounties)\n",
    "        print(all_data.shape)\n",
    "        if (all_data.shape[0] < 1) :\n",
    "            history = history[:h-1]\n",
    "            break\n",
    "        all_counties = all_data['county_fips'].unique()\n",
    "        y_test_date = {county_fips: None for county_fips in all_counties}\n",
    "        y_train_date = {county_fips: None for county_fips in all_counties}\n",
    "        y_train = {county_fips: None for county_fips in all_counties}\n",
    "        y_test = {county_fips: None for county_fips in all_counties}\n",
    "        for county_fips in all_counties:\n",
    "            print(\"county_fips = \", county_fips)\n",
    "            data = all_data[all_data['county_fips']==county_fips]\n",
    "            print(data.shape)\n",
    "            parallel_outputs = {}\n",
    "            X_train_train, X_train_val, X_test, y_train_train_date, y_train_val_date, y_test_date[county_fips] = preprocess(data, spatial_mode, 1)\n",
    "            indx_c = 0\n",
    "            for c in covariates_names:\n",
    "                indx_c += 1\n",
    "                y_val[county_fips][(h, indx_c)] = np.array(y_train_val_date['Target']).reshape(-1)\n",
    "                if indx_c == maxC:\n",
    "                    break\n",
    "            for method in methods:\n",
    "                X_train_train_to_use[county_fips][h][method] = X_train_train.copy()\n",
    "                X_train_val_to_use[county_fips][h][method]= X_train_val.copy()\n",
    "                X_test_to_use[county_fips][h][method] = X_test.copy()\n",
    "                if method in models_to_log:\n",
    "                    negative_features=['temperature']\n",
    "                    for covar in covariates_names:\n",
    "                        if (' t' in covar) and (covar.split(' ')[0] not in negative_features):\n",
    "                            X_train_train_to_use[county_fips][h][method][covar] = np.log((X_train_train_to_use[county_fips][h][method][covar] + 1).astype(float))\n",
    "                            X_train_val_to_use[county_fips][h][method][covar] = np.log((X_train_val_to_use[county_fips][h][method][covar] + 1).astype(float))\n",
    "                            X_test_to_use[county_fips][h][method][covar] = np.log((X_test_to_use[county_fips][h][method][covar] + 1).astype(float))\n",
    "                    fix_log_list = ['total_population', 'population_density', 'area', 'median_household_income',\n",
    "                                    'houses_density', 'airport_distance','deaths_per_100000']\n",
    "                    for covar in fix_log_list:\n",
    "                        if covar in covariates_names:\n",
    "                            X_train_train_to_use[county_fips][h][method][covar] = np.log((X_train_train_to_use[county_fips][h][method][covar] + 1).astype(float))\n",
    "                            X_train_val_to_use[county_fips][h][method][covar] = np.log((X_train_val_to_use[county_fips][h][method][covar] + 1).astype(float))\n",
    "                            X_test_to_use[county_fips][h][method][covar] = np.log((X_test_to_use[county_fips][h][method][covar] + 1).astype(float))\n",
    "            \n",
    "            y_train_date[county_fips] = (pd.DataFrame(y_train_train_date).append(pd.DataFrame(y_train_val_date))).reset_index(drop=True)\n",
    "            y_train_train = np.array(y_train_train_date['Target']).reshape(-1)\n",
    "            y_train_val = np.array(y_train_val_date['Target']).reshape(-1)\n",
    "            y_test[county_fips] = np.array(y_test_date[county_fips]['Target']).reshape(-1)\n",
    "            y_train[county_fips] = np.array((pd.DataFrame(y_train_train).append(pd.DataFrame(y_train_val))).reset_index(drop=True)).reshape(-1)\n",
    "\n",
    "#             # find best loss\n",
    "#             if (h==1):\n",
    "#               best_loss = update_best_loss('none_mixed_model', spatial_mode ,county_fips,best_loss,X_train_train_to_use,X_train_val_to_use,\\\n",
    "#                                           y_train_train,y_train_val,None,None,data.columns.drop(['Target','date of day t','county_fips']),\\\n",
    "#                                             numberOfCovariates,maxC)\n",
    "#               # update list of county losses (mode of this list will be used as best loss)\n",
    "#               for method in ['GBM', 'NN']:\n",
    "#                 counties_best_loss_list[method].append(best_loss[method])\n",
    "\n",
    "\n",
    "            covariates_list = []\n",
    "            loom = ProcessLoom(max_runner_cap=len(base_data.columns) * len(none_mixed_methods) + 5)\n",
    "            indx_c = 0\n",
    "            for c in covariates_names:\n",
    "                indx_c += 1\n",
    "                for covariate in data.columns:\n",
    "                    if c.split(' ')[0] in covariate:\n",
    "                        covariates_list.append(covariate)\n",
    "                for method in none_mixed_methods:\n",
    "                    X_train_train_temp = X_train_train_to_use[county_fips][h][method][covariates_list]\n",
    "                    X_train_val_temp = X_train_val_to_use[county_fips][h][method][covariates_list]\n",
    "                    loom.add_function(parallel_run, [method, X_train_train_temp, X_train_val_temp, y_train_train, y_train_val, best_loss, indx_c])\n",
    "                if indx_c == maxC:\n",
    "                    break\n",
    "            parallel_outputs['non_mixed'] = loom.execute()\n",
    "            ind = 0\n",
    "            for c in range(1, numberOfCovariates + 1):\n",
    "                for method in none_mixed_methods:\n",
    "                    y_prediction[county_fips][method][(h, c)], y_prediction_train[county_fips][method][(h, c)] = parallel_outputs['non_mixed'][ind]['output']\n",
    "                    ind += 1\n",
    "                if c == maxC:\n",
    "                    break\n",
    "            filename = env_address + 'validation.out'\n",
    "            my_shelf = shelve.open(filename, 'n')\n",
    "            for key in dir():\n",
    "                try:\n",
    "                    my_shelf[key] = locals()[key]\n",
    "                except:\n",
    "                    print('ERROR shelving: {0}'.format(key))\n",
    "            my_shelf.close()\n",
    "\n",
    "#             # find best loss\n",
    "#             if h == 1 :\n",
    "#               best_loss = update_best_loss('mixed_model', spatial_mode, county_fips,best_loss,None,None,y_train_train,\\\n",
    "#                         y_train_val,y_prediction_train,y_prediction,None,\\\n",
    "#                         numberOfCovariates,maxC)\n",
    "#               # update list of county losses (mode of this list will be used as best loss)\n",
    "#               counties_best_loss_list['MM_NN'].append(best_loss['MM_NN'])\n",
    "\n",
    "            loom = ProcessLoom(max_runner_cap=len(base_data.columns) * len(mixed_methods) + 5)\n",
    "            indx_c = 0\n",
    "            for c in range(1, numberOfCovariates + 1):\n",
    "                indx_c += 1\n",
    "                for mixed_method in mixed_methods:\n",
    "                    y_predictions_test, y_predictions_train = [], []\n",
    "                    y_predictions_test.extend([y_prediction[county_fips]['GBM'][(h, c)], y_prediction[county_fips]['GLM'][(h, c)],\n",
    "                                                y_prediction[county_fips]['KNN'][(h, c)], y_prediction[county_fips]['NN'][(h, c)]])\n",
    "                    y_prediction_test_np = np.array(y_predictions_test).reshape(len(y_predictions_test), -1)\n",
    "                    X_test_mixedModel = pd.DataFrame(y_prediction_test_np.transpose())\n",
    "                    y_predictions_train.extend([y_prediction_train[county_fips]['GBM'][(h, c)], y_prediction_train[county_fips]['GLM'][(h, c)],\n",
    "                                                y_prediction_train[county_fips]['KNN'][(h, c)], y_prediction_train[county_fips]['NN'][(h, c)]])\n",
    "                    y_prediction_train_np = np.array(y_predictions_train).reshape(len(y_predictions_train), -1)\n",
    "                    X_train_mixedModel = pd.DataFrame(y_prediction_train_np.transpose())\n",
    "                    loom.add_function(mixed_parallel_run, [mixed_method, X_train_mixedModel, X_test_mixedModel, y_train_train, y_train_val, best_loss])\n",
    "                if indx_c == maxC:\n",
    "                    break\n",
    "            parallel_outputs['mixed'] = loom.execute()\n",
    "            ind = 0\n",
    "            for c in range(1, numberOfCovariates + 1):\n",
    "                for mixed_method in mixed_methods:\n",
    "                    y_prediction[county_fips][mixed_method][(h, c)], y_prediction_train[county_fips][mixed_method][(h, c)] = parallel_outputs['mixed'][ind]['output']\n",
    "                    y_prediction[county_fips][mixed_method][(h, c)] = np.array(y_prediction[county_fips][mixed_method][(h, c)]).ravel()\n",
    "                    y_prediction_train[county_fips][mixed_method][(h, c)] = np.array(y_prediction_train[county_fips][mixed_method][(h, c)]).ravel()\n",
    "                    ind += 1\n",
    "                if c == maxC:\n",
    "                    break\n",
    "            filename = env_address + 'validation.out'\n",
    "            my_shelf = shelve.open(filename, 'n')\n",
    "            for key in dir():\n",
    "                try:\n",
    "                    my_shelf[key] = locals()[key]\n",
    "                except:\n",
    "                    print('ERROR shelving: {0}'.format(key))\n",
    "            my_shelf.close()\n",
    "        print(\"########################################################################################################\")\n",
    "        number_of_improved_methods = 0\n",
    "        indx_c = 0\n",
    "        covariates_list=['county_fips','date of day t']\n",
    "        for c in covariates_names:\n",
    "            print(indx_c)\n",
    "            indx_c += 1\n",
    "            for covariate in data.columns:\n",
    "                if c.split(' ')[0] in covariate:\n",
    "                    covariates_list.append(covariate)\n",
    "            for method in methods:\n",
    "                X_train_train_temp = flatten(data=X_train_train_to_use, h=h, method=method, covariates_list=covariates_list, state=3)\n",
    "                X_train_val_temp = flatten(data=X_train_val_to_use, h=h, method=method, covariates_list=covariates_list, state=3)\n",
    "                X_test_temp = flatten(data=X_test_to_use, h=h, method=method, covariates_list=covariates_list, state=3)\n",
    "                validation_errors['MAE'][method][(h, indx_c)], validation_errors['MAPE'][method][(h, indx_c)], \\\n",
    "                validation_errors['adj-R2'][method][(h, indx_c)], validation_errors['sec'][method][(h, indx_c)], \\\n",
    "                validation_errors['MASE'][method][(h, indx_c)] = \\\n",
    "                    get_errors(h, indx_c, method, flatten(data=y_prediction, h=h, c=indx_c, method=method, state=1), flatten(data=y_prediction_train, h=h, c=indx_c, method=method, state=1), flatten(data=y_val, h=h, c=indx_c, state=2),\n",
    "                                train_val_MASE_denominator, numberOfSelectedCounties, mode='val')\n",
    "                for error in error_names:\n",
    "                    if validation_errors[error][method][(h, indx_c)] < minError[method][error]:\n",
    "                        minError[method][error] = validation_errors[error][method][(h, indx_c)]\n",
    "                        best_h[method][error] = h\n",
    "                        best_c[method][error] = indx_c\n",
    "                        if error == 'MAPE':\n",
    "                            number_of_improved_methods += 1\n",
    "                            print(method+' improved')\n",
    "                        if error == 'MAPE' and method != 'MM_GLM' and method != 'MM_NN':\n",
    "                            historical_X_train[method] = (X_train_train_temp.append(X_train_val_temp)).reset_index(\n",
    "                                drop=True)\n",
    "                            historical_X_test[method] = X_test_temp\n",
    "                            historical_y_train[method] = flatten(data=y_train, state=5)\n",
    "                            historical_y_test[method] = flatten(data=y_test, state=5)\n",
    "                            historical_y_train_date[method] = flatten(data=y_train_date, state=4)\n",
    "                            historical_y_test_date[method] = flatten(data=y_test_date, state=4)\n",
    "            filename = env_address + 'validation.out'\n",
    "            my_shelf = shelve.open(filename, 'n')\n",
    "            for key in dir():\n",
    "                try:\n",
    "                    my_shelf[key] = locals()[key]\n",
    "                except:\n",
    "                    print('ERROR shelving: {0}'.format(key))\n",
    "            my_shelf.close()\n",
    "            if indx_c == maxC:\n",
    "                break\n",
    "#         if h == 1:\n",
    "#           best_loss = get_best_loss_mode(counties_best_loss_list)\n",
    "        \n",
    "        filename = env_address + 'validation.out'\n",
    "        my_shelf = shelve.open(filename, 'n')  # 'n' for new\n",
    "        for key in dir():\n",
    "            try:\n",
    "                my_shelf[key] = locals()[key]\n",
    "            except:\n",
    "                print('ERROR shelving: {0}'.format(key))\n",
    "        my_shelf.close()\n",
    "        push('logs of h=' + str(h) + ' added')\n",
    "        if (number_of_improved_methods == 0) or (h == maxHistory//2) :\n",
    "          print('number of improved methods for h=',h,':',number_of_improved_methods)\n",
    "          print('jump to test process')\n",
    "          test_process(h, r, target_name,spatial_mode, target_mode,best_h,best_c,historical_X_train,\\\n",
    "                 historical_X_test, historical_y_train_date, historical_y_test_date, best_loss,\\\n",
    "                 numberOfSelectedCounties, covariates_names, maxHistory, test_address, env_address, mail_address)\n",
    "\n",
    "\n",
    "\n",
    "    # plot table for best results\n",
    "    table_data = []\n",
    "    for method in methods:\n",
    "        table_data.append([best_h[method]['MAPE'], best_c[method]['MAPE'], round(minError[method]['MAE'], 2),\n",
    "                            round(minError[method]['MAPE'], 2), round(minError[method]['adj-R2'], 2),\n",
    "                            round(minError[method]['sec'], 2), round(minError[method]['MASE'], 2)])\n",
    "    table_name = 'tabel_of_best_validation_results'\n",
    "    plot_table(table_data, columns_table, methods, table_name, mode='val')\n",
    "    # plot the results of methods on validation set\n",
    "\n",
    "    for error in error_names:\n",
    "        plot_results(3, 2, numberOfCovariates, methods, history, validation_errors[error], complete_error_names[error])\n",
    "\n",
    "    # mail the validation results\n",
    "    selected_for_email = [validation_address]\n",
    "    zip_file_name = 'validation results for h =' + str(maxHistory) + ' #counties=' + str(numberOfSelectedCountiesname)\n",
    "    make_zip(selected_for_email, zip_file_name)\n",
    "    send_email(zip_file_name + '.zip')\n",
    "    push('plots added')\n",
    "    test_process(h, r, target_name,spatial_mode, target_mode,best_h,best_c,historical_X_train,\\\n",
    "                 historical_X_test, historical_y_train_date, historical_y_test_date, best_loss,\\\n",
    "                 numberOfSelectedCounties, covariates_names, maxHistory, test_address, env_address, mail_address)\n",
    "\n",
    "    print(\"y_prediction\", y_prediction)\n",
    "    print(\"y_val\", y_val)\n",
    "    print(validation_errors)\n",
    "    print(\"best_h\", best_h)\n",
    "    print(\"best_c\", best_c)\n",
    "    print(historical_X_train['GBM'].columns.values)\n",
    "    print(historical_X_train['GBM'].shape)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    begin = time.time()\n",
    "    maxHistory = 14\n",
    "    maxC = 100\n",
    "    validation_address = './'+str(iteration)+' results/counties=' + str(numberOfSelectedCountiesname) + ' max_history=' + str(maxHistory) + '/validation/'\n",
    "    test_address = './' +str(iteration)+ ' results/counties=' + str(numberOfSelectedCountiesname) + ' max_history=' + str(maxHistory) + '/test/'\n",
    "    env_address = './' +str(iteration)+ ' results/counties=' + str(numberOfSelectedCountiesname) + ' max_history=' + str(maxHistory) + '/session_parameters/'\n",
    "    mail_address = './'+str(iteration)+' results/counties=' + str(numberOfSelectedCountiesname) + ' max_history=' + str(maxHistory) + '/email/'\n",
    "    if not os.path.exists(mail_address):\n",
    "        os.makedirs(mail_address)\n",
    "    if not os.path.exists(test_address):\n",
    "        os.makedirs(test_address)\n",
    "    if not os.path.exists(validation_address):\n",
    "        os.makedirs(validation_address)\n",
    "    if not os.path.exists(env_address):\n",
    "        os.makedirs(env_address)\n",
    "    push('new folders added')\n",
    "    models_to_log = ['NN', 'GLM', 'GBM']\n",
    "    main(maxHistory, maxC)\n",
    "    end = time.time()\n",
    "    push('final results added')\n",
    "    print(\"The total time of execution in minutes: \", round((end - begin) / 60, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       county_fips  Target date of day t  confirmed t  death t  \\\n",
      "38           53061   154.0      02/29/20          1.0      0.0   \n",
      "39           53061   176.0      03/01/20          2.0      0.0   \n",
      "40           53061   200.0      03/02/20          4.0      1.0   \n",
      "41           53061   254.0      03/03/20          6.0      0.0   \n",
      "42           53061   310.0      03/04/20          8.0      0.0   \n",
      "...            ...     ...           ...          ...      ...   \n",
      "18079        56039   105.0      06/02/20        100.0      0.0   \n",
      "18080        56039   107.0      06/03/20        100.0      0.0   \n",
      "18081        56039   108.0      06/04/20        100.0      0.0   \n",
      "18082        56039   109.0      06/05/20        100.0      0.0   \n",
      "18083        56039   109.0      06/06/20        100.0      0.0   \n",
      "\n",
      "       daily-state-test t  total_population  houses_density  virus-pressure t  \\\n",
      "38                   41.0            814901        150.3485               2.5   \n",
      "39                   73.0            814901        150.3485               1.5   \n",
      "40                  184.0            814901        150.3485               2.5   \n",
      "41                  225.0            814901        150.3485               3.5   \n",
      "42                  245.0            814901        150.3485               5.0   \n",
      "...                   ...               ...             ...               ...   \n",
      "18079               595.0             23081          3.5126               3.5   \n",
      "18080               886.0             23081          3.5126               4.0   \n",
      "18081               361.0             23081          3.5126               6.5   \n",
      "18082               623.0             23081          3.5126               1.5   \n",
      "18083               896.0             23081          3.5126               3.0   \n",
      "\n",
      "       age_40_59  ...   %insured    smokers  diabetes  Religious  \\\n",
      "38            31  ...  93.792360  12.962309       8.2       31.0   \n",
      "39            31  ...  93.792360  12.962309       8.2       31.0   \n",
      "40            31  ...  93.792360  12.962309       8.2       31.0   \n",
      "41            31  ...  93.792360  12.962309       8.2       31.0   \n",
      "42            31  ...  93.792360  12.962309       8.2       31.0   \n",
      "...          ...  ...        ...        ...       ...        ...   \n",
      "18079         29  ...  83.965483  14.546369       2.2       26.0   \n",
      "18080         29  ...  83.965483  14.546369       2.2       26.0   \n",
      "18081         29  ...  83.965483  14.546369       2.2       26.0   \n",
      "18082         29  ...  83.965483  14.546369       2.2       26.0   \n",
      "18083         29  ...  83.965483  14.546369       2.2       26.0   \n",
      "\n",
      "       median_household_income     area  some_college_or_higher  \\\n",
      "38                       87096  2087.27                    68.5   \n",
      "39                       87096  2087.27                    68.5   \n",
      "40                       87096  2087.27                    68.5   \n",
      "41                       87096  2087.27                    68.5   \n",
      "42                       87096  2087.27                    68.5   \n",
      "...                        ...      ...                     ...   \n",
      "18079                    99087  3995.38                    81.2   \n",
      "18080                    99087  3995.38                    81.2   \n",
      "18081                    99087  3995.38                    81.2   \n",
      "18082                    99087  3995.38                    81.2   \n",
      "18083                    99087  3995.38                    81.2   \n",
      "\n",
      "            county_name  state_name  state_fips  \n",
      "38     Snohomish County  Washington          53  \n",
      "39     Snohomish County  Washington          53  \n",
      "40     Snohomish County  Washington          53  \n",
      "41     Snohomish County  Washington          53  \n",
      "42     Snohomish County  Washington          53  \n",
      "...                 ...         ...         ...  \n",
      "18079      Teton County     Wyoming          56  \n",
      "18080      Teton County     Wyoming          56  \n",
      "18081      Teton County     Wyoming          56  \n",
      "18082      Teton County     Wyoming          56  \n",
      "18083      Teton County     Wyoming          56  \n",
      "\n",
      "[8924 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# h is the number of days before day (t)\n",
    "# r indicates how many days after day (t) --> target-day = day(t+r)\n",
    "# target could be number of deaths or number of confirmed \n",
    "def makeHistoricalData(h, r, target, feature_selection, spatial_mode, target_mode, address, iteration):\n",
    "    ''' in this code when h is 1, it means there is no history and we have just one column for each covariate\n",
    "    so when h is 0, we put h equal to 1, because when h is 0 that means there no history (as when h is 1) '''\n",
    "    if h == 0:\n",
    "        h = 1\n",
    "\n",
    "\n",
    "    ##################################################################### imputation\n",
    "\n",
    "    independantOfTimeData = pd.read_csv(address + 'fixed-data.csv')\n",
    "    timeDeapandantData = pd.read_csv(address + 'temporal-data.csv')\n",
    "    \n",
    "    num = len(independantOfTimeData['county_fips'].unique().tolist())//20\n",
    "    fips=independantOfTimeData['county_fips'].unique().tolist()\n",
    "    tempfips=fips[(iteration-1)*num:(iteration)*num]\n",
    "    independantOfTimeData=independantOfTimeData[independantOfTimeData['county_fips'].isin(tempfips)]\n",
    "    timeDeapandantData=timeDeapandantData[timeDeapandantData['county_fips'].isin(tempfips)]\n",
    "    \n",
    "\n",
    "    # impute missing values for tests in first days with min\n",
    "    timeDeapandantData.loc[timeDeapandantData['daily-state-test']<0,'daily-state-test']=abs(timeDeapandantData.loc[timeDeapandantData['daily-state-test']<0,'daily-state-test'])\n",
    "\n",
    "    if spatial_mode=='country':\n",
    "\n",
    "        # Next 3 lines create dataframe contains only daily-state-test to impute this feature\n",
    "        temp=pd.DataFrame(index=timeDeapandantData['county_fips'].unique().tolist(),columns=timeDeapandantData['date'].unique().tolist())\n",
    "        for i in timeDeapandantData['date'].unique():\n",
    "            temp[i]=timeDeapandantData.loc[timeDeapandantData['date']==i,'daily-state-test'].tolist()\n",
    "\n",
    "        # Next line find min daily-state-test performed in each county for impute first days missing values with min\n",
    "        county_min_test=temp.replace(0,np.NaN).T.min()\n",
    "\n",
    "        # impute missing tests for first days with min test performed in each county\n",
    "        for i in temp.columns:\n",
    "            temp.loc[pd.isna(temp[i]),i]=county_min_test[pd.isna(temp[i])]\n",
    "\n",
    "        #replace imputed values in timeDeapandantData\n",
    "        for i in timeDeapandantData['date'].unique():\n",
    "            timeDeapandantData.loc[timeDeapandantData['date']==i,'daily-state-test']=temp[i].tolist()\n",
    "\n",
    "    else:\n",
    "        # for state and county mode we dont impute daily-state-test\n",
    "        timeDeapandantData=timeDeapandantData#[~(pd.isna(timeDeapandantData['daily-state-test']))]\n",
    "\n",
    "\n",
    "\n",
    "    #Next 12 lines remove counties with all missing values for some features (counties with partly missing values have been imputed)\n",
    "    independantOfTime_features_with_nulls=['ventilator_capacity','icu_beds','deaths_per_100000']\n",
    "\n",
    "    for i in independantOfTime_features_with_nulls:\n",
    "        nullind=independantOfTimeData.loc[pd.isnull(independantOfTimeData[i]),'county_fips'].unique()\n",
    "        timeDeapandantData=timeDeapandantData[~timeDeapandantData['county_fips'].isin(nullind)]\n",
    "        independantOfTimeData=independantOfTimeData[~independantOfTimeData['county_fips'].isin(nullind)]\n",
    "\n",
    "    timeDeapandant_features_with_nulls=['social-distancing-travel-distance-grade','social-distancing-total-grade',\n",
    "                                                'temperature','precipitation']\n",
    "\n",
    "    for i in timeDeapandant_features_with_nulls:\n",
    "        nullind=timeDeapandantData.loc[pd.isnull(timeDeapandantData[i]),'county_fips'].unique()\n",
    "        timeDeapandantData=timeDeapandantData[~timeDeapandantData['county_fips'].isin(nullind)]\n",
    "        independantOfTimeData=independantOfTimeData[~independantOfTimeData['county_fips'].isin(nullind)]\n",
    "\n",
    "\n",
    "    ##################################################################### cumulative mode\n",
    "    \n",
    "    \n",
    "    if target_mode == 'cumulative': # make target cumulative by adding the values of the previous day to each day\n",
    "        timeDeapandantData=timeDeapandantData.sort_values(by=['date','county_fips'])\n",
    "\n",
    "        dates=timeDeapandantData['date'].unique()\n",
    "        for i in range(len(dates)-1): \n",
    "            timeDeapandantData.loc[timeDeapandantData['date']==dates[i+1],target]=\\\n",
    "            list(np.array(timeDeapandantData.loc[timeDeapandantData['date']==dates[i+1],target])+\\\n",
    "                 np.array(timeDeapandantData.loc[timeDeapandantData['date']==dates[i],target]))\n",
    "\n",
    "    \n",
    "    ###################################################################### weekly average mode\n",
    "    \n",
    "    \n",
    "    if target_mode == 'weeklyaverage': # make target weekly averaged\n",
    "        def make_weekly(dailydata):\n",
    "            dailydata['date']=dailydata['date'].apply(lambda x: datetime.datetime.strptime(x,'%m/%d/%y'))\n",
    "            dailydata.drop(['weekend'],axis=1,inplace=True)\n",
    "            dailydata.sort_values(by=['date','county_fips'],inplace=True)\n",
    "            numberofcounties=len(dailydata['county_fips'].unique())\n",
    "            numberofweeks=len(dailydata['date'].unique())//7\n",
    "\n",
    "            weeklydata=pd.DataFrame(columns=dailydata.columns.drop('date'))\n",
    "\n",
    "            for i in range(numberofweeks):\n",
    "                temp_df=dailydata.tail(numberofcounties*7) # weekly average of last week for all counties\n",
    "                dailydata=dailydata.iloc[:-(numberofcounties*7),:]\n",
    "                temp_df=temp_df.groupby(['county_fips']).mean().reset_index()\n",
    "                temp_df['date']=numberofweeks-i # week number \n",
    "                weeklydata=weeklydata.append(temp_df)\n",
    "            weeklydata.sort_values(by=['county_fips','date'],inplace=True)\n",
    "            weeklydata=weeklydata.reset_index(drop=True)\n",
    "            return(weeklydata)\n",
    "        \n",
    "        timeDeapandantData=make_weekly(timeDeapandantData)\n",
    "    \n",
    "    \n",
    "    ###################################################################### weekly moving average mode\n",
    "    if target_mode == 'weeklymovingaverage':\n",
    "        def make_moving_weekly_average(dailydata):\n",
    "            dailydata['date']=dailydata['date'].apply(lambda x: datetime.datetime.strptime(x,'%m/%d/%y'))\n",
    "            dailydata.sort_values(by=['date','county_fips'],inplace=True)\n",
    "            numberofcounties=len(dailydata['county_fips'].unique())\n",
    "            numberofdays=len(dailydata['date'].unique())\n",
    "            dates=dailydata['date'].unique()\n",
    "\n",
    "            weeklydata=pd.DataFrame(columns=dailydata.columns)\n",
    "\n",
    "            while numberofdays>=7:\n",
    "                    current_day_previous_week_data=dailydata.tail(numberofcounties*7) \n",
    "                    current_day_data = dailydata.tail(numberofcounties)\n",
    "\n",
    "                    # weekly average of last week for all counties\n",
    "                    current_day_previous_week_data=current_day_previous_week_data.groupby(['county_fips']).mean().reset_index()\n",
    "                    # add weekly moving averaged target of lastweek to last day target\n",
    "                    current_day_data.loc[:,(target)] = current_day_previous_week_data.loc[:,(target)].tolist()\n",
    "\n",
    "                    weeklydata = weeklydata.append(current_day_data)\n",
    "                    dailydata=dailydata.iloc[:-(numberofcounties),:]# remove last day for all counties from daily data\n",
    "                    numberofdays = numberofdays-1\n",
    "            weeklydata=weeklydata.sort_values(by=['county_fips','date'])\n",
    "            weeklydata['date']=weeklydata['date'].apply(lambda x: datetime.datetime.strftime(x,'%m/%d/%y'))\n",
    "\n",
    "            return(weeklydata)\n",
    "        timeDeapandantData=make_moving_weekly_average(timeDeapandantData)\n",
    "\n",
    "    \n",
    "    ##################################################################\n",
    "\n",
    "\n",
    "    allData = pd.merge(independantOfTimeData, timeDeapandantData, on='county_fips')\n",
    "    allData = allData.sort_values(by=['date', 'county_fips'])\n",
    "    allData = allData.reset_index(drop=True)\n",
    "    # this columns are not numercal and wouldn't be included in correlation matrix, we store them to concatenate them later\n",
    "    notNumericlData = allData[['county_name', 'state_name', 'county_fips', 'state_fips', 'date']]\n",
    "    allData=allData.drop(['county_name', 'state_name', 'county_fips', 'state_fips', 'date'],axis=1)\n",
    "\n",
    "    # next 19 lines ranking columns with mRMR\n",
    "    cor=allData.corr().abs()\n",
    "    valid_feature=cor.index.drop([target])\n",
    "    overall_rank_df=pd.DataFrame(index=cor.index,columns=['mrmr_rank'])\n",
    "    for i in cor.index:\n",
    "        overall_rank_df.loc[i,'mrmr_rank']=cor.loc[i,target]-cor.loc[i,valid_feature].mean()\n",
    "    overall_rank_df=overall_rank_df.sort_values(by='mrmr_rank',ascending=False)\n",
    "    overall_rank=overall_rank_df.index.tolist()\n",
    "    final_rank=[]\n",
    "    final_rank=overall_rank[0:2]\n",
    "    overall_rank=overall_rank[2:]\n",
    "    while len(overall_rank)>0:\n",
    "        temp=pd.DataFrame(index=overall_rank,columns=['mrmr_rank'])\n",
    "        for i in overall_rank:\n",
    "            temp.loc[i,'mrmr_rank']=cor.loc[i,target]-cor.loc[i,final_rank[1:]].mean()\n",
    "        temp=temp.sort_values(by='mrmr_rank',ascending=False)\n",
    "        final_rank.append(temp.index[0])\n",
    "        overall_rank.remove(temp.index[0])\n",
    "\n",
    "    # next 6 lines arranges columns in order of correlations with target or by mRMR rank\n",
    "    if(feature_selection=='mrmr'):\n",
    "        ix=final_rank\n",
    "    else:\n",
    "        ix = allData.corr().abs().sort_values(target, ascending=False).index\n",
    "\n",
    "    allData = allData.loc[:, ix]\n",
    "    allData = pd.concat([allData, notNumericlData], axis=1)\n",
    "\n",
    "    nameOfTimeDependantCovariates = timeDeapandantData.columns.values.tolist()\n",
    "    nameOfAllCovariates = allData.columns.values.tolist()\n",
    "\n",
    "    result = pd.DataFrame()  # we store historical data in this dataframe\n",
    "    totalNumberOfCounties = len(allData['county_fips'].unique())\n",
    "    totalNumberOfDays = len(allData['date'].unique())\n",
    "\n",
    "    # in this loop we make historical data\n",
    "    for name in nameOfAllCovariates:\n",
    "        # if covariate is time dependant\n",
    "        if name in nameOfTimeDependantCovariates and name not in ['date', 'county_fips']:\n",
    "            temporalDataFrame = allData[[name]] # selecting column of the covariate that is being processed\n",
    "            threshold = 0\n",
    "            while threshold != h:\n",
    "                # get value of covariate that is being processed in first (totalNumberOfDays-h-r+1) days\n",
    "                temp = temporalDataFrame.head((totalNumberOfDays-h-r+1)*totalNumberOfCounties).copy().reset_index(drop=True)\n",
    "                temp.rename(columns={name: (name + ' t-' + str(h-threshold-1))}, inplace=True) # renaming column\n",
    "                result = pd.concat([result, temp], axis=1)\n",
    "                # deleting the values in first day in temporalDataFrame dataframe (similiar to shift)\n",
    "                temporalDataFrame = temporalDataFrame.iloc[totalNumberOfCounties:]\n",
    "                threshold += 1\n",
    "        # if covariate is independant of time\n",
    "        elif name not in nameOfTimeDependantCovariates and name not in ['date', 'county_fips']:\n",
    "            # we dont need covariates that is fixed for each county in county mode\n",
    "            # but also we need county and state name in all modes\n",
    "            if (spatial_mode != 'county') or (name in ['county_name', 'state_name', 'state_fips']):\n",
    "              temporalDataFrame = allData[[name]]\n",
    "              temp = temporalDataFrame.head((totalNumberOfDays-h-r+1)*totalNumberOfCounties).copy().reset_index(drop=True)\n",
    "              result = pd.concat([result, temp], axis=1)\n",
    "\n",
    "    # next 3 lines is for adding FIPS code to final dataframe\n",
    "    temporalDataFrame = allData[['county_fips']]\n",
    "    temp = temporalDataFrame.head((totalNumberOfDays-h-r+1)*totalNumberOfCounties).copy().reset_index(drop=True)\n",
    "    result.insert(0, 'county_fips', temp)\n",
    "\n",
    "    # next 3 lines is for adding date of day (t) to final dataframe\n",
    "    temporalDataFrame = allData[['date']]\n",
    "    temporalDataFrame = temporalDataFrame[totalNumberOfCounties*(h-1):]\n",
    "    temp = temporalDataFrame.head((totalNumberOfDays-h-r+1)*totalNumberOfCounties).copy().reset_index(drop=True)\n",
    "    result.insert(1, 'date of day t', temp)\n",
    "\n",
    "    # next 3 lines is for adding target to final dataframe\n",
    "    temporalDataFrame = allData[[target]]\n",
    "    temporalDataFrame = temporalDataFrame.tail((totalNumberOfDays-h-r+1)*totalNumberOfCounties).reset_index(drop=True)\n",
    "    result.insert(1, 'Target', temporalDataFrame)\n",
    "    for i in result.columns:\n",
    "        if i.endswith('t-0'):\n",
    "            result.rename(columns={i: i[:-2]}, inplace=True)\n",
    "\n",
    "    result.dropna(inplace=True)\n",
    "\n",
    "    result=result.sort_values(by=['county_fips','date of day t']).reset_index(drop=True)\n",
    "    totalNumberOfDays=len(result['date of day t'].unique())\n",
    "    county_end_index=0\n",
    "    overall_non_zero_index=list()\n",
    "    for i in result['county_fips'].unique():\n",
    "        county_data = result[result['county_fips']==i]#.reset_index(drop=True)\n",
    "        county_end_index = county_end_index+len(result[result['county_fips']==i])\n",
    "\n",
    "        # we dont use counties with zero values for target variable in all history dates\n",
    "        if (county_data[target+' t'].sum()>0):\n",
    "            if h==1:\n",
    "                # find first row index with non_zero values for target variable in all history dates when history length<7 \n",
    "                first_non_zero_date_index = county_data[target+' t'].ne(0).idxmax()\n",
    "            elif h<7:\n",
    "                # find first row index with non_zero values for target variable in all history dates when history length<7 \n",
    "                first_non_zero_date_index = county_data[target+' t-'+str(h-1)].ne(0).idxmax()\n",
    "            else:\n",
    "                # find first row index with non_zero values for target variable in 7 last days of history when history length>7 \n",
    "                first_non_zero_date_index = county_data[target+' t'].ne(0).idxmax()+7\n",
    "\n",
    "            zero_removed_county_index=[i for i in range(first_non_zero_date_index,county_end_index)]\n",
    "            \n",
    "            # we choose r days for test and r days for validation so at least we must have r days for train -> 3*r\n",
    "            if len(zero_removed_county_index) >= 3*r:\n",
    "                    overall_non_zero_index = overall_non_zero_index + zero_removed_county_index\n",
    "   \n",
    "\n",
    "    \n",
    "    zero_removed_data=result.loc[overall_non_zero_index,:]\n",
    "    result=result.reset_index()\n",
    "    # we use reindex to avoid pandas warnings\n",
    "    zero_removed_data=result.loc[result['index'].isin(overall_non_zero_index),:]\n",
    "    zero_removed_data=zero_removed_data.drop(['index'],axis=1)\n",
    "    result = zero_removed_data\n",
    "\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    h = 0\n",
    "    r = 14\n",
    "    target = 'confirmed'\n",
    "    feature_selection = 'mrmr'\n",
    "    spatial_mode = 'country'\n",
    "    target_mode = 'cumulative'\n",
    "    address = './'\n",
    "    iteration = 20\n",
    "    result = makeHistoricalData(h, r, target, feature_selection, spatial_mode, target_mode,address, iteration)\n",
    "    # Storing the result in a csv file\n",
    "    # result.to_csv('dataset_h=' + str(h) + '.csv', mode='w', index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ex makehist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# h is the number of days before day (t)\n",
    "# r indicates how many days after day (t) --> target-day = day(t+r)\n",
    "# target could be number of deaths or number of confirmed \n",
    "def makeHistoricalData(h, r, target, feature_selection, spatial_mode, target_mode, address):\n",
    "    ''' in this code when h is 1, it means there is no history and we have just one column for each covariate\n",
    "    so when h is 0, we put h equal to 1, because when h is 0 that means there no history (as when h is 1) '''\n",
    "    if h == 0:\n",
    "        h = 1\n",
    "\n",
    "\n",
    "    ##################################################################### imputation\n",
    "\n",
    "    independantOfTimeData = pd.read_csv(address + 'fixed-data.csv')\n",
    "    timeDeapandantData = pd.read_csv(address + 'temporal-data.csv')\n",
    "\n",
    "    # impute missing values for tests in first days with min\n",
    "    timeDeapandantData.loc[timeDeapandantData['daily-state-test']<0,'daily-state-test']=abs(timeDeapandantData.loc[timeDeapandantData['daily-state-test']<0,'daily-state-test'])\n",
    "\n",
    "    if spatial_mode=='country':\n",
    "\n",
    "        # Next 3 lines create dataframe contains only daily-state-test to impute this feature\n",
    "        temp=pd.DataFrame(index=timeDeapandantData['county_fips'].unique().tolist(),columns=timeDeapandantData['date'].unique().tolist())\n",
    "        for i in timeDeapandantData['date'].unique():\n",
    "            temp[i]=timeDeapandantData.loc[timeDeapandantData['date']==i,'daily-state-test'].tolist()\n",
    "\n",
    "        # Next line find min daily-state-test performed in each county for impute first days missing values with min\n",
    "        county_min_test=temp.replace(0,np.NaN).T.min()\n",
    "\n",
    "        # impute missing tests for first days with min test performed in each county\n",
    "        for i in temp.columns:\n",
    "            temp.loc[pd.isna(temp[i]),i]=county_min_test[pd.isna(temp[i])]\n",
    "\n",
    "        #replace imputed values in timeDeapandantData\n",
    "        for i in timeDeapandantData['date'].unique():\n",
    "            timeDeapandantData.loc[timeDeapandantData['date']==i,'daily-state-test']=temp[i].tolist()\n",
    "\n",
    "    else:\n",
    "        # for state and county mode we dont impute daily-state-test\n",
    "        timeDeapandantData=timeDeapandantData#[~(pd.isna(timeDeapandantData['daily-state-test']))]\n",
    "\n",
    "\n",
    "\n",
    "    #Next 12 lines remove counties with all missing values for some features (counties with partly missing values have been imputed)\n",
    "    independantOfTime_features_with_nulls=['ventilator_capacity','icu_beds','deaths_per_100000']\n",
    "\n",
    "    for i in independantOfTime_features_with_nulls:\n",
    "        nullind=independantOfTimeData.loc[pd.isnull(independantOfTimeData[i]),'county_fips'].unique()\n",
    "        timeDeapandantData=timeDeapandantData[~timeDeapandantData['county_fips'].isin(nullind)]\n",
    "        independantOfTimeData=independantOfTimeData[~independantOfTimeData['county_fips'].isin(nullind)]\n",
    "\n",
    "    timeDeapandant_features_with_nulls=['social-distancing-travel-distance-grade','social-distancing-total-grade',\n",
    "                                                'temperature','precipitation']\n",
    "\n",
    "    for i in timeDeapandant_features_with_nulls:\n",
    "        nullind=timeDeapandantData.loc[pd.isnull(timeDeapandantData[i]),'county_fips'].unique()\n",
    "        timeDeapandantData=timeDeapandantData[~timeDeapandantData['county_fips'].isin(nullind)]\n",
    "        independantOfTimeData=independantOfTimeData[~independantOfTimeData['county_fips'].isin(nullind)]\n",
    "\n",
    "\n",
    "    ##################################################################### cumulative mode\n",
    "    \n",
    "    \n",
    "    if target_mode == 'cumulative': # make target cumulative by adding the values of the previous day to each day\n",
    "        timeDeapandantData=timeDeapandantData.sort_values(by=['date','county_fips'])\n",
    "\n",
    "        dates=timeDeapandantData['date'].unique()\n",
    "        for i in range(len(dates)-1): \n",
    "            timeDeapandantData.loc[timeDeapandantData['date']==dates[i+1],target]=\\\n",
    "            list(np.array(timeDeapandantData.loc[timeDeapandantData['date']==dates[i+1],target])+\\\n",
    "                 np.array(timeDeapandantData.loc[timeDeapandantData['date']==dates[i],target]))\n",
    "\n",
    "    \n",
    "    ###################################################################### weekly average mode\n",
    "    \n",
    "    \n",
    "    if target_mode == 'weeklyaverage': # make target weekly averaged\n",
    "        def make_weekly(dailydata):\n",
    "            dailydata['date']=dailydata['date'].apply(lambda x: datetime.datetime.strptime(x,'%m/%d/%y'))\n",
    "            dailydata.drop(['weekend'],axis=1,inplace=True)\n",
    "            dailydata.sort_values(by=['date','county_fips'],inplace=True)\n",
    "            numberofcounties=len(dailydata['county_fips'].unique())\n",
    "            numberofweeks=len(dailydata['date'].unique())//7\n",
    "\n",
    "            weeklydata=pd.DataFrame(columns=dailydata.columns.drop('date'))\n",
    "\n",
    "            for i in range(numberofweeks):\n",
    "                temp_df=dailydata.tail(numberofcounties*7) # weekly average of last week for all counties\n",
    "                dailydata=dailydata.iloc[:-(numberofcounties*7),:]\n",
    "                temp_df=temp_df.groupby(['county_fips']).mean().reset_index()\n",
    "                temp_df['date']=numberofweeks-i # week number \n",
    "                weeklydata=weeklydata.append(temp_df)\n",
    "            weeklydata.sort_values(by=['county_fips','date'],inplace=True)\n",
    "            weeklydata=weeklydata.reset_index(drop=True)\n",
    "            return(weeklydata)\n",
    "        \n",
    "        timeDeapandantData=make_weekly(timeDeapandantData)\n",
    "    \n",
    "    \n",
    "    ###################################################################### weekly moving average mode\n",
    "    if target_mode == 'weeklymovingaverage':\n",
    "        def make_moving_weekly_average(dailydata):\n",
    "            dailydata['date']=dailydata['date'].apply(lambda x: datetime.datetime.strptime(x,'%m/%d/%y'))\n",
    "            dailydata.sort_values(by=['date','county_fips'],inplace=True)\n",
    "            numberofcounties=len(dailydata['county_fips'].unique())\n",
    "            numberofdays=len(dailydata['date'].unique())\n",
    "            dates=dailydata['date'].unique()\n",
    "\n",
    "            weeklydata=pd.DataFrame(columns=dailydata.columns)\n",
    "\n",
    "            while numberofdays>=7:\n",
    "                    current_day_previous_week_data=dailydata.tail(numberofcounties*7) \n",
    "                    current_day_data = dailydata.tail(numberofcounties)\n",
    "\n",
    "                    # weekly average of last week for all counties\n",
    "                    current_day_previous_week_data=current_day_previous_week_data.groupby(['county_fips']).mean().reset_index()\n",
    "                    # add weekly moving averaged target of lastweek to last day target\n",
    "                    current_day_data.loc[:,(target)] = current_day_previous_week_data.loc[:,(target)].tolist()\n",
    "\n",
    "                    weeklydata = weeklydata.append(current_day_data)\n",
    "                    dailydata=dailydata.iloc[:-(numberofcounties),:]# remove last day for all counties from daily data\n",
    "                    numberofdays = numberofdays-1\n",
    "            weeklydata=weeklydata.sort_values(by=['county_fips','date'])\n",
    "            weeklydata['date']=weeklydata['date'].apply(lambda x: datetime.datetime.strftime(x,'%m/%d/%y'))\n",
    "\n",
    "            return(weeklydata)\n",
    "        timeDeapandantData=make_moving_weekly_average(timeDeapandantData)\n",
    "\n",
    "    \n",
    "    ##################################################################\n",
    "\n",
    "\n",
    "    allData = pd.merge(independantOfTimeData, timeDeapandantData, on='county_fips')\n",
    "    allData = allData.sort_values(by=['date', 'county_fips'])\n",
    "    allData = allData.reset_index(drop=True)\n",
    "    # this columns are not numercal and wouldn't be included in correlation matrix, we store them to concatenate them later\n",
    "    notNumericlData = allData[['county_name', 'state_name', 'county_fips', 'state_fips', 'date']]\n",
    "    allData=allData.drop(['county_name', 'state_name', 'county_fips', 'state_fips', 'date'],axis=1)\n",
    "\n",
    "    # next 19 lines ranking columns with mRMR\n",
    "    cor=allData.corr().abs()\n",
    "    valid_feature=cor.index.drop([target])\n",
    "    overall_rank_df=pd.DataFrame(index=cor.index,columns=['mrmr_rank'])\n",
    "    for i in cor.index:\n",
    "        overall_rank_df.loc[i,'mrmr_rank']=cor.loc[i,target]-cor.loc[i,valid_feature].mean()\n",
    "    overall_rank_df=overall_rank_df.sort_values(by='mrmr_rank',ascending=False)\n",
    "    overall_rank=overall_rank_df.index.tolist()\n",
    "    final_rank=[]\n",
    "    final_rank=overall_rank[0:2]\n",
    "    overall_rank=overall_rank[2:]\n",
    "    while len(overall_rank)>0:\n",
    "        temp=pd.DataFrame(index=overall_rank,columns=['mrmr_rank'])\n",
    "        for i in overall_rank:\n",
    "            temp.loc[i,'mrmr_rank']=cor.loc[i,target]-cor.loc[i,final_rank[1:]].mean()\n",
    "        temp=temp.sort_values(by='mrmr_rank',ascending=False)\n",
    "        final_rank.append(temp.index[0])\n",
    "        overall_rank.remove(temp.index[0])\n",
    "\n",
    "    # next 6 lines arranges columns in order of correlations with target or by mRMR rank\n",
    "    if(feature_selection=='mrmr'):\n",
    "        ix=final_rank\n",
    "    else:\n",
    "        ix = allData.corr().abs().sort_values(target, ascending=False).index\n",
    "\n",
    "    allData = allData.loc[:, ix]\n",
    "    allData = pd.concat([allData, notNumericlData], axis=1)\n",
    "\n",
    "    nameOfTimeDependantCovariates = timeDeapandantData.columns.values.tolist()\n",
    "    nameOfAllCovariates = allData.columns.values.tolist()\n",
    "\n",
    "    result = pd.DataFrame()  # we store historical data in this dataframe\n",
    "    totalNumberOfCounties = len(allData['county_fips'].unique())\n",
    "    totalNumberOfDays = len(allData['date'].unique())\n",
    "\n",
    "    # in this loop we make historical data\n",
    "    for name in nameOfAllCovariates:\n",
    "        # if covariate is time dependant\n",
    "        if name in nameOfTimeDependantCovariates and name not in ['date', 'county_fips']:\n",
    "            temporalDataFrame = allData[[name]] # selecting column of the covariate that is being processed\n",
    "            threshold = 0\n",
    "            while threshold != h:\n",
    "                # get value of covariate that is being processed in first (totalNumberOfDays-h-r+1) days\n",
    "                temp = temporalDataFrame.head((totalNumberOfDays-h-r+1)*totalNumberOfCounties).copy().reset_index(drop=True)\n",
    "                temp.rename(columns={name: (name + ' t-' + str(h-threshold-1))}, inplace=True) # renaming column\n",
    "                result = pd.concat([result, temp], axis=1)\n",
    "                # deleting the values in first day in temporalDataFrame dataframe (similiar to shift)\n",
    "                temporalDataFrame = temporalDataFrame.iloc[totalNumberOfCounties:]\n",
    "                threshold += 1\n",
    "        # if covariate is independant of time\n",
    "        elif name not in nameOfTimeDependantCovariates and name not in ['date', 'county_fips']:\n",
    "            # we dont need covariates that is fixed for each county in county mode\n",
    "            # but also we need county and state name in all modes\n",
    "            if (spatial_mode != 'county') or (name in ['county_name', 'state_name', 'state_fips']):\n",
    "              temporalDataFrame = allData[[name]]\n",
    "              temp = temporalDataFrame.head((totalNumberOfDays-h-r+1)*totalNumberOfCounties).copy().reset_index(drop=True)\n",
    "              result = pd.concat([result, temp], axis=1)\n",
    "\n",
    "    # next 3 lines is for adding FIPS code to final dataframe\n",
    "    temporalDataFrame = allData[['county_fips']]\n",
    "    temp = temporalDataFrame.head((totalNumberOfDays-h-r+1)*totalNumberOfCounties).copy().reset_index(drop=True)\n",
    "    result.insert(0, 'county_fips', temp)\n",
    "\n",
    "    # next 3 lines is for adding date of day (t) to final dataframe\n",
    "    temporalDataFrame = allData[['date']]\n",
    "    temporalDataFrame = temporalDataFrame[totalNumberOfCounties*(h-1):]\n",
    "    temp = temporalDataFrame.head((totalNumberOfDays-h-r+1)*totalNumberOfCounties).copy().reset_index(drop=True)\n",
    "    result.insert(1, 'date of day t', temp)\n",
    "\n",
    "    # next 3 lines is for adding target to final dataframe\n",
    "    temporalDataFrame = allData[[target]]\n",
    "    temporalDataFrame = temporalDataFrame.tail((totalNumberOfDays-h-r+1)*totalNumberOfCounties).reset_index(drop=True)\n",
    "    result.insert(1, 'Target', temporalDataFrame)\n",
    "    for i in result.columns:\n",
    "        if i.endswith('t-0'):\n",
    "            result.rename(columns={i: i[:-2]}, inplace=True)\n",
    "\n",
    "    result.dropna(inplace=True)\n",
    "\n",
    "    result=result.sort_values(by=['county_fips','date of day t']).reset_index(drop=True)\n",
    "    totalNumberOfDays=len(result['date of day t'].unique())\n",
    "    county_end_index=0\n",
    "    overall_non_zero_index=list()\n",
    "    for i in result['county_fips'].unique():\n",
    "        county_data = result[result['county_fips']==i]#.reset_index(drop=True)\n",
    "        county_end_index = county_end_index+len(result[result['county_fips']==i])\n",
    "\n",
    "        # we dont use counties with zero values for target variable in all history dates\n",
    "        if (county_data[target+' t'].sum()>0):\n",
    "            if h==1:\n",
    "                # find first row index with non_zero values for target variable in all history dates when history length<7 \n",
    "                first_non_zero_date_index = county_data[target+' t'].ne(0).idxmax()\n",
    "            elif h<7:\n",
    "                # find first row index with non_zero values for target variable in all history dates when history length<7 \n",
    "                first_non_zero_date_index = county_data[target+' t-'+str(h-1)].ne(0).idxmax()\n",
    "            else:\n",
    "                # find first row index with non_zero values for target variable in 7 last days of history when history length>7 \n",
    "                first_non_zero_date_index = county_data[target+' t'].ne(0).idxmax()+7\n",
    "\n",
    "            zero_removed_county_index=[i for i in range(first_non_zero_date_index,county_end_index)]\n",
    "            \n",
    "            # we choose r days for test and r days for validation so at least we must have r days for train -> 3*r\n",
    "            if len(zero_removed_county_index) >= 3*r:\n",
    "                    overall_non_zero_index = overall_non_zero_index + zero_removed_county_index\n",
    "   \n",
    "\n",
    "    \n",
    "    zero_removed_data=result.loc[overall_non_zero_index,:]\n",
    "    result=result.reset_index()\n",
    "    # we use reindex to avoid pandas warnings\n",
    "    zero_removed_data=result.loc[result['index'].isin(overall_non_zero_index),:]\n",
    "    zero_removed_data=zero_removed_data.drop(['index'],axis=1)\n",
    "    result = zero_removed_data\n",
    "\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    h = 0\n",
    "    r = 14\n",
    "    target = 'confirmed'\n",
    "    feature_selection = 'mrmr'\n",
    "    spatial_mode = 'country'\n",
    "    target_mode = 'cumulative'\n",
    "    address = './'\n",
    "    # result = makeHistoricalData(h, r, target, feature_selection, spatial_mode, target_mode,address)\n",
    "    # Storing the result in a csv file\n",
    "    # result.to_csv('dataset_h=' + str(h) + '.csv', mode='w', index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
