{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxMrtlhTGglO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!add-apt-repository ppa:deadsnakes/ppa\n",
        "!apt-get update\n",
        "!apt-get install python3.7\n",
        "!apt-get install python3.7-dev\n",
        "\n",
        "!wget https://bootstrap.pypa.io/get-pip.py && python3.7 get-pip.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjDKPpQrkFzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT_Yg7x-IqJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i1R2ce4pf8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -la 'gdrive/My Drive/cnn'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cjjmwHUCFsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import csv\n",
        "from datetime import date\n",
        "from datetime import timedelta\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, Dense, BatchNormalization, MaxPooling2D\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from numpy import array\n",
        "\n",
        "_CSV_Directory_ = ''\n",
        "_JSON_Directory_ = ''\n",
        "\n",
        "startDay = datetime.datetime.strptime('2020-01-22', '%Y-%m-%d')\n",
        "endDay = datetime.datetime.strptime('2020-05-08', '%Y-%m-%d')\n",
        "dayLen = (endDay - startDay).days\n",
        "dataTrain = []\n",
        "dataTest = []\n",
        "hashCounties = [-1] * 78031     #78030 is biggest county fips\n",
        "\n",
        "countiesData_temporal = {}\n",
        "countiesData_fix = {}\n",
        "\n",
        "input_shape = [0, 0, 0, 0]\n",
        "\n",
        "def loadIntersection(jsonFilename):\n",
        "    jsonMetaData = []\n",
        "    with open(_JSON_Directory_ + jsonFilename) as jsonFile:\n",
        "        jsonMetaData = json.load(jsonFile)\n",
        "    return jsonMetaData\n",
        "\n",
        "def loadCounties(csvFilename):\n",
        "    csvData = []\n",
        "    with open(_CSV_Directory_ + csvFilename) as csvFile:\n",
        "        csvDriver = csv.DictReader(csvFile)\n",
        "        for row in csvDriver:\n",
        "            csvData.append(row)\n",
        "    return csvData\n",
        "\n",
        "# Compare two date. This function implemented for specefic use, getting baseDay object and counter, then compare it to targetDay in our Data format(e.g. 05/18/20)\n",
        "def dayComp(baseDay, dayCounter, targetDay):\n",
        "    day1 = (baseDay + timedelta(days=dayCounter)).isoformat()\n",
        "    day2 = datetime.datetime.strptime(targetDay, '%m/%d/%y').strftime('%Y-%m-%d')\n",
        "    if (day1 == day2):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def fromIsotoDataFormat(day):\n",
        "    return day.strftime('%m/%d/%y')\n",
        "\n",
        "def init_hashCounties():\n",
        "    counties = loadCounties('gdrive/My Drive/cnn/full-data-county-fips.csv')\n",
        "    for i in range(len(counties)):\n",
        "        hashCounties[int(counties[i]['county_fips'], 10)] = i\n",
        "\n",
        "def binary_search(target_fips, target_date):\n",
        "    global countiesData_temporal\n",
        "    target = (target_fips, datetime.datetime.strptime(target_date, '%Y-%m-%d'))\n",
        "\n",
        "    l = 0\n",
        "    r = len(countiesData_temporal)\n",
        "\n",
        "    # Find first row of target county\n",
        "    while (1):\n",
        "        mid = (r - l) // 2\n",
        "        fips = int(countiesData_temporal[l + mid]['county_fips'], 10)\n",
        "\n",
        "        if (fips == target[0] and l + mid > 0 and int(countiesData_temporal[l + mid - 1]['county_fips'], 10) != target[0]):\n",
        "            l = l + mid\n",
        "            r = l + 1000\n",
        "            break\n",
        "\n",
        "        elif (fips >= target[0]):\n",
        "            r = l + mid\n",
        "\n",
        "        else:\n",
        "            l = l + mid + 1\n",
        "\n",
        "        if (r == l):\n",
        "            return -1\n",
        "\n",
        "    target_daysFromStart = (target[1] - startDay).days\n",
        "    if (target_daysFromStart <= dayLen):\n",
        "        return l + target_daysFromStart\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def calculateIndex(target_fips, target_date):\n",
        "    target = (target_fips, datetime.datetime.strptime(target_date, '%Y-%m-%dT%H:%M:%S'))\n",
        "\n",
        "    target_daysFromStart = (target[1] - startDay).days\n",
        "    target_countiesFromStart = hashCounties[target[0]]\n",
        "\n",
        "    if (target_daysFromStart <= dayLen and target_countiesFromStart != -1):\n",
        "        index = target_countiesFromStart * (dayLen + 1) + target_daysFromStart\n",
        "        return (index, target_countiesFromStart)\n",
        "    else:\n",
        "        return (-1, target_countiesFromStart)\n",
        "\n",
        "def calculateGridData(counties):\n",
        "    global countiesData_temporal, countiesData_fix\n",
        "    confirmed = 0\n",
        "    death = 0\n",
        "    virusPressure = 0\n",
        "    virusPressure_weightSum = 0\n",
        "    meat_plants = 0\n",
        "    social_distancing_visitation_grade = 0\n",
        "    social_distancing_visitation_grade_weightSum = 0\n",
        "    population = 0\n",
        "    area = 0\n",
        "    population_density = 0\n",
        "    longitude = 0\n",
        "    longitude_sum = 0\n",
        "    social_distancing_travel_distance_grade = 0\n",
        "    social_distancing_travel_distance_grade_weightSum = 0\n",
        "    houses = 0\n",
        "    houses_density = 0\n",
        "    for county in counties:\n",
        "        index_temporal, index_fix = calculateIndex(county['fips'], (startDay + timedelta(days=i)).isoformat())\n",
        "        if (index_temporal != -1):\n",
        "            confirmed += round(float(countiesData_temporal[index_temporal]['confirmed']) * county['percent'])\n",
        "            death += round(float(countiesData_temporal[index_temporal]['death']) * county['percent'])\n",
        "            meat_plants += round(int(countiesData_fix[index_fix]['meat_plants'], 10) * county['percent'])\n",
        "            virusPressure += float(countiesData_temporal[index_temporal]['virus-pressure']) * county['percent']\n",
        "            virusPressure_weightSum += county['percent']\n",
        "            social_distancing_visitation_grade += float(countiesData_temporal[index_temporal][ 'social-distancing-visitation-grade']) * county['percent']\n",
        "            social_distancing_visitation_grade_weightSum += county['percent']\n",
        "            population += round(int(countiesData_fix[index_fix]['total_population'], 10) * county['percent'])\n",
        "            area += float(countiesData_fix[index_fix]['area']) * county['percent']\n",
        "            longitude += float(countiesData_fix[index_fix]['longitude'])\n",
        "            longitude_sum += 1\n",
        "            social_distancing_travel_distance_grade += float(countiesData_temporal[index_temporal]['social-distancing-travel-distance-grade']) * county['percent']\n",
        "            social_distancing_travel_distance_grade_weightSum += county['percent']\n",
        "            houses += float(countiesData_fix[index_fix]['houses_density']) * float(countiesData_fix[index_fix]['area']) * county['percent']\n",
        "\n",
        "    if virusPressure_weightSum != 0:\n",
        "        virusPressure /= virusPressure_weightSum\n",
        "    if social_distancing_visitation_grade_weightSum != 0:\n",
        "        social_distancing_visitation_grade /= social_distancing_visitation_grade_weightSum\n",
        "    if area != 0:\n",
        "        population_density = round(population / area, 2)\n",
        "        houses_density = round(houses / area, 2)\n",
        "    if longitude_sum != 0:\n",
        "        longitude /= longitude_sum\n",
        "    if social_distancing_travel_distance_grade_weightSum != 0:\n",
        "        social_distancing_travel_distance_grade /= social_distancing_travel_distance_grade_weightSum\n",
        "\n",
        "    return [confirmed, round(virusPressure, 2), meat_plants, death, round(social_distancing_visitation_grade, 1), population_density, population, round(longitude, 3), round(social_distancing_travel_distance_grade, 1), houses_density]\n",
        "\n",
        "def init_days():\n",
        "    global startDay\n",
        "    global endDay\n",
        "    global dayLen\n",
        "    startDay = datetime.datetime.strptime(countiesData_temporal[0]['date'], '%m/%d/%y')\n",
        "    endDay = startDay\n",
        "    \n",
        "    for row in countiesData_temporal:\n",
        "        day = datetime.datetime.strptime(row['date'], '%m/%d/%y')\n",
        "        if day > endDay:\n",
        "            endDay = day\n",
        "            dayLen = (endDay - startDay).days\n",
        "\n",
        "        elif day == startDay and row != countiesData_temporal[0]:\n",
        "            break\n",
        "\n",
        "def split_d4Datas(imageArray, data_index):\n",
        "    output = []\n",
        "    for i in range(len(imageArray)):\n",
        "        output.append([imageArray[i][data_index]])\n",
        "\n",
        "    return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPqZtXWpGYam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# time_mainStart = time.time()\n",
        "\n",
        "gridIntersection = loadIntersection('gdrive/My Drive/cnn/map_intersection_1.json')\n",
        "countiesData_temporal = loadCounties('gdrive/My Drive/cnn/full-temporal-data.csv')\n",
        "countiesData_fix = loadCounties('gdrive/My Drive/cnn/full-fixed-data.csv')\n",
        "\n",
        "init_hashCounties()\n",
        "init_days()\n",
        "\n",
        "################################################################ creating image array(CNN input) ### Binary Search\n",
        "\n",
        "# time_imageCreation = time.time()\n",
        "\n",
        "# each row on imageArray include image data on day i\n",
        "imageArray = []\n",
        "\n",
        "for i in range(dayLen):\n",
        "    grid = []\n",
        "    for x in range(len(gridIntersection)):\n",
        "        gridRow = []\n",
        "        for y in range(len(gridIntersection[x])):\n",
        "            gridCell = calculateGridData(gridIntersection[x][y])\n",
        "            gridRow.append(gridCell)\n",
        "        grid.append(gridRow)\n",
        "    imageArray.append(grid)\n",
        "\n",
        "# # Show data\n",
        "# for i in range(len(imageArray)):\n",
        "#     print(\"day \" + str(i))\n",
        "#     for x in range(len(imageArray[i])):\n",
        "#         for y in range(len(imageArray[i][x])):\n",
        "#             print(imageArray[i][x][y], end='')\n",
        "#         print('')\n",
        "#     print('')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbzY7gqhtDSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################ normalize data\n",
        "\n",
        "# time_imageNormalization = time.time()\n",
        "\n",
        "imageNormal = []\n",
        "shape_imageArray = array(imageArray).shape\n",
        "\n",
        "imageArray = array(imageArray).reshape(shape_imageArray[0] * shape_imageArray[1] * shape_imageArray[2], shape_imageArray[3])\n",
        "\n",
        "normalizeObject_f0 = MinMaxScaler()\n",
        "normalizeObject_f1 = MinMaxScaler()\n",
        "normalizeObject_f2 = MinMaxScaler()\n",
        "normalizeObject_f3 = MinMaxScaler()\n",
        "normalizeObject_f4 = MinMaxScaler()\n",
        "normalizeObject_f5 = MinMaxScaler()\n",
        "normalizeObject_f6 = MinMaxScaler()\n",
        "normalizeObject_f7 = MinMaxScaler()\n",
        "normalizeObject_f8 = MinMaxScaler()\n",
        "normalizeObject_f9 = MinMaxScaler()\n",
        "\n",
        "imageArray_f0 = split_d4Datas(imageArray, 0)\n",
        "imageArray_f1 = split_d4Datas(imageArray, 1)\n",
        "imageArray_f2 = split_d4Datas(imageArray, 2)\n",
        "imageArray_f3 = split_d4Datas(imageArray, 3)\n",
        "imageArray_f4 = split_d4Datas(imageArray, 4)\n",
        "imageArray_f5 = split_d4Datas(imageArray, 5)\n",
        "imageArray_f6 = split_d4Datas(imageArray, 6)\n",
        "imageArray_f7 = split_d4Datas(imageArray, 7)\n",
        "imageArray_f8 = split_d4Datas(imageArray, 8)\n",
        "imageArray_f9 = split_d4Datas(imageArray, 9)\n",
        "\n",
        "imageArray_f0 = normalizeObject_f0.fit_transform(imageArray_f0)\n",
        "imageArray_f1 = normalizeObject_f1.fit_transform(imageArray_f1)\n",
        "imageArray_f2 = normalizeObject_f2.fit_transform(imageArray_f2)\n",
        "imageArray_f3 = normalizeObject_f3.fit_transform(imageArray_f3)\n",
        "imageArray_f4 = normalizeObject_f4.fit_transform(imageArray_f4)\n",
        "imageArray_f5 = normalizeObject_f5.fit_transform(imageArray_f5)\n",
        "imageArray_f6 = normalizeObject_f6.fit_transform(imageArray_f6)\n",
        "imageArray_f7 = normalizeObject_f7.fit_transform(imageArray_f7)\n",
        "imageArray_f8 = normalizeObject_f8.fit_transform(imageArray_f8)\n",
        "imageArray_f9 = normalizeObject_f9.fit_transform(imageArray_f9)\n",
        "\n",
        "for i in range(len(imageArray)):\n",
        "    imageNormal.append([imageArray_f0[i][0], imageArray_f1[i][0], imageArray_f2[i][0],\n",
        "                      imageArray_f3[i][0], imageArray_f4[i][0], imageArray_f5[i][0],\n",
        "                      imageArray_f6[i][0], imageArray_f7[i][0], imageArray_f8[i][0], imageArray_f9[i][0]])\n",
        "\n",
        "imageNormal = array(imageNormal)\n",
        "imageNormal = imageNormal.reshape(shape_imageArray[0], shape_imageArray[1], shape_imageArray[2], shape_imageArray[3])\n",
        "    \n",
        "# time_lap = time.time()\n",
        "\n",
        "# # Show data\n",
        "# for i in range(len(imageNormal)):\n",
        "#     print(\"day \" + str(i))\n",
        "#     for x in range(len(imageNormal[i])):\n",
        "#         for y in range(len(imageNormal[i][x])):\n",
        "#             print(imageNormal[i][x][y], end='')\n",
        "#         print('')\n",
        "#     print('')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF2LHkIptFjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################ split imageArray into train Data(dataTrain) and test Data(dataTest)\n",
        "\n",
        "# dataTrain = imageNormal[:-14]\n",
        "# dataTest = imageNormal[-28:]\n",
        "\n",
        "data_shape = (shape_imageArray[1], shape_imageArray[2], shape_imageArray[3])\n",
        "\n",
        "x_dataTrain = imageNormal[:-28][:-14]\n",
        "y_dataTrain = imageNormal[:-28][14:]\n",
        "y_dataTrain_final = []\n",
        "for i in range(len(y_dataTrain)):\n",
        "    grid = []\n",
        "    for j in range(data_shape[0]):\n",
        "        gridRow = []\n",
        "        for k in range(data_shape[1]):\n",
        "            gridCell = [y_dataTrain[i][j][k][0]]\n",
        "            gridRow.append(gridCell)\n",
        "        grid.append(gridRow)\n",
        "    y_dataTrain_final.append(grid)\n",
        "\n",
        "y_dataTrain = array(y_dataTrain_final)\n",
        "\n",
        "x_dataValidation = imageNormal[-42:-14][:-14]\n",
        "y_dataValidation = imageNormal[-42:-14][14:]\n",
        "y_dataValidation_final = []\n",
        "for i in range(len(y_dataValidation)):\n",
        "    grid = []\n",
        "    for j in range(data_shape[0]):\n",
        "        gridRow = []\n",
        "        for k in range(data_shape[1]):\n",
        "            gridCell = [y_dataValidation[i][j][k][0]]\n",
        "            gridRow.append(gridCell)\n",
        "        grid.append(gridRow)\n",
        "    y_dataValidation_final.append(grid)\n",
        "\n",
        "y_dataValidation = array(y_dataValidation_final)\n",
        "\n",
        "x_dataTest = imageNormal[-28:][:-14]\n",
        "y_dataTest = imageNormal[-28:][14:]\n",
        "y_dataTest_final = []\n",
        "for i in range(len(y_dataTest)):\n",
        "    grid = []\n",
        "    for j in range(data_shape[0]):\n",
        "        gridRow = []\n",
        "        for k in range(data_shape[1]):\n",
        "            gridCell = [y_dataTest[i][j][k][0]]\n",
        "            gridRow.append(gridCell)\n",
        "        grid.append(gridRow)\n",
        "    y_dataTest_final.append(grid)\n",
        "\n",
        "y_dataTest = array(y_dataTest_final)\n",
        "\n",
        "# Clear memory\n",
        "gridIntersection.clear()\n",
        "countiesData_temporal.clear()\n",
        "countiesData_fix.clear()\n",
        "# imageArray.clear()\n",
        "# imageNormal.clear()\n",
        "\n",
        "################################################################ print execution time\n",
        "    \n",
        "# time_endTime = time.time()\n",
        "\n",
        "# print('\\t|Image creation time: {0}'.format(time_imageNormalization - time_imageCreation))\n",
        "# print('\\t|Image normalization time: {0}'.format(time_lap - time_imageNormalization))\n",
        "# print('\\t|full execution time: {0}'.format(time_endTime - time_mainStart))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZZwnzuMwHjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "a2406fd6-415d-43a5-9be1-0093b8463a84"
      },
      "source": [
        "\n",
        "################################################################ init model\n",
        "model = keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=data_shape))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size = (3,3), padding='same', activation=\"relu\"))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=128, kernel_size = (3,3), padding='same', activation=\"relu\"))\n",
        "model.add(Conv2D(filters=128, kernel_size = (3,3), padding='same', activation=\"relu\"))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())    \n",
        "model.add(Conv2D(filters=256, kernel_size = (3,3), padding='same', activation=\"relu\"))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(512,activation=\"relu\"))\n",
        "model.add(Dense(32,activation=\"relu\"))\n",
        "model.add(Dense(1,activation=\"softmax\"))\n",
        "\n",
        "model.compile('adam', 'mean_squared_error', metrics=['accuracy'])\n",
        "# model.compile(loss=keras.losses.poisson, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "# model.compile(optimizer='adam', loss=tf.keras.losses.Poisson())\n",
        "\n",
        "model.fit(x_dataTrain, y_dataTrain, batch_size=32, epochs=25, verbose=1, validation_data=(x_dataValidation, y_dataValidation))\n",
        "score = model.evaluate(x_dataTest, y_dataTest, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "3/3 [==============================] - 1s 445ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/25\n",
            "3/3 [==============================] - 1s 361ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/25\n",
            "3/3 [==============================] - 1s 366ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/25\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/25\n",
            "3/3 [==============================] - 1s 356ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/25\n",
            "3/3 [==============================] - 1s 358ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/25\n",
            "3/3 [==============================] - 1s 359ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/25\n",
            "3/3 [==============================] - 1s 360ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/25\n",
            "3/3 [==============================] - 1s 356ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/25\n",
            "3/3 [==============================] - 1s 354ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/25\n",
            "3/3 [==============================] - 1s 362ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/25\n",
            "3/3 [==============================] - 1s 355ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/25\n",
            "3/3 [==============================] - 1s 360ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/25\n",
            "3/3 [==============================] - 1s 363ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/25\n",
            "3/3 [==============================] - 1s 365ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/25\n",
            "3/3 [==============================] - 1s 363ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/25\n",
            "3/3 [==============================] - 1s 357ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/25\n",
            "3/3 [==============================] - 1s 359ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/25\n",
            "3/3 [==============================] - 1s 355ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/25\n",
            "3/3 [==============================] - 1s 358ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/25\n",
            "3/3 [==============================] - 1s 355ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/25\n",
            "3/3 [==============================] - 1s 356ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/25\n",
            "3/3 [==============================] - 1s 359ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/25\n",
            "3/3 [==============================] - 1s 357ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/25\n",
            "3/3 [==============================] - 1s 357ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.9823 - val_accuracy: 0.0000e+00\n",
            "Test loss: 0.968929648399353\n",
            "Test accuracy: 0.00015873015217948705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wtnj22h3scq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3acd522b-51b3-46c9-ae4b-dfd7a0907baf"
      },
      "source": [
        "y_dataTest_shape = array(y_dataTest).shape\n",
        "print(y_dataTest_shape)\n",
        "y_dataTest_original = normalizeObject_f0.inverse_transform(array(y_dataTest).reshape(y_dataTest_shape[0] * y_dataTest_shape[1] * y_dataTest_shape[2], y_dataTest_shape[3]))\n",
        "y_dataTest_original = y_dataTest_original.reshape(y_dataTest_shape[0], y_dataTest_shape[1], y_dataTest_shape[2], y_dataTest_shape[3])\n",
        "\n",
        "pred = model.predict(x_dataTest)\n",
        "pred_shape = pred.shape\n",
        "print(pred_shape)\n",
        "pred = normalizeObject_f0.inverse_transform(pred.reshape(pred_shape[0] * pred_shape[1] * pred_shape[2], pred_shape[3]))\n",
        "pred = pred.reshape(pred_shape[0], pred_shape[1], pred_shape[2], pred_shape[3])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14, 30, 15, 1)\n",
            "(14, 30, 15, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZTfkaDbyshC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c26e233f-1be8-4fdc-902b-9d9df52e39f7"
      },
      "source": [
        "for i in range(len(y_dataTest)):\n",
        "    print(y_dataTest[i][0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[0.01408451]\n",
            " [0.01408451]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[0.01408451]\n",
            " [0.01408451]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[0.01408451]\n",
            " [0.01408451]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]]\n",
            "[[0.01408451]\n",
            " [0.01408451]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]]\n",
            "[[0.01408451]\n",
            " [0.01408451]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[0.01408451]\n",
            " [0.01408451]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}